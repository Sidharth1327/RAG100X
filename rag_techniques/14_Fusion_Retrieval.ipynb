{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c936b9",
   "metadata": {},
   "source": [
    "## 🧠 Fusion Retrieval for Smarter Search | RAG100X\n",
    "\n",
    "This notebook implements **Fusion Retrieval** — a technique that blends both **vector-based similarity search** and **keyword-based BM25 retrieval** to improve the **relevance and coverage** of retrieved chunks.\n",
    "\n",
    "Instead of relying on a single retrieval method, fusion combines the **semantic power of dense embeddings** with the **precision of keyword matching**, leading to more accurate and diverse retrieval — especially useful when queries are ambiguous or domain-specific.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ What You’ll Learn\n",
    "\n",
    "- Why using only vector or keyword search can miss important results  \n",
    "- How to build a **hybrid retriever** that fuses FAISS + BM25  \n",
    "- How to normalize and combine relevance scores from both sources  \n",
    "- How to tune the fusion ratio (`alpha`) to balance semantic vs. lexical signals  \n",
    "- Why fusion improves robustness across different types of queries  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Real-world Analogy\n",
    "\n",
    "Imagine searching a library for info on \"artificial intelligence\":\n",
    "\n",
    "> 🔍 BM25 finds books with the exact phrase in titles or chapters  \n",
    "> 🧠 FAISS retrieves books that conceptually relate to AI, even if the term isn’t mentioned directly  \n",
    "> 🧪 Fusion blends both to give you the **most relevant and well-rounded** set of sources\n",
    "\n",
    "✅ That’s fusion retrieval — **semantic + lexical synergy**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 How Fusion Retrieval Works Under the Hood\n",
    "\n",
    "| Step                        | What Happens                                                                 |\n",
    "|-----------------------------|------------------------------------------------------------------------------|\n",
    "| 1. Document Chunking        | PDF is split into overlapping chunks using a recursive text splitter         |\n",
    "| 2. FAISS Vector Index       | Chunks are embedded using OpenAI embeddings and stored in FAISS              |\n",
    "| 3. BM25 Index               | Same chunks are indexed via BM25 for keyword-based retrieval                 |\n",
    "| 4. Dual Retrieval           | A query is run through both FAISS and BM25 retrievers                        |\n",
    "| 5. Score Normalization      | Results from both methods are scored and normalized                          |\n",
    "| 6. Weighted Fusion          | Scores are combined using a tunable `alpha` (e.g. 0.5 for equal weight)      |\n",
    "| 7. Top-k Selection          | Highest-ranking fused results are passed to the LLM                         |\n",
    "\n",
    "🧠 You get results that combine **semantic depth** with **lexical precision**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 Why Fusion Retrieval Works So Well\n",
    "\n",
    "- ⚖️ **Balanced relevance**: Captures both meanings and exact terms  \n",
    "- 🧠 **Resilient to query type**: Works well for vague or specific queries  \n",
    "- 🧪 **Customizable weighting**: Adjust fusion ratio for different use cases  \n",
    "- 📈 **Better coverage**: Reduces missed hits from one-sided retrieval  \n",
    "\n",
    "---\n",
    "\n",
    "### 🏗️ Why This Matters in Production\n",
    "\n",
    "Vector search alone might miss:\n",
    "\n",
    "> \"How does GPT learn?\" → May not match docs that say “language model training”\n",
    "\n",
    "BM25 alone might miss:\n",
    "\n",
    "> \"LLM capabilities in summarization\" → Finds docs with keywords, but not conceptually related ones\n",
    "\n",
    "✅ Fusion ensures both **surface-level matches and deep semantic connections** are retrieved.\n",
    "\n",
    "This is critical for:\n",
    "\n",
    "- Complex search queries  \n",
    "- Knowledge-heavy domains (e.g. law, medicine, finance)  \n",
    "- Maximizing both recall and precision in production RAG systems  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Where This Fits in RAG100X\n",
    "\n",
    "In previous projects, you explored:\n",
    "\n",
    "1. Raw vector retrieval from PDF, CSV, Web  \n",
    "2. Chunking enhancements (Semantic, Propositional, Header-based)  \n",
    "3. Compression and Query Expansion techniques (RSE, CEW, HyDE, HyPE)  \n",
    "4. Windowed and segment-aware context methods\n",
    "\n",
    "Now, in **Day 14**, you enhance the **retrieval strategy itself**:  \n",
    "> 💡 **Blend the strengths of different retrievers for smarter, richer context.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e11e738",
   "metadata": {},
   "source": [
    "## 📦 Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain numpy python-dotenv rank-bm25\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from typing import List\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6769bd",
   "metadata": {},
   "source": [
    "### Define document path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad99f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1cd58c",
   "metadata": {},
   "source": [
    "## 📄 Step 1: Encode PDF to Vector Store and Prepare for BM25 | RAG100X\n",
    "\n",
    "Before we can perform fusion retrieval, we need to prepare our **document corpus** in two formats:\n",
    "\n",
    "1. A **vector store** (for semantic search via FAISS)  \n",
    "2. A **split document list** (for lexical search via BM25)\n",
    "\n",
    "This function does exactly that — it takes in a PDF file, splits its content into manageable chunks, and creates a vector store using OpenAI embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 What’s Happening Here?\n",
    "\n",
    "- **PDF Loading**: The `PyPDFLoader` reads the full document and extracts its pages as raw text.\n",
    "- **Chunking**: We split the text into smaller overlapping chunks using `RecursiveCharacterTextSplitter`. This helps preserve context while fitting within LLM token limits.\n",
    "- **Cleaning**: A custom function `replace_t_with_space()` is applied to fix formatting issues (like `\\t`).\n",
    "- **Embedding**: We encode the chunks using `OpenAIEmbeddings`, turning them into high-dimensional vectors.\n",
    "- **Vector Store**: These embeddings are saved into a FAISS index for fast similarity-based retrieval.\n",
    "- **Return Value**: We return both the FAISS vector store and the cleaned split documents (needed later for BM25 indexing).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9835f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# Function for Cleaning the document\n",
    "def replace_t_with_space(list_of_documents):\n",
    "    \"\"\"\n",
    "    Replaces all tab characters ('\\t') with spaces in the page content of each document\n",
    "\n",
    "    Args:\n",
    "        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n",
    "\n",
    "    Returns:\n",
    "        The modified list of documents with tab characters replaced by spaces.\n",
    "    \"\"\"\n",
    "\n",
    "    for doc in list_of_documents:\n",
    "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
    "    return list_of_documents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def encode_pdf_and_get_split_documents(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF into a FAISS vector store using OpenAI embeddings,\n",
    "    and returns the split text chunks for BM25 indexing.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the PDF file.\n",
    "        chunk_size (int): Number of characters in each chunk.\n",
    "        chunk_overlap (int): Overlap between adjacent chunks for context preservation.\n",
    "\n",
    "    Returns:\n",
    "        vectorstore (FAISS): Vector store with OpenAI-embedded chunks.\n",
    "        cleaned_texts (List[Document]): List of cleaned, split chunks (for BM25).\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Load the PDF document\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()  # Extract pages as Document objects\n",
    "\n",
    "    # Step 2: Split text into overlapping chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Step 3: Clean up formatting issues (e.g. remove tab characters)\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    # Step 4: Embed chunks using OpenAI and store in FAISS vector DB\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
    "\n",
    "    # Return both vector store and chunked docs for BM25\n",
    "    return vectorstore, cleaned_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a34ca4",
   "metadata": {},
   "source": [
    "### Create vectorstore and get the chunked documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d07350",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorstore, cleaned_texts = encode_pdf_and_get_split_documents(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1bc03e",
   "metadata": {},
   "source": [
    "## 🔍 Create BM25 Index for Keyword-Based Retrieval | RAG100X\n",
    "\n",
    "While vector stores handle **semantic similarity**, we also want to support **exact keyword-based matching**. That’s where **BM25** comes in.\n",
    "\n",
    "BM25 (Best Matching 25) is a traditional, **lexical retrieval algorithm** that works great when users search with exact terms. It builds on **TF-IDF**, but adds normalization to handle document length more fairly.\n",
    "\n",
    "\n",
    "\n",
    "### 🧠 What’s Happening Here?\n",
    "\n",
    "- **Purpose**: We’re building a **BM25 index** from the previously split and cleaned document chunks.\n",
    "- **Input**: The function expects a list of `Document` objects (already chunked and cleaned).\n",
    "- **Tokenization**: For each document, we split the text into words using simple whitespace-based tokenization.\n",
    "- **Indexing**: `BM25Okapi` (from the `rank_bm25` library) is used to build the index over these tokenized lists.\n",
    "\n",
    "\n",
    "\n",
    "### ⚙️ Under the Hood\n",
    "\n",
    "Let’s say we have two documents:\n",
    "\n",
    "- Doc 1: `\"Climate change is caused by carbon emissions.\"`\n",
    "- Doc 2: `\"Carbon dioxide leads to global warming.\"`\n",
    "\n",
    "After tokenization:\n",
    "\n",
    "```python\n",
    "[\n",
    "  [\"Climate\", \"change\", \"is\", \"caused\", \"by\", \"carbon\", \"emissions.\"],\n",
    "  [\"Carbon\", \"dioxide\", \"leads\", \"to\", \"global\", \"warming.\"]\n",
    "]\n",
    "\n",
    "When someone searches for \"carbon emissions\", BM25 scores each document based on how many query terms match, and how rare those words are across the corpus (via IDF weighting). The result is a ranked list of documents most likely to answer the query.\n",
    "\n",
    " BM25 complements semantic search by surfacing documents with strong lexical overlap, even if their meaning isn’t captured well by embeddings. This sets us up to later fuse both scores for better results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f2e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def create_bm25_index(documents: List[Document]) -> BM25Okapi:\n",
    "    \"\"\"\n",
    "    Creates a BM25 index over the cleaned document chunks.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): Chunks of text wrapped as LangChain Document objects.\n",
    "\n",
    "    Returns:\n",
    "        BM25Okapi: A keyword-aware index that can score chunks based on query relevance.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize each document using simple whitespace split.\n",
    "    # More advanced NLP tokenization (e.g., stemming, stopword removal) could improve results.\n",
    "    tokenized_docs = [doc.page_content.split() for doc in documents]\n",
    "\n",
    "    # Create the BM25 index using the tokenized text\n",
    "    bm25_index = BM25Okapi(tokenized_docs)\n",
    "\n",
    "    return bm25_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e9d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = create_bm25_index(cleaned_texts) # Create BM25 index from the cleaned texts (chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189bbda7",
   "metadata": {},
   "source": [
    "## ♾️ Fusion-Based Retrieval | Combine Semantic + Keyword Matching | RAG100X\n",
    "\n",
    "In this step, we build a **hybrid retriever** that merges two powerful techniques:\n",
    "\n",
    "- 🧠 **Vector Search** — captures semantic similarity based on meaning\n",
    "- 🔍 **BM25 Keyword Search** — captures exact word overlap\n",
    "\n",
    "Instead of choosing one or the other, we **combine the scores** of both approaches using **score-level fusion**. This balances precision (exact match) and recall (semantic understanding), leading to more accurate retrieval.\n",
    "\n",
    "\n",
    "### 🧠 Why Combine BM25 and Vector Search?\n",
    "\n",
    "Let’s say the user query is:\n",
    "\n",
    "> \"causes of global warming\"\n",
    "\n",
    "- **BM25** will surface documents that literally contain “causes”, “global”, and “warming”.\n",
    "- **Vector Search** might find:\n",
    "  - “burning fossil fuels has increased Earth’s temperature”\n",
    "  - “CO₂ emissions driving climate change”\n",
    "\n",
    "While the second set might not contain the original words, they’re clearly **semantically aligned**.\n",
    "\n",
    "By **fusing** both, we ensure the retrieved chunks:\n",
    "- Are *semantically relevant*\n",
    "- And *lexically grounded*\n",
    "\n",
    "This makes the retrieval much more reliable for downstream LLMs.\n",
    "\n",
    "\n",
    "### ⚙️ Step-by-Step Breakdown\n",
    "\n",
    "#### 1. **Get All Documents**\n",
    "We retrieve all indexed documents from the vectorstore (even though we only need top-k), because we’ll need to **score all documents** to compare properly.\n",
    "\n",
    "#### 2. **BM25 Scoring**\n",
    "We compute keyword-based scores using BM25, which works by comparing term frequency, inverse document frequency, and document length.\n",
    "\n",
    "#### 3. **Vector Scoring**\n",
    "We run a semantic search using FAISS and get similarity scores (lower is better — they're distances).\n",
    "\n",
    "#### 4. **Score Normalization**\n",
    "Since the score scales are very different (BM25 vs FAISS), we normalize both to the range `[0, 1]`:\n",
    "- BM25 scores are already in a positive range.\n",
    "- FAISS scores are inverted (because lower = better), so we flip them by `1 - normed`.\n",
    "\n",
    "We also add a small epsilon to prevent divide-by-zero errors.\n",
    "\n",
    "#### 5. **Score Fusion**\n",
    "We blend the two normalized scores using a weighted average:\n",
    "\n",
    "- `final_score = alpha * vector_score + (1 - alpha) * bm25_score`\n",
    "\n",
    "- `alpha = 1.0` → only vector search\n",
    "- `alpha = 0.0` → only BM25\n",
    "- `alpha = 0.5` → equal weight\n",
    "\n",
    "#### 6. **Rank and Return**\n",
    "We sort documents by this combined score and return the top `k` most relevant ones.\n",
    "\n",
    "### ✅ Benefits of Fusion Retrieval\n",
    "\n",
    "- ✅ Balances *meaning* and *exact matches*\n",
    "- ✅ Handles synonyms, paraphrasing, and misspellings better than BM25 alone\n",
    "- ✅ Improves precision by grounding abstract matches with lexical overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569df8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "def fusion_retrieval(vectorstore, bm25, query: str, k: int = 5, alpha: float = 0.5) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Combines semantic (vector-based) and lexical (BM25-based) retrieval scores,\n",
    "    normalizes them, and returns the top-k documents ranked by the fused score.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: FAISS vector index for semantic similarity.\n",
    "        bm25: BM25 index for keyword matching.\n",
    "        query: The user's input query.\n",
    "        k: Number of top documents to return.\n",
    "        alpha: Weight for semantic score in fusion (between 0 and 1).\n",
    "        \n",
    "    Returns:\n",
    "        List[Document]: Top-k most relevant documents based on fused ranking.\n",
    "    \"\"\"\n",
    "    \n",
    "    epsilon = 1e-8  # To avoid division-by-zero during normalization\n",
    "\n",
    "    # Step 1: Retrieve all documents from the vectorstore (to align scores properly)\n",
    "    all_docs = vectorstore.similarity_search(\"\", k=vectorstore.index.ntotal)\n",
    "\n",
    "    # Step 2: Compute BM25 keyword scores for all documents\n",
    "    bm25_scores = bm25.get_scores(query.split())\n",
    "\n",
    "    # Step 3: Perform vector similarity search and get FAISS distances\n",
    "    vector_results = vectorstore.similarity_search_with_score(query, k=len(all_docs))\n",
    "    vector_scores = np.array([score for _, score in vector_results])  # Lower = better\n",
    "\n",
    "    # Step 4: Normalize both score sets to [0, 1] range\n",
    "    # Invert vector scores because FAISS returns distances\n",
    "    vector_scores = 1 - (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores) + epsilon)\n",
    "\n",
    "    # Normalize BM25 scores (already higher = better)\n",
    "    bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + epsilon)\n",
    "\n",
    "    # Step 5: Combine scores using weighted average\n",
    "    combined_scores = alpha * vector_scores + (1 - alpha) * bm25_scores\n",
    "\n",
    "    # Step 6: Sort documents by combined score (high to low)\n",
    "    sorted_indices = np.argsort(combined_scores)[::-1]\n",
    "\n",
    "    # Step 7: Return top-k documents\n",
    "    return [all_docs[i] for i in sorted_indices[:k]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317673b3",
   "metadata": {},
   "source": [
    "### Use Case example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e23b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_context(context):\n",
    "    \"\"\"\n",
    "    Display the contents of the provided context list.\n",
    "\n",
    "    Args:\n",
    "        context (list): A list of context items to be displayed.\n",
    "\n",
    "    Prints each context item in the list with a heading indicating its position.\n",
    "    \"\"\"\n",
    "    for i, c in enumerate(context):\n",
    "        print(f\"Context {i + 1}:\")\n",
    "        print(c)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "# Query\n",
    "query = \"What are the impacts of climate change on the environment?\"\n",
    "\n",
    "# Perform fusion retrieval\n",
    "top_docs = fusion_retrieval(vectorstore, bm25, query, k=5, alpha=0.5)\n",
    "docs_content = [doc.page_content for doc in top_docs]\n",
    "show_context(docs_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf0da1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📘 Summary & Credits\n",
    "\n",
    "This notebook is based on the excellent open-source repository [RAG_Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques).  \n",
    "I referred to that work to understand how the pipeline is structured and then reimplemented the same concept in a **fully self-contained** way, but using recent models — as part of my personal learning journey.\n",
    "\n",
    "The purpose of this notebook is purely **educational**:  \n",
    "- To deepen my understanding of Retrieval-Augmented Generation systems  \n",
    "- To keep a clean, trackable log of what I’ve built and learned  \n",
    "- And to serve as a future reference for myself or others starting from scratch\n",
    "\n",
    "To support that, I’ve added clear, concise markdowns throughout the notebook — explaining *why* each package was installed, *why* each line of code exists, and *how* each component fits into the overall RAG pipeline. It’s designed to help anyone (including my future self) grasp the **how** and the **why**, not just the **what**.\n",
    "\n",
    "## 🔍 Why Use Fusion Retrieval in RAG?\n",
    "\n",
    "Semantic search alone can miss important keywords. Keyword search alone can miss meaning.\n",
    "\n",
    "**Fusion Retrieval** combines the best of both:\n",
    "- 🧠 Captures **semantic relevance** using vector embeddings  \n",
    "- 🔍 Preserves **keyword precision** using BM25 scoring  \n",
    "- ⚖️ Produces balanced, high-quality document retrieval for LLMs  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What’s New in This Version?\n",
    "\n",
    "This implementation includes:\n",
    "\n",
    "- 🔄 A **score-level fusion** of FAISS (vector) and BM25 (keyword) retrieval  \n",
    "- ⚙️ **Score normalization** to align semantic and lexical scales  \n",
    "- ⚖️ **Weighted blending** of scores with customizable alpha control  \n",
    "- 📚 End-to-end retrieval pipeline returning the top-K most relevant documents  \n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Inferences & Key Takeaways\n",
    "\n",
    "- ✅ Fusion improves **recall and precision** over using BM25 or vector search alone  \n",
    "- 🔍 Helps catch **keyword-sensitive** queries while retaining semantic flexibility  \n",
    "- ⚖️ Tunable alpha allows dynamic prioritization of meaning vs. match  \n",
    "- 📦 Easily integratable with any LangChain retriever + BM25 wrapper  \n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 What Could Be Added Next?\n",
    "\n",
    "- 📊 Add evaluation with retrieval metrics (precision, recall, MRR)  \n",
    "- 🔬 Explore rank fusion techniques (Reciprocal Rank Fusion, Borda Count)  \n",
    "- 🧪 Integrate rerankers post-fusion to boost top-K accuracy  \n",
    "- 🌐 Extend to multi-query or multi-modal fusion (e.g., images + text)  \n",
    "\n",
    "---\n",
    "## 💡 Final Word\n",
    "\n",
    "This notebook is part of my larger personal project: **RAG100x** — a challenge to build and log my journney in RAG from 0 100 in the coming months.\n",
    "\n",
    "It’s not built to impress — it’s built to **progress**.  \n",
    "Everything here is structured to enable **daily iteration**, focused experimentation, and clean documentation.\n",
    "\n",
    "If you're exploring RAG from first principles, feel free to use this as a scaffold for your own builds. And of course — check out the original repository for broader implementations and ideas."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
