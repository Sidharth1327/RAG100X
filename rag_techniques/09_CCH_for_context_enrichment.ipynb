{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86685ce4",
   "metadata": {},
   "source": [
    "## 🧠 Contextual Chunk Headers (CCH) for Better Retrieval | RAG100X\n",
    "\n",
    "This notebook implements **Contextual Chunk Headers (CCH)** — a simple yet powerful technique to improve retrieval quality by giving each document chunk its own meaningful, LLM-generated title.\n",
    "\n",
    "Instead of storing raw text chunks in your vectorstore (which may lack clear intent or context), CCH uses an LLM to **generate a short, descriptive header** for every chunk — like a topic label or section title. This header is then prepended to the chunk before embedding.\n",
    "\n",
    "The result? A more semantically rich vector that better aligns with user queries — especially when queries are short, vague, or topically phrased.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ What You’ll Learn\n",
    "\n",
    "- Why plain text chunks often fail to capture the *intent* of the content  \n",
    "- How CCH uses GPT-4 to summarize each chunk into a contextual title  \n",
    "- How prepending this header improves semantic similarity in retrieval  \n",
    "- When this helps — and how it compares to baseline RAG  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Real-world Analogy\n",
    "\n",
    "Imagine you have a box of textbook pages, all cut into random paragraphs. You’re asked:\n",
    "\n",
    "> *\"Do you have anything about 'causes of the French Revolution'?\"*\n",
    "\n",
    "Now you have to go through each paragraph and **guess** what it’s about — because there's no section title or chapter heading.\n",
    "\n",
    "But what if each paragraph had a header like:\n",
    "- “Economic Inequality and Tax Burden”\n",
    "- “Role of Enlightenment Ideas”\n",
    "- “Monarchy and Political Structure”\n",
    "\n",
    "Suddenly, matching a user’s question becomes much easier — you can filter fast based on the header, then read the details.\n",
    "\n",
    "✅ **CCH adds that helpful title to every chunk — so your retriever doesn’t have to guess.**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔬 How CCH Works Under the Hood\n",
    "\n",
    "Let’s say we have this chunk from a long report:\n",
    "\n",
    "> “In pre-revolutionary France, the Third Estate bore the majority of tax burden while the clergy and nobility were largely exempt. This economic disparity led to growing unrest…”\n",
    "\n",
    "With **standard RAG**:\n",
    "\n",
    "| Step    | What Happens                            |\n",
    "|---------|-----------------------------------------|\n",
    "| Embed   | Only this paragraph is embedded as-is   |\n",
    "| Retrieve| User query is matched to paragraph text |\n",
    "| Issue   | Context is missing — vague match        |\n",
    "\n",
    "With **CCH**, we do this:\n",
    "\n",
    "| Step              | What Happens                                                            |\n",
    "|-------------------|-------------------------------------------------------------------------|\n",
    "| 1. Chunk          | We extract a paragraph from the document                               |\n",
    "| 2. Prompting      | GPT-4 is asked: *“Write a short header or title summarizing this text”*|\n",
    "| 3. Output         | → “Tax Inequality in Pre-Revolutionary France”                         |\n",
    "| 4. Combine        | Prepend the header to the chunk: *“Tax Inequality... \\n\\n In pre…”*    |\n",
    "| 5. Embedding      | The combined text is embedded and stored in FAISS                      |\n",
    "| 6. Retrieval      | User query is now matched to the richer representation                 |\n",
    "\n",
    "✅ This gives better similarity when the user asks:\n",
    "> *\"Why was the tax system unfair before the French Revolution?\"*\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Why This Works So Well\n",
    "\n",
    "- 🏷️ **Topic priming**: The header acts like a title, helping the embedding capture what the chunk is *about*  \n",
    "- 🧠 **Better alignment**: User queries often resemble headers — not full paragraphs  \n",
    "- ⚡ **Low overhead**: Just one extra LLM call per chunk, done offline  \n",
    "\n",
    "---\n",
    "\n",
    "### 🏗️ Why This Matters in Production\n",
    "\n",
    "In most corpora — like reports, transcripts, or long articles — individual chunks don’t explain themselves well.\n",
    "\n",
    "For example:\n",
    "> *“We experimented with a 3-layer CNN followed by max pooling…”*\n",
    "\n",
    "Without context, this tells you *what* was done, but not *why*. A good header like:\n",
    "\n",
    "> **“Model Architecture for Toxic Comment Classification”**\n",
    "\n",
    "…adds missing intent. Now, if a user searches for “toxic comment classifier architecture”, retrieval becomes much more effective.\n",
    "\n",
    "**CCH injects semantic clarity into every chunk.**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Where This Fits in RAG100X\n",
    "\n",
    "So far in RAG100X, we’ve explored:\n",
    "\n",
    "1. PDF-based QA  \n",
    "2. CSV-based search  \n",
    "3. Web-based hallucination grading  \n",
    "4. Chunk-size optimization  \n",
    "5. Proposition-aware chunking  \n",
    "6. Query rewriting + decomposition  \n",
    "7. HyDE: Imagine answers before retrieval  \n",
    "8. HyPE: Store likely questions during indexing  \n",
    "\n",
    "Now in **Day 9**, we flip the focus back to chunks — and **make them smarter by adding titles**.\n",
    "\n",
    "> 💡 **CCH gives every chunk a name — so it’s easier to find, even when the user isn’t specific.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dddd2cb",
   "metadata": {},
   "source": [
    "## 📦 Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c6896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain openai python-dotenv tiktoken\n",
    "\n",
    "import cohere\n",
    "# Tokenizer used to count tokens in chunks (important for consistent chunking)\n",
    "import tiktoken\n",
    "\n",
    "# Typing hint for cleaner code and better autocompletion\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "os.environ[\"CO_API_KEY\"] = os.getenv('CO_API_KEY') # Cohere API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY') # OpenAI API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4ada0",
   "metadata": {},
   "source": [
    "### 📌 Chunk the Input Document into Manageable Segments\n",
    "\n",
    "In this step, we **split the raw document text into smaller, overlapping chunks** to make it retrievable and LLM-friendly:\n",
    "\n",
    "- **RecursiveCharacterTextSplitter**: A smart splitter from LangChain that breaks the text while trying to keep natural boundaries (like paragraphs or sentences).\n",
    "- **Chunk Size**: Controls how much text goes into each chunk. Here we use 800 characters per chunk.\n",
    "- **No Overlap**: We set `chunk_overlap=0` for simplicity, but this can be tuned to preserve context across chunk boundaries.\n",
    "\n",
    "We load the input file (in this case, a plain `.txt` version of Nike’s 2023 Annual Report), split it into chunks, and store the results for further processing (e.g., embedding).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183879a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Function to split large text into smaller chunks\n",
    "def split_into_chunks(text: str, chunk_size: int = 800) -> list[str]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,      # Max length per chunk\n",
    "        chunk_overlap=0,            # No overlap between chunks\n",
    "        length_function=len         # Use character count as length metric\n",
    "    )\n",
    "    documents = text_splitter.create_documents([text])\n",
    "    return [document.page_content for document in documents]\n",
    "\n",
    "# Path to input text document\n",
    "FILE_PATH = \"data/nike_2023_annual_report.txt\"\n",
    "\n",
    "# Read the raw text and split into chunks\n",
    "with open(FILE_PATH, \"r\") as file:\n",
    "    document_text = file.read()\n",
    "\n",
    "chunks = split_into_chunks(document_text, chunk_size=800)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c701b307",
   "metadata": {},
   "source": [
    "### 🧠 Generate a Descriptive Document Title\n",
    "\n",
    "To add context to each chunk during indexing, we generate a **document-level title** using GPT-4o. This acts as a \"header\" for each chunk and improves retrieval quality.\n",
    "\n",
    "- **Why this matters**: A good title makes it easier to rank and retrieve relevant chunks by adding semantic meaning at the document level.\n",
    "- **Prompt Template**: We design a structured system message instructing the LLM to output *only* the title — nothing else.\n",
    "- **Truncation Handling**: If the document is too long, we use `tiktoken` to safely truncate it to fit within model limits.\n",
    "- **Model Used**: `gpt-4o-mini` via OpenAI's chat API.\n",
    "\n",
    "This step returns a clean, high-signal title like *\"Nike Inc. 2023 Annual Financial Report\"* that will later be prepended to each chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e79bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants used in the prompt to extract the document title\n",
    "DOCUMENT_TITLE_PROMPT = \"\"\"\n",
    "INSTRUCTIONS\n",
    "What is the title of the following document?\n",
    "\n",
    "Your response MUST be the title of the document, and nothing else. DO NOT respond with anything else.\n",
    "\n",
    "{document_title_guidance}\n",
    "\n",
    "{truncation_message}\n",
    "\n",
    "DOCUMENT\n",
    "{document_text}\n",
    "\"\"\".strip()\n",
    "\n",
    "TRUNCATION_MESSAGE = \"\"\"\n",
    "Also note that the document text provided below is just the first ~{num_words} words of the document. That should be plenty for this task. Your response should still pertain to the entire document, not just the text provided below.\n",
    "\"\"\".strip()\n",
    "\n",
    "MAX_CONTENT_TOKENS = 4000\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "# Tokenizer to manage truncation before sending text to the LLM\n",
    "TOKEN_ENCODER = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "\n",
    "# Wrapper to call OpenAI's chat model\n",
    "def make_llm_call(chat_messages: list[dict]) -> str:\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=chat_messages,\n",
    "        max_tokens=MAX_CONTENT_TOKENS,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Truncate text to stay within token limits\n",
    "def truncate_content(content: str, max_tokens: int) -> tuple[str, int]:\n",
    "    tokens = TOKEN_ENCODER.encode(content, disallowed_special=())\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    return TOKEN_ENCODER.decode(truncated_tokens), min(len(tokens), max_tokens)\n",
    "\n",
    "# Generate a descriptive title for the document using an LLM\n",
    "def get_document_title(document_text: str, document_title_guidance: str = \"\") -> str:\n",
    "    document_text, num_tokens = truncate_content(document_text, MAX_CONTENT_TOKENS)\n",
    "    truncation_message = TRUNCATION_MESSAGE.format(num_words=3000) if num_tokens >= MAX_CONTENT_TOKENS else \"\"\n",
    "\n",
    "    prompt = DOCUMENT_TITLE_PROMPT.format(\n",
    "        document_title_guidance=document_title_guidance,\n",
    "        document_text=document_text,\n",
    "        truncation_message=truncation_message\n",
    "    )\n",
    "    chat_messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return make_llm_call(chat_messages)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    document_title = get_document_title(document_text)\n",
    "    print(f\"Document Title: {document_title}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cea14a",
   "metadata": {},
   "source": [
    "### 🧪 Add Contextual Chunk Header and Measure Impact\n",
    "\n",
    "One powerful technique to improve chunk relevance is **adding a contextual header**—usually the document title—to each chunk before retrieval. This provides the retriever or reranker with additional semantic clues, which can be especially helpful when the chunk content is ambiguous on its own.\n",
    "\n",
    "In this section, we:\n",
    "\n",
    "- Define a function `rerank_documents()` that uses **Cohere’s Rerank API** to assign relevance scores to each chunk based on a query.\n",
    "- Define `compare_chunk_similarities()` to test a specific chunk with and without the **document title prepended**.\n",
    "- Use similarity scores to show how headers can enhance retrieval quality by grounding the chunk in its broader context.\n",
    "\n",
    "This evaluation gives us a measurable way to **quantify the improvement** from adding a chunk header using LLM-powered reranking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7535378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_documents(query: str, chunks: List[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Rerank document chunks using Cohere Rerank API.\n",
    "\n",
    "    Returns a list of relevance scores for each chunk, based on similarity to the query.\n",
    "    The scores are returned in the original chunk order.\n",
    "    \"\"\"\n",
    "    MODEL = \"rerank-english-v3.0\"\n",
    "    client = cohere.Client(api_key=os.environ[\"CO_API_KEY\"])\n",
    "\n",
    "    reranked_results = client.rerank(model=MODEL, query=query, documents=chunks)\n",
    "    results = reranked_results.results\n",
    "\n",
    "    reranked_indices = [result.index for result in results]\n",
    "    reranked_similarity_scores = [result.relevance_score for result in results]\n",
    "\n",
    "    # Reorder scores to match original chunk order\n",
    "    similarity_scores = [0] * len(chunks)\n",
    "    for i, index in enumerate(reranked_indices):\n",
    "        similarity_scores[index] = reranked_similarity_scores[i]\n",
    "\n",
    "    return similarity_scores\n",
    "\n",
    "\n",
    "def compare_chunk_similarities(chunk_index: int, chunks: List[str], document_title: str, query: str) -> None:\n",
    "    \"\"\"\n",
    "    Compare similarity scores for a chunk with and without a contextual document title.\n",
    "\n",
    "    Prints both scores side-by-side and helps visualize the impact of header-based grounding.\n",
    "    \"\"\"\n",
    "    chunk_text = chunks[chunk_index]\n",
    "    chunk_wo_header = chunk_text\n",
    "    chunk_w_header = f\"Document Title: {document_title}\\n\\n{chunk_text}\"\n",
    "\n",
    "    similarity_scores = rerank_documents(query, [chunk_wo_header, chunk_w_header])\n",
    "\n",
    "    print(f\"\\n📄 Chunk header:\\nDocument Title: {document_title}\")\n",
    "    print(f\"\\n📄 Chunk text:\\n{chunk_text}\")\n",
    "    print(f\"\\n🔍 Query: {query}\")\n",
    "    print(f\"\\n❌ Similarity without header: {similarity_scores[0]:.4f}\")\n",
    "    print(f\"✅ Similarity with header:    {similarity_scores[1]:.4f}\")\n",
    "\n",
    "\n",
    "# Example run — compare relevance with vs. without header\n",
    "CHUNK_INDEX_TO_INSPECT = 86\n",
    "QUERY = \"Nike climate change impact\"\n",
    "\n",
    "compare_chunk_similarities(CHUNK_INDEX_TO_INSPECT, chunks, document_title, QUERY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3be8bf",
   "metadata": {},
   "source": [
    "### 📊 Inference from External Benchmarks\n",
    "\n",
    "While this notebook focuses on the implementation and intuition behind Contextual Chunk Headers (CCH), it's worth noting how this technique performs in broader benchmarks.\n",
    "\n",
    "The original KITE benchmark—spanning diverse corpora like legal opinions, financial filings, and open-source docs—shows consistent gains when using CCH. Average performance improved by **~28%** across datasets, with the most dramatic gains seen in long, complex documents like 10-Ks and legal texts.\n",
    "\n",
    "This reinforces our intuition: adding contextual metadata like document titles or summaries **helps the retriever and reranker better understand the purpose of each chunk**—especially in ambiguous or multi-topic scenarios.\n",
    "\n",
    "While we haven’t run these full-scale benchmarks ourselves, this external evidence strengthens the case for including lightweight context (e.g., titles or section headers) in real-world RAG pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3576d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📘 Summary & Credits\n",
    "\n",
    "This notebook is based on the excellent open-source repository [RAG_Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques).  \n",
    "I referred to that work to understand how the pipeline is structured and then reimplemented the same concept in a **fully self-contained** way, but using recent models — as part of my personal learning journey.\n",
    "\n",
    "The purpose of this notebook is purely **educational**:  \n",
    "- To deepen my understanding of Retrieval-Augmented Generation systems  \n",
    "- To keep a clean, trackable log of what I’ve built and learned  \n",
    "- And to serve as a future reference for myself or others starting from scratch\n",
    "\n",
    "To support that, I’ve added clear, concise markdowns throughout the notebook — explaining *why* each package was installed, *why* each line of code exists, and *how* each component fits into the overall RAG pipeline. It’s designed to help anyone (including my future self) grasp the **how** and the **why**, not just the **what**.\n",
    "\n",
    "## 🔍 Why Use CCH in RAG?\n",
    "\n",
    "Traditional RAG systems chunk documents without preserving **contextual information** like section titles or summaries. This leads to chunks that are semantically isolated and harder for retrievers to interpret.\n",
    "\n",
    "**Contextual Chunk Headers (CCH)** solves this by:\n",
    "- 🧩 **Adding lightweight context (e.g., title + summary) to each chunk during indexing**\n",
    "- 🔍 **Helping retrievers and rerankers disambiguate and score chunks more accurately**\n",
    "\n",
    "This enriches each chunk with purpose-driven metadata, making retrieval **more targeted and semantically aware**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What’s New in This Version?\n",
    "\n",
    "This CCH implementation includes:\n",
    "\n",
    "- 🏷️ **Dynamic header construction** — Titles and summaries are added as structured headers  \n",
    "- 🧠 **GPT-4o-based summarization** — Generates concise summaries per document  \n",
    "- 🧱 **Chunking with injected context** — Each chunk is stored with its respective header  \n",
    "- 📦 **End-to-end self-contained notebook** — Designed for clarity, reproducibility, and modularity  \n",
    "\n",
    "It’s built to be easily adapted to other RAG pipelines or document types.\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Inferences & Key Takeaways\n",
    "\n",
    "While we haven’t benchmarked this on large datasets, insights from public evaluations (e.g., KITE) suggest:\n",
    "\n",
    "- 🧠 CCH improves both **retrieval quality** and **answer grounding**  \n",
    "- 📄 It especially helps on long or noisy documents (e.g., 10-Ks, legal texts)  \n",
    "- 🎯 Even a simple title+summary header yields significant gains in chunk relevance  \n",
    "\n",
    "Contextualizing chunks helps retrieval **without modifying the query or increasing model size**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 What Could Be Added Next?\n",
    "\n",
    "To make this more production-ready or to extend it further:\n",
    "\n",
    "- 🔁 **Pair with rerankers** — Use LLMs like GPT-4 or Cohere ReRank for post-retrieval scoring  \n",
    "- 📚 **Evaluate on real benchmarks** — Try on datasets like KITE or FinanceBench  \n",
    "- 🧪 **Test different header strategies** — Try including section paths, TOC hierarchy, etc.  \n",
    "- ⚡ **Add fast local embedding options** — Swap in BGE or Instructor models for cost efficiency  \n",
    "- 🧠 **Explore retrieval fusion** — Combine raw chunk and CCH-based retrieval for hybrid gains  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 💡 Final Word\n",
    "\n",
    "This notebook is part of my larger personal project: **RAG100x** — a challenge to build and log my journney in RAG from 0 100 in the coming months.\n",
    "\n",
    "It’s not built to impress — it’s built to **progress**.  \n",
    "Everything here is structured to enable **daily iteration**, focused experimentation, and clean documentation.\n",
    "\n",
    "If you're exploring RAG from first principles, feel free to use this as a scaffold for your own builds. And of course — check out the original repository for broader implementations and ideas."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
