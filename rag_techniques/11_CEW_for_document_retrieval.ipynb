{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ec54a4",
   "metadata": {},
   "source": [
    "## 🧠 Context Window Enrichment for Better Retrieval | RAG100X\n",
    "\n",
    "This notebook implements **Context Window Enrichment** — a technique that augments vector-retrieved chunks by including their **neighboring context**, improving semantic coherence for downstream LLM generation.\n",
    "\n",
    "Instead of treating chunks as isolated units, we reconstruct **semantic windows** around each top-retrieved chunk by including adjacent text spans. This mimics how humans read: understanding a paragraph by seeing what came before and after.\n",
    "\n",
    "The result? Richer context, fewer hallucinations, and answers grounded in complete ideas.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ What You’ll Learn\n",
    "\n",
    "- Why single chunks often miss important supporting context  \n",
    "- How adding “pre” and “post” neighbors helps complete the meaning  \n",
    "- How to reconstruct enriched windows using chunk IDs and overlaps  \n",
    "- When this strategy improves performance over plain Top-k chunk retrieval  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Real-world Analogy\n",
    "\n",
    "Suppose someone gives you just this line from a textbook:\n",
    "\n",
    "> *\"...and thus, the algorithm converges in O(log n) time.\"*\n",
    "\n",
    "You’re left wondering: *Which algorithm? What was the problem setup?*  \n",
    "By seeing the sentences before and after, the meaning becomes clear.\n",
    "\n",
    "✅ **Context Window Enrichment ensures LLMs get that full picture — not just a floating quote.**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔬 How Context Window Enrichment Works\n",
    "\n",
    "Let’s say your document is split into overlapping chunks using LangChain. When you run a query:\n",
    "\n",
    "| Step                     | What Happens                                                                   |\n",
    "|--------------------------|----------------------------------------------------------------------------------|\n",
    "| 1. Chunking              | Document split into overlapping chunks and indexed into a FAISS vector store    |\n",
    "| 2. Retrieval             | Top-k most relevant chunks are retrieved via OpenAI embeddings                  |\n",
    "| 3. Context Windowing     | For each top chunk, we identify and include N neighbors before and after        |\n",
    "| 4. Deduplication         | Overlapping or repeated chunks are pruned to stay within token limits           |\n",
    "| 5. Output                | The enriched chunk windows are sent to the LLM for final generation             |\n",
    "\n",
    "🔁 These padded windows ensure more complete answers — especially for queries that relate to topics spread across multiple adjacent chunks.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Why This Works So Well\n",
    "\n",
    "- 🧩 **Captures semantic flow**: Ideas span multiple chunks — windowing reconnects them  \n",
    "- 📚 **Boosts factual grounding**: More complete segments reduce missing links and ambiguity  \n",
    "- 🧠 **Great for retrieval + rerankers**: Windows improve reranking and answer fluency  \n",
    "\n",
    "---\n",
    "\n",
    "### 🏗️ Why This Matters in Production\n",
    "\n",
    "Imagine retrieving this chunk alone:\n",
    "\n",
    "> *\"In 2015, Google introduced TensorFlow...\"*\n",
    "\n",
    "Helpful? Yes. But isolated? Also yes.\n",
    "\n",
    "Now, with windowed context:\n",
    "\n",
    "1. *\"The rise of deep learning frameworks...\"*  \n",
    "2. *\"In 2015, Google introduced TensorFlow...\"*  \n",
    "3. *\"This marked a major shift in accessible model training...\"*\n",
    "\n",
    "✅ This enriched passage supports better reasoning — not just fact extraction.\n",
    "\n",
    "**Context Windowing bridges the gap between chunk retrieval and story comprehension.**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Where This Fits in RAG100X\n",
    "\n",
    "So far in RAG100X, you’ve seen:\n",
    "\n",
    "1. Simple PDF QA with FAISS  \n",
    "2. CSV semantic search  \n",
    "3. Blog-based web RAG  \n",
    "4. Chunk size sensitivity studies  \n",
    "5. Proposition-aware chunking  \n",
    "6. Query rewriting and transformation  \n",
    "7. HyDE (imagined documents)  \n",
    "8. HyPE (imagined questions)  \n",
    "9. CCH (contextual headers)  \n",
    "10. RSE (segment optimization)\n",
    "\n",
    "Now in **Day 11**, we zoom in on a **low-cost, high-impact enhancement**:  \n",
    "> 💡 **Context Window Enrichment gives each retrieved chunk the support it needs to shine.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1a373f",
   "metadata": {},
   "source": [
    "## 📦 Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a133f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain python-dotenv PyMuPDF langchain-community\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d5d35",
   "metadata": {},
   "source": [
    "### Path to PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad33a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc2827d",
   "metadata": {},
   "source": [
    "### 📄 Step 1: Load and Chunk the PDF Document with Index Metadata\n",
    "\n",
    "Before we can perform any kind of intelligent document retrieval, we first need to **read the PDF content** and split it into smaller units called **chunks**. Chunking helps large documents become searchable and easier to process by language models.\n",
    "\n",
    "But instead of just splitting the text blindly, we also assign each chunk a **chronological index**. This index captures the original order of chunks in the document.\n",
    "\n",
    "\n",
    "### 🧠 Why Track Chunk Indices?\n",
    "\n",
    "When we apply techniques like **context enrichment windows**, we often want to include not just one retrieved chunk but also the chunks that **surround it** — either before, after, or both.\n",
    "\n",
    "To do this effectively, we need to know the position of each chunk in the document. That’s where the index comes in.\n",
    "\n",
    "\n",
    "### 🧪 Example\n",
    "\n",
    "If your document has `10,000` characters, and you choose:\n",
    "\n",
    "- `chunk_size = 500`\n",
    "- `chunk_overlap = 100`\n",
    "\n",
    "Then the function will produce overlapping chunks like:\n",
    "\n",
    "- **Chunk 0** → characters `0–500`  \n",
    "- **Chunk 1** → characters `400–900`  \n",
    "- **Chunk 2** → characters `800–1300`  \n",
    "- ... and so on.\n",
    "\n",
    "Each chunk is stored along with:\n",
    "\n",
    "- ✅ `index` → its position in the document  \n",
    "- ✅ `text` → the full original document (for reference)\n",
    "\n",
    "This index is critical when later applying **context window enrichment**, where we retrieve not just the top chunk, but also its **neighbors** to preserve coherence and improve answer quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4017c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fitz\n",
    "\n",
    "def read_pdf_to_string(path):\n",
    "    \"\"\"\n",
    "    Read a PDF document from the specified path and return its content as a string.\n",
    "\n",
    "    Args:\n",
    "        path (str): The file path to the PDF document.\n",
    "\n",
    "    Returns:\n",
    "        str: The concatenated text content of all pages in the PDF document.\n",
    "\n",
    "    The function uses the 'fitz' library (PyMuPDF) to open the PDF document, iterate over each page,\n",
    "    extract the text content from each page, and append it to a single string.\n",
    "    \"\"\"\n",
    "    # Open the PDF document located at the specified path\n",
    "    doc = fitz.open(path)\n",
    "    content = \"\"\n",
    "    # Iterate over each page in the document\n",
    "    for page_num in range(len(doc)):\n",
    "        # Get the current page\n",
    "        page = doc[page_num]\n",
    "        # Extract the text content from the current page and append it to the content string\n",
    "        content += page.get_text()\n",
    "    return content\n",
    "\n",
    "# Read the PDF into a string\n",
    "content = read_pdf_to_string(path)\n",
    "\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def split_text_to_chunks_with_indices(text: str, chunk_size: int, chunk_overlap: int) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Splits a long text into overlapping chunks and adds metadata for RSE.\n",
    "    \n",
    "    Args:\n",
    "        text: The full document content as a string.\n",
    "        chunk_size: Number of characters per chunk.\n",
    "        chunk_overlap: Number of characters shared between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A list of LangChain Document objects with:\n",
    "            - page_content: the chunked text\n",
    "            - metadata: index (position in doc) and original text\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "\n",
    "        # Each chunk has the chunk text and metadata:\n",
    "        # - 'index' stores its position in the document\n",
    "        # - 'text' stores the full original text for later scoring\n",
    "        chunks.append(Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"index\": len(chunks),  # chronological position\n",
    "                \"text\": text           # full doc (used in reranking)\n",
    "            }\n",
    "        ))\n",
    "\n",
    "        start += chunk_size - chunk_overlap  # move forward with overlap\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81f4098",
   "metadata": {},
   "source": [
    "### Split our document accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d7883",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_size = 400\n",
    "chunk_overlap = 200\n",
    "docs = split_text_to_chunks_with_indices(content, chunks_size, chunk_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b3dc2",
   "metadata": {},
   "source": [
    "### Create vector store and retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e627d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "chunks_query_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a189ab3d",
   "metadata": {},
   "source": [
    "### Retrieve a Specific Chunk from the Vector Store by Index\n",
    "\n",
    "Once all chunks are embedded and stored in the vector store, there may be situations where you want to **retrieve a specific chunk by its original order** in the document — not by semantic similarity.\n",
    "\n",
    "This function allows us to fetch the **kᵗʰ chunk** (based on its original position, i.e., the `index` stored during chunking). It’s especially useful when reconstructing **contiguous segments** around a highly relevant chunk — for example, in **context enrichment** or **Relevant Segment Extraction (RSE)**.\n",
    "\n",
    "\n",
    "### 🔍 Why This Matters\n",
    "\n",
    "Most vector stores return **top-k semantically similar** chunks.\n",
    "\n",
    "But for techniques like RSE or when applying **sliding windows**, we often need:\n",
    "\n",
    "- A specific chunk (e.g., the one most relevant to the query)\n",
    "- Chunks surrounding it in the original document\n",
    "\n",
    "This utility function helps with that by looking up chunks using the `index` stored in the metadata.\n",
    "\n",
    "\n",
    "### ⚠️ Limitations\n",
    "\n",
    "This implementation **scans all documents** in the vector store, which is inefficient for large datasets.\n",
    "\n",
    "For production systems, it's better to:\n",
    "\n",
    "- Maintain a direct index-to-document mapping (e.g., a dictionary)\n",
    "- Or pre-load all docs with indices into memory\n",
    "\n",
    "\n",
    "### 🧪 Example Use Case\n",
    "\n",
    "Let’s say:\n",
    "\n",
    "- You already used semantic search to identify that chunk with index `42` is highly relevant.\n",
    "- Now you want to fetch its neighboring chunks (e.g., `41` and `43`) to reconstruct a more complete context window.\n",
    "\n",
    "This function lets you fetch **any such chunk on demand by index**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc5cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_by_index(vectorstore, target_index: int) -> Document:\n",
    "    \"\"\"\n",
    "    Retrieve a chunk from the vectorstore based on its index in the metadata.\n",
    "\n",
    "    Args:\n",
    "    - vectorstore: The vectorstore object containing embedded chunks.\n",
    "    - target_index: The index of the chunk to retrieve (as stored in metadata).\n",
    "\n",
    "    Returns:\n",
    "    - The Document object that matches the given index, or None if not found.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Perform a similarity search with an empty string just to retrieve all docs\n",
    "    # k = vectorstore.index.ntotal ensures all documents are returned\n",
    "    all_docs = vectorstore.similarity_search(\"\", k=vectorstore.index.ntotal)\n",
    "\n",
    "    # Loop through all documents and return the one with matching metadata index\n",
    "    for doc in all_docs:\n",
    "        if doc.metadata.get('index') == target_index:\n",
    "            return doc\n",
    "\n",
    "    # Return None if no matching chunk found\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27087b45",
   "metadata": {},
   "source": [
    "### 🔁 Retrieve with Context Window (Overlap-Aware)\n",
    "\n",
    "This function improves standard semantic retrieval by adding a few chunks before and after each relevant chunk — forming a broader context window.\n",
    "\n",
    "Instead of returning isolated chunks, we fetch neighboring ones based on their original position, and merge them carefully to avoid duplication due to chunk overlaps.\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ How it works (in short)\n",
    "\n",
    "- Run semantic search to find top relevant chunks.\n",
    "- For each, fetch a few neighbors before and after (based on `num_neighbors`).\n",
    "- Merge the text while trimming repeated regions caused by chunk overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6325477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_with_context_overlap(vectorstore, retriever, query: str, num_neighbors: int = 1, chunk_size: int = 200, chunk_overlap: int = 20) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve semantically relevant chunks and expand them with neighboring chunks\n",
    "    to build wider, more meaningful context windows.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Get the top relevant chunks based on semantic similarity\n",
    "    relevant_chunks = retriever.get_relevant_documents(query)\n",
    "    result_sequences = []\n",
    "\n",
    "    for chunk in relevant_chunks:\n",
    "        # Step 2: Get the index of the current relevant chunk\n",
    "        current_index = chunk.metadata.get('index')\n",
    "        if current_index is None:\n",
    "            continue  # Skip if no index found\n",
    "\n",
    "        # Step 3: Determine the window of surrounding chunks\n",
    "        start_index = max(0, current_index - num_neighbors)\n",
    "        end_index = current_index + num_neighbors + 1  # +1 because Python range is exclusive\n",
    "\n",
    "        # Step 4: Collect the neighbor chunks in that range\n",
    "        neighbor_chunks = []\n",
    "        for i in range(start_index, end_index):\n",
    "            neighbor_chunk = get_chunk_by_index(vectorstore, i)\n",
    "            if neighbor_chunk:\n",
    "                neighbor_chunks.append(neighbor_chunk)\n",
    "\n",
    "        # Step 5: Sort chunks to maintain their original order in the document\n",
    "        neighbor_chunks.sort(key=lambda x: x.metadata.get('index', 0))\n",
    "\n",
    "        # Step 6: Concatenate chunk contents with proper handling of overlap\n",
    "        concatenated_text = neighbor_chunks[0].page_content\n",
    "        for i in range(1, len(neighbor_chunks)):\n",
    "            current_chunk = neighbor_chunks[i].page_content\n",
    "            # Trim the overlapping region before appending\n",
    "            overlap_start = max(0, len(concatenated_text) - chunk_overlap)\n",
    "            concatenated_text = concatenated_text[:overlap_start] + current_chunk\n",
    "\n",
    "        # Step 7: Add the final enriched context window to the result\n",
    "        result_sequences.append(concatenated_text)\n",
    "\n",
    "    return result_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ee057",
   "metadata": {},
   "source": [
    "### 🔍 Comparing Regular Retrieval vs. Context-Enriched Retrieval\n",
    "\n",
    "In this step, we compare:\n",
    "\n",
    "1. **Baseline Retrieval** — returns the top semantically relevant chunk as-is.\n",
    "2. **Context-Enriched Retrieval** — expands the top chunk by adding its neighboring chunks from the original document, forming a wider and more coherent window.\n",
    "\n",
    "This helps illustrate how enriching the context can provide a fuller and more connected passage for generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91183956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline approach\n",
    "query = \"Explain the role of deforestation and fossil fuels in climate change.\"\n",
    "baseline_chunk = chunks_query_retriever.get_relevant_documents(query, k=1)\n",
    "\n",
    "# Focused context enrichment approach\n",
    "enriched_chunks = retrieve_with_context_overlap(\n",
    "    vectorstore,\n",
    "    chunks_query_retriever,\n",
    "    query,\n",
    "    num_neighbors=1,\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "print(\"Baseline Chunk:\")\n",
    "print(baseline_chunk[0].page_content)\n",
    "\n",
    "print(\"\\nEnriched Chunks:\")\n",
    "print(enriched_chunks[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3468560",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📘 Summary & Credits\n",
    "\n",
    "This notebook is based on the excellent open-source repository [RAG_Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques).  \n",
    "I referred to that work to understand how the pipeline is structured and then reimplemented the same concept in a **fully self-contained** way, but using recent models — as part of my personal learning journey.\n",
    "\n",
    "The purpose of this notebook is purely **educational**:  \n",
    "- To deepen my understanding of Retrieval-Augmented Generation systems  \n",
    "- To keep a clean, trackable log of what I’ve built and learned  \n",
    "- And to serve as a future reference for myself or others starting from scratch\n",
    "\n",
    "To support that, I’ve added clear, concise markdowns throughout the notebook — explaining *why* each package was installed, *why* each line of code exists, and *how* each component fits into the overall RAG pipeline. It’s designed to help anyone (including my future self) grasp the **how** and the **why**, not just the **what**.\n",
    "\n",
    "## 🔍 Why Use Context Enrichment in RAG?\n",
    "\n",
    "Standard top-k retrieval returns isolated chunks, which can miss important surrounding context.\n",
    "\n",
    "**Context Enrichment** addresses this by:\n",
    "- 🔗 Retrieving **neighboring chunks** around relevant ones\n",
    "- 🧱 Reconstructing **coherent local windows** from the original document\n",
    "- 🎯 Improving grounding and reducing hallucinations in generated answers\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What’s New in This Version?\n",
    "\n",
    "This implementation includes:\n",
    "\n",
    "- ➕ **Neighbor-aware chunk retrieval** by index  \n",
    "- 🔄 **Overlap-aware concatenation** to reduce repetition and preserve flow  \n",
    "- ⚙️ Plug-and-play utility that wraps around any vector store + retriever  \n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Inferences & Key Takeaways\n",
    "\n",
    "- ✅ Adding nearby chunks boosts context **without increasing retrieval k**  \n",
    "- 📚 Especially useful for **longer-form or structured content**  \n",
    "- 🔍 Retains local coherence, which helps LLMs better understand and generate  \n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 What Could Be Added Next?\n",
    "\n",
    "- 🧪 Evaluate impact on QA performance vs. baseline top-k  \n",
    "- ⚖️ Add dynamic neighbor selection based on chunk length or doc structure  \n",
    "- 🔌 Turn into a custom retriever class for easier integration with LangChain or LlamaIndex  \n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Final Word\n",
    "\n",
    "This notebook is part of my larger personal project: **RAG100x** — a challenge to build and log my journney in RAG from 0 100 in the coming months.\n",
    "\n",
    "It’s not built to impress — it’s built to **progress**.  \n",
    "Everything here is structured to enable **daily iteration**, focused experimentation, and clean documentation.\n",
    "\n",
    "If you're exploring RAG from first principles, feel free to use this as a scaffold for your own builds. And of course — check out the original repository for broader implementations and ideas.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
