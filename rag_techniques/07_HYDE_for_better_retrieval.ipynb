{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f7ec69",
   "metadata": {},
   "source": [
    "## üß† Hypothetical Document Embeddings (HyDE) for Better Retrieval | RAG100X\n",
    "\n",
    "This notebook implements **Hypothetical Document Embedding (HyDE)** ‚Äî a powerful technique to improve how Retrieval-Augmented Generation (RAG) systems understand and retrieve information from vector databases.\n",
    "\n",
    "Instead of embedding the *original* user query (which might be short, vague, or ambiguous), HyDE first uses an LLM to **generate a detailed \"hypothetical document\"** ‚Äî a short passage that *imagines* what a good answer to the query might look like. We then embed this **hypothetical document** instead of the query itself to perform similarity search.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What You‚Äôll Learn\n",
    "\n",
    "- How short queries often mismatch with longer document chunks in vector search  \n",
    "- How HyDE creates better alignment by simulating what the user wants  \n",
    "- How to generate, embed, and use hypothetical documents for retrieval  \n",
    "- When HyDE improves performance ‚Äî and when it doesn‚Äôt\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Real-world Analogy\n",
    "\n",
    "Imagine you walk into a library and ask:  \n",
    "> *\"What‚Äôs the impact of climate change?\"*\n",
    "\n",
    "The librarian might bring you a general book or article that matches the words ‚Äúclimate change‚Äù ‚Äî but it might not be what you *really* wanted.\n",
    "\n",
    "Now imagine you say:  \n",
    "> *\"I‚Äôm writing a paper on how climate change causes rising sea levels, melting glaciers, and biodiversity loss ‚Äî I need evidence from recent studies.\"*\n",
    "\n",
    "Now the librarian brings you **exactly** the right papers ‚Äî because you gave them more **semantic context**.\n",
    "\n",
    "That‚Äôs what HyDE does behind the scenes ‚Äî it turns your short query into a more descriptive version, so the vector search works more like that helpful librarian.\n",
    "\n",
    "---\n",
    "\n",
    "### üî¨ How HyDE Works Under the Hood\n",
    "\n",
    "Let‚Äôs say the original query is:\n",
    "\n",
    "> *\"Climate change effects?\"*\n",
    "\n",
    "Here's what happens in standard vs. HyDE RAG:\n",
    "\n",
    "| Step                      | Standard RAG                                      | HyDE RAG                                                       |\n",
    "|---------------------------|--------------------------------------------------|----------------------------------------------------------------|\n",
    "| 1. User Query             | Short query like ‚ÄúClimate change effects‚Äù        | Same short query                                               |\n",
    "| 2. Embedding              | Embed the query directly                         | Use LLM to generate a **pseudo-answer paragraph**              |\n",
    "| 3. Vector Search          | Retrieve documents most similar to short query   | Embed the pseudo-answer, retrieve similar chunks to that       |\n",
    "| 4. Answer Generation      | Generate based on retrieved docs                 | Same, but retrieval is more semantically aligned               |\n",
    "\n",
    "‚úÖ The generated pseudo-answer doesn‚Äôt need to be factually correct ‚Äî it just needs to be **semantically similar** to the right documents.\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è Why This Matters in Production\n",
    "\n",
    "- **Short queries are common**: Think of users on chatbots, voice assistants, or search bars.  \n",
    "- **Embedding mismatch is real**: Query embeddings often don‚Äôt ‚Äúlook like‚Äù document embeddings.  \n",
    "- **HyDE is a drop-in improvement**: You don‚Äôt need to change your vector DB or data ‚Äî just rewrite the query, embed, and search.\n",
    "\n",
    "> In high-stakes use cases (e.g., medical, legal, scientific), improving retrieval relevance can make the difference between a helpful answer and a harmful one.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Where This Fits in RAG100X\n",
    "\n",
    "So far, RAG100x has explored:\n",
    "\n",
    "1. PDF-based QA  \n",
    "2. CSV-based retrieval  \n",
    "3. Blog-based hallucination checks  \n",
    "4. Chunk-size tuning  \n",
    "5. Proposition-level chunking  \n",
    "6. Query rewriting & decomposition\n",
    "\n",
    "Now in **Day 7**, we bring it full circle ‚Äî not by transforming the question *format*, but by **imagining the answer first**. This subtle shift helps your system \"think like a user\" and fetch better context before answering.\n",
    "\n",
    "> üí° HyDE is like a smart assistant that says: *\"Let me first imagine what you're really looking for... and then search accordingly.\"*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc8462",
   "metadata": {},
   "source": [
    "## üì¶ Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e93d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71bbe402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a7ed34",
   "metadata": {},
   "source": [
    "## üìÑ Load Dataset: Understanding Climate Change\n",
    "\n",
    "This notebook uses a sample document titled **\"Understanding Climate Change\"**, stored in the `data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a17d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37df191",
   "metadata": {},
   "source": [
    "## üìÑ Encode PDF into FAISS Vectorstore\n",
    "\n",
    "This section defines a function to load a PDF, preprocess its content, split it into meaningful chunks, convert it into vector embeddings, and store it in a FAISS vectorstore for efficient semantic retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79815069",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "def replace_t_with_space(list_of_documents):\n",
    "    \"\"\"\n",
    "    Replaces all tab characters ('\\t') with spaces in the page content of each document.\n",
    "\n",
    "    Args:\n",
    "        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n",
    "\n",
    "    Returns:\n",
    "        The modified list of documents with tab characters replaced by spaces.\n",
    "    \"\"\"\n",
    "    for doc in list_of_documents:\n",
    "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Clean tab characters\n",
    "    return list_of_documents\n",
    "\n",
    "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Loads a PDF, splits it into overlapping text chunks, embeds it using OpenAI embeddings,\n",
    "    and stores the result in a FAISS vectorstore.\n",
    "\n",
    "    Args:\n",
    "        path: Path to the input PDF file.\n",
    "        chunk_size: Size of each text chunk (in characters).\n",
    "        chunk_overlap: Overlap between chunks to preserve context.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vectorstore containing the embedded PDF chunks.\n",
    "    \"\"\"\n",
    "    # Load PDF and extract raw text pages\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split the text into overlapping chunks for better context retention\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Clean each chunk (e.g., remove tab characters)\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    # Create vector embeddings and store in FAISS for fast retrieval\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
    "\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b6f4f",
   "metadata": {},
   "source": [
    "## üß† Define the HyDE Retriever Class\n",
    "\n",
    "This section defines a custom retriever class using the **Hypothetical Document Embeddings (HyDE)** technique ‚Äî an advanced retrieval strategy where, instead of embedding the user query directly, we first use an LLM to **generate a synthetic (hypothetical) document** that answers the query. We then embed this generated document and use it for similarity search.\n",
    "\n",
    "This approach often leads to better recall, especially for short, vague, or under-specified queries.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß `__init__()` ‚Äî Initialization\n",
    "\n",
    "- `files_path`: Path to the PDF file(s) that will be ingested and indexed.\n",
    "- `chunk_size`: Length (in characters) of each document chunk. Larger chunks capture more context.\n",
    "- `chunk_overlap`: Number of characters overlapping between consecutive chunks. Prevents cutting off mid-sentence.\n",
    "\n",
    "**Inside the constructor:**\n",
    "- A ChatOpenAI LLM (`gpt-4o-mini`) is initialized with `temperature=0` for deterministic output, and `max_tokens=4000` to allow long generation.\n",
    "- OpenAI‚Äôs embedding model is initialized for converting text into vector representations.\n",
    "- The input PDF is chunked and embedded into a FAISS vectorstore using `encode_pdf()`, which is a helper function (you will need to define it explicitly).\n",
    "- A prompt template is created to instruct the LLM to generate a detailed document of a specified length that answers a given query.\n",
    "- The prompt is chained with the LLM to form `hyde_chain`, which will later be invoked for generation.\n",
    "\n",
    "---\n",
    "\n",
    "### üìÑ `generate_hypothetical_document(query)`\n",
    "\n",
    "Takes a user query and uses the `hyde_chain` to generate a synthetic document that directly answers it.  \n",
    "- The document size is fixed to `chunk_size` characters to ensure it‚Äôs consistent with the size of chunks in the vectorstore.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç `retrieve(query, k=3)`\n",
    "\n",
    "- First, generates a hypothetical answer using the method above.\n",
    "- Then, performs a vector similarity search against the embedded chunks using that synthetic document.\n",
    "- Returns the top `k` most relevant documents and the generated hypothetical document.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b70a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "class HyDERetriever:\n",
    "    def __init__(self, files_path, chunk_size=500, chunk_overlap=100):\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.vectorstore = encode_pdf(files_path, chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "    \n",
    "        \n",
    "        self.hyde_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"chunk_size\"],\n",
    "            template=\"\"\"Given the question '{query}', generate a hypothetical document that directly answers this question. The document should be detailed and in-depth.\n",
    "            the document size has be exactly {chunk_size} characters.\"\"\",\n",
    "        )\n",
    "        self.hyde_chain = self.hyde_prompt | self.llm\n",
    "\n",
    "    def generate_hypothetical_document(self, query):\n",
    "        input_variables = {\"query\": query, \"chunk_size\": self.chunk_size}\n",
    "        return self.hyde_chain.invoke(input_variables).content\n",
    "\n",
    "    def retrieve(self, query, k=3):\n",
    "        hypothetical_doc = self.generate_hypothetical_document(query)\n",
    "        similar_docs = self.vectorstore.similarity_search(hypothetical_doc, k=k)\n",
    "        return similar_docs, hypothetical_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6062ba5f",
   "metadata": {},
   "source": [
    "### üì¶ `text_wrap` Utility Function\n",
    "\n",
    "This small utility function helps improve the readability of long text outputs‚Äîlike generated answers or retrieved documents‚Äîby wrapping them to a fixed width (default: 120 characters). It uses Python's built-in `textwrap` module and is especially helpful for formatting printed results in Colab or terminal environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ea8881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "def text_wrap(text, width=120):\n",
    "    \"\"\"\n",
    "    Wraps the input text to the specified width.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to wrap.\n",
    "        width (int): The width at which to wrap the text.\n",
    "\n",
    "    Returns:\n",
    "        str: The wrapped text.\n",
    "    \"\"\"\n",
    "    return textwrap.fill(text, width=width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b12f75",
   "metadata": {},
   "source": [
    "### üîç Creating and Using a HyDE Retriever Instance\n",
    "\n",
    "In this section, we instantiate the `HyDERetriever` and demonstrate how it enhances retrieval quality by generating a *hypothetical document* based on the user query. This synthetic document acts as a proxy for the user‚Äôs intent and is used to search the vector store.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- `retriever = HyDERetriever(path)`  \n",
    "  This line initializes an instance of our previously defined `HyDERetriever` class. It loads and encodes the PDF at the given path into a FAISS vectorstore using OpenAI embeddings, while also preparing the LLM-based HyDE prompt chain for generating hypothetical documents.\n",
    "\n",
    "- `test_query = \"What is the main cause of climate change?\"`  \n",
    "  We define a natural language query that a user might ask. This is the input to test our HyDE retrieval pipeline.\n",
    "\n",
    "- `results, hypothetical_doc = retriever.retrieve(test_query)`  \n",
    "  This is the key step of the HyDE method:\n",
    "  1. The model first generates a detailed hypothetical answer using the `gpt-4o-mini` LLM.\n",
    "  2. This generated document is then used as a *semantic anchor* to search the FAISS vector store.\n",
    "  3. The top-k similar chunks (default `k=3`) are retrieved based on cosine similarity between their embeddings and the embedding of the hypothetical document.\n",
    "\n",
    "- `docs_content = [doc.page_content for doc in results]`  \n",
    "  We extract just the text content of the retrieved documents, discarding metadata. This makes it easier to visualize and compare them with the hypothetical generation.\n",
    "\n",
    "- `print(text_wrap(hypothetical_doc))`  \n",
    "  Prints the hypothetical document with word wrapping for better readability. This helps developers and evaluators understand how well the synthetic document captures the intent of the query.\n",
    "\n",
    "- `show_context(docs_content)`  \n",
    "  A utility function (defined earlier in your notebook) that formats and displays the retrieved contexts. It helps visualize whether the retrieved chunks are truly relevant to the hypothetical document ‚Äî an important part of evaluating HyDE-based retrieval systems.\n",
    "\n",
    "\n",
    "\n",
    "Together, this flow demonstrates how HyDE retrieves semantically aligned information using generated knowledge as a bridge ‚Äî especially useful when the user query is short, vague, or underspecified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = HyDERetriever(path)\n",
    "\n",
    "\n",
    "# Demonstrate on a use case\n",
    "test_query = \"What is the main cause of climate change?\"\n",
    "results, hypothetical_doc = retriever.retrieve(test_query)\n",
    "\n",
    "\n",
    "#Plot the hypothetical document and the retrieved documnets\n",
    "docs_content = [doc.page_content for doc in results]\n",
    "\n",
    "print(\"hypothetical_doc:\\n\")\n",
    "print(text_wrap(hypothetical_doc)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ab623",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìò Summary & Credits\n",
    "\n",
    "This notebook is based on the excellent open-source repository [RAG_Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques).  \n",
    "I referred to that work to understand how the pipeline is structured and then reimplemented the same concept in a **fully self-contained** way, but using recent models ‚Äî as part of my personal learning journey.\n",
    "\n",
    "The purpose of this notebook is purely **educational**:  \n",
    "- To deepen my understanding of Retrieval-Augmented Generation systems  \n",
    "- To keep a clean, trackable log of what I‚Äôve built and learned  \n",
    "- And to serve as a future reference for myself or others starting from scratch\n",
    "\n",
    "To support that, I‚Äôve added clear, concise markdowns throughout the notebook ‚Äî explaining *why* each package was installed, *why* each line of code exists, and *how* each component fits into the overall RAG pipeline. It‚Äôs designed to help anyone (including my future self) grasp the **how** and the **why**, not just the **what**.\n",
    "\n",
    "\n",
    "## üîç Why Use HyDE in RAG?\n",
    "\n",
    "Traditional RAG systems rely on embedding the **user query directly** and retrieving chunks based on cosine similarity. But when queries are short, abstract, or under-specified, this can limit retrieval quality.\n",
    "\n",
    "**HyDE (Hypothetical Document Embeddings)** solves this by:\n",
    "- ‚ú® **Expanding the query into a detailed, in-domain pseudo-document**\n",
    "- üìö **Embedding that document** to improve semantic alignment with relevant chunks\n",
    "\n",
    "This leads to more **targeted, context-rich retrieval**, especially useful when user queries are vague or exploratory.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What‚Äôs New in This Version?\n",
    "\n",
    "This implementation of HyDE offers:\n",
    "\n",
    "- ü§ñ **GPT-4o-based hypothetical generation** ‚Äî Fast, smart pseudo-docs generated in real-time  \n",
    "- üß± **Self-contained architecture** ‚Äî All logic defined within the notebook, no external wrappers  \n",
    "- üìê **Customizable prompt & chunk size** ‚Äî Fine control over generation and embedding quality  \n",
    "- üîç **Plug-and-play retriever** ‚Äî HyDERetriever class is reusable across any PDF corpus  \n",
    "\n",
    "You can drop it into most LangChain or OpenAI pipelines with minimal modifications.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Inferences & Key Takeaways\n",
    "\n",
    "Running HyDE on real queries like *\"What is the main cause of climate change?\"* shows:\n",
    "\n",
    "- üìÑ The generated hypothetical doc captures nuanced concepts and keywords  \n",
    "- üì• FAISS retrievals are noticeably more relevant, even without query rewriting  \n",
    "- üí° Combining HyDE with chunk optimization leads to **significant gains in grounding**  \n",
    "\n",
    "It‚Äôs a lightweight yet powerful way to **boost RAG accuracy without retraining or new data**.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ What Could Be Added Next?\n",
    "\n",
    "For a production-grade HyDE-powered RAG:\n",
    "\n",
    "- üîÅ **Use it in parallel with traditional queries** ‚Äî Ensembling often yields the best results  \n",
    "- üß™ **Evaluate hallucination & relevancy deltas** ‚Äî Track how HyDE improves grounding  \n",
    "- üß† **Swap GPT-4o with smaller models** ‚Äî Try Claude Haiku or Phi-3 for faster inference  \n",
    "- üßµ **Chain it with query rewriting** ‚Äî Rewrite ‚Üí HyDE ‚Üí Retrieve for maximal alignment  \n",
    "- üîå **Deploy with pgvector, Chroma, or Elastic** ‚Äî Adapt vectorstore backends for scalability  \n",
    "\n",
    "---\n",
    "## üí° Final Word\n",
    "\n",
    "This notebook is part of my larger personal project: **RAG100x** ‚Äî a challenge to build and log my journney in RAG from 0 100 in the coming months.\n",
    "\n",
    "It‚Äôs not built to impress ‚Äî it‚Äôs built to **progress**.  \n",
    "Everything here is structured to enable **daily iteration**, focused experimentation, and clean documentation.\n",
    "\n",
    "If you're exploring RAG from first principles, feel free to use this as a scaffold for your own builds. And of course ‚Äî check out the original repository for broader implementations and ideas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
