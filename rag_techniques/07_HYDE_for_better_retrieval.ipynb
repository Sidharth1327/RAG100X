{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f7ec69",
   "metadata": {},
   "source": [
    "## 🧠 Hypothetical Document Embeddings (HyDE) for Better Retrieval | RAG100X\n",
    "\n",
    "This notebook implements **Hypothetical Document Embedding (HyDE)** — a powerful technique to improve how Retrieval-Augmented Generation (RAG) systems understand and retrieve information from vector databases.\n",
    "\n",
    "Instead of embedding the *original* user query (which might be short, vague, or ambiguous), HyDE first uses an LLM to **generate a detailed \"hypothetical document\"** — a short passage that *imagines* what a good answer to the query might look like. We then embed this **hypothetical document** instead of the query itself to perform similarity search.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ What You’ll Learn\n",
    "\n",
    "- How short queries often mismatch with longer document chunks in vector search  \n",
    "- How HyDE creates better alignment by simulating what the user wants  \n",
    "- How to generate, embed, and use hypothetical documents for retrieval  \n",
    "- When HyDE improves performance — and when it doesn’t\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Real-world Analogy\n",
    "\n",
    "Imagine you walk into a library and ask:  \n",
    "> *\"What’s the impact of climate change?\"*\n",
    "\n",
    "The librarian might bring you a general book or article that matches the words “climate change” — but it might not be what you *really* wanted.\n",
    "\n",
    "Now imagine you say:  \n",
    "> *\"I’m writing a paper on how climate change causes rising sea levels, melting glaciers, and biodiversity loss — I need evidence from recent studies.\"*\n",
    "\n",
    "Now the librarian brings you **exactly** the right papers — because you gave them more **semantic context**.\n",
    "\n",
    "That’s what HyDE does behind the scenes — it turns your short query into a more descriptive version, so the vector search works more like that helpful librarian.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔬 How HyDE Works Under the Hood\n",
    "\n",
    "Let’s say the original query is:\n",
    "\n",
    "> *\"Climate change effects?\"*\n",
    "\n",
    "Here's what happens in standard vs. HyDE RAG:\n",
    "\n",
    "| Step                      | Standard RAG                                      | HyDE RAG                                                       |\n",
    "|---------------------------|--------------------------------------------------|----------------------------------------------------------------|\n",
    "| 1. User Query             | Short query like “Climate change effects”        | Same short query                                               |\n",
    "| 2. Embedding              | Embed the query directly                         | Use LLM to generate a **pseudo-answer paragraph**              |\n",
    "| 3. Vector Search          | Retrieve documents most similar to short query   | Embed the pseudo-answer, retrieve similar chunks to that       |\n",
    "| 4. Answer Generation      | Generate based on retrieved docs                 | Same, but retrieval is more semantically aligned               |\n",
    "\n",
    "✅ The generated pseudo-answer doesn’t need to be factually correct — it just needs to be **semantically similar** to the right documents.\n",
    "\n",
    "---\n",
    "\n",
    "### 🏗️ Why This Matters in Production\n",
    "\n",
    "- **Short queries are common**: Think of users on chatbots, voice assistants, or search bars.  \n",
    "- **Embedding mismatch is real**: Query embeddings often don’t “look like” document embeddings.  \n",
    "- **HyDE is a drop-in improvement**: You don’t need to change your vector DB or data — just rewrite the query, embed, and search.\n",
    "\n",
    "> In high-stakes use cases (e.g., medical, legal, scientific), improving retrieval relevance can make the difference between a helpful answer and a harmful one.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Where This Fits in RAG100X\n",
    "\n",
    "So far, RAG100x has explored:\n",
    "\n",
    "1. PDF-based QA  \n",
    "2. CSV-based retrieval  \n",
    "3. Blog-based hallucination checks  \n",
    "4. Chunk-size tuning  \n",
    "5. Proposition-level chunking  \n",
    "6. Query rewriting & decomposition\n",
    "\n",
    "Now in **Day 7**, we bring it full circle — not by transforming the question *format*, but by **imagining the answer first**. This subtle shift helps your system \"think like a user\" and fetch better context before answering.\n",
    "\n",
    "> 💡 HyDE is like a smart assistant that says: *\"Let me first imagine what you're really looking for... and then search accordingly.\"*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc8462",
   "metadata": {},
   "source": [
    "## 📦 Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e93d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71bbe402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a7ed34",
   "metadata": {},
   "source": [
    "## 📄 Load Dataset: Understanding Climate Change\n",
    "\n",
    "This notebook uses a sample document titled **\"Understanding Climate Change\"**, stored in the `data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a17d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37df191",
   "metadata": {},
   "source": [
    "## 📄 Encode PDF into FAISS Vectorstore\n",
    "\n",
    "This section defines a function to load a PDF, preprocess its content, split it into meaningful chunks, convert it into vector embeddings, and store it in a FAISS vectorstore for efficient semantic retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79815069",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "def replace_t_with_space(list_of_documents):\n",
    "    \"\"\"\n",
    "    Replaces all tab characters ('\\t') with spaces in the page content of each document.\n",
    "\n",
    "    Args:\n",
    "        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n",
    "\n",
    "    Returns:\n",
    "        The modified list of documents with tab characters replaced by spaces.\n",
    "    \"\"\"\n",
    "    for doc in list_of_documents:\n",
    "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Clean tab characters\n",
    "    return list_of_documents\n",
    "\n",
    "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Loads a PDF, splits it into overlapping text chunks, embeds it using OpenAI embeddings,\n",
    "    and stores the result in a FAISS vectorstore.\n",
    "\n",
    "    Args:\n",
    "        path: Path to the input PDF file.\n",
    "        chunk_size: Size of each text chunk (in characters).\n",
    "        chunk_overlap: Overlap between chunks to preserve context.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vectorstore containing the embedded PDF chunks.\n",
    "    \"\"\"\n",
    "    # Load PDF and extract raw text pages\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split the text into overlapping chunks for better context retention\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Clean each chunk (e.g., remove tab characters)\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    # Create vector embeddings and store in FAISS for fast retrieval\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
    "\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b6f4f",
   "metadata": {},
   "source": [
    "## 🧠 Define the HyDE Retriever Class\n",
    "\n",
    "This section defines a custom retriever class using the **Hypothetical Document Embeddings (HyDE)** technique — an advanced retrieval strategy where, instead of embedding the user query directly, we first use an LLM to **generate a synthetic (hypothetical) document** that answers the query. We then embed this generated document and use it for similarity search.\n",
    "\n",
    "This approach often leads to better recall, especially for short, vague, or under-specified queries.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 `__init__()` — Initialization\n",
    "\n",
    "- `files_path`: Path to the PDF file(s) that will be ingested and indexed.\n",
    "- `chunk_size`: Length (in characters) of each document chunk. Larger chunks capture more context.\n",
    "- `chunk_overlap`: Number of characters overlapping between consecutive chunks. Prevents cutting off mid-sentence.\n",
    "\n",
    "**Inside the constructor:**\n",
    "- A ChatOpenAI LLM (`gpt-4o-mini`) is initialized with `temperature=0` for deterministic output, and `max_tokens=4000` to allow long generation.\n",
    "- OpenAI’s embedding model is initialized for converting text into vector representations.\n",
    "- The input PDF is chunked and embedded into a FAISS vectorstore using `encode_pdf()`, which is a helper function (you will need to define it explicitly).\n",
    "- A prompt template is created to instruct the LLM to generate a detailed document of a specified length that answers a given query.\n",
    "- The prompt is chained with the LLM to form `hyde_chain`, which will later be invoked for generation.\n",
    "\n",
    "---\n",
    "\n",
    "### 📄 `generate_hypothetical_document(query)`\n",
    "\n",
    "Takes a user query and uses the `hyde_chain` to generate a synthetic document that directly answers it.  \n",
    "- The document size is fixed to `chunk_size` characters to ensure it’s consistent with the size of chunks in the vectorstore.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 `retrieve(query, k=3)`\n",
    "\n",
    "- First, generates a hypothetical answer using the method above.\n",
    "- Then, performs a vector similarity search against the embedded chunks using that synthetic document.\n",
    "- Returns the top `k` most relevant documents and the generated hypothetical document.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b70a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "class HyDERetriever:\n",
    "    def __init__(self, files_path, chunk_size=500, chunk_overlap=100):\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.vectorstore = encode_pdf(files_path, chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "    \n",
    "        \n",
    "        self.hyde_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"chunk_size\"],\n",
    "            template=\"\"\"Given the question '{query}', generate a hypothetical document that directly answers this question. The document should be detailed and in-depth.\n",
    "            the document size has be exactly {chunk_size} characters.\"\"\",\n",
    "        )\n",
    "        self.hyde_chain = self.hyde_prompt | self.llm\n",
    "\n",
    "    def generate_hypothetical_document(self, query):\n",
    "        input_variables = {\"query\": query, \"chunk_size\": self.chunk_size}\n",
    "        return self.hyde_chain.invoke(input_variables).content\n",
    "\n",
    "    def retrieve(self, query, k=3):\n",
    "        hypothetical_doc = self.generate_hypothetical_document(query)\n",
    "        similar_docs = self.vectorstore.similarity_search(hypothetical_doc, k=k)\n",
    "        return similar_docs, hypothetical_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6062ba5f",
   "metadata": {},
   "source": [
    "### 📦 `text_wrap` Utility Function\n",
    "\n",
    "This small utility function helps improve the readability of long text outputs—like generated answers or retrieved documents—by wrapping them to a fixed width (default: 120 characters). It uses Python's built-in `textwrap` module and is especially helpful for formatting printed results in Colab or terminal environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ea8881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "def text_wrap(text, width=120):\n",
    "    \"\"\"\n",
    "    Wraps the input text to the specified width.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to wrap.\n",
    "        width (int): The width at which to wrap the text.\n",
    "\n",
    "    Returns:\n",
    "        str: The wrapped text.\n",
    "    \"\"\"\n",
    "    return textwrap.fill(text, width=width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b12f75",
   "metadata": {},
   "source": [
    "### 🔍 Creating and Using a HyDE Retriever Instance\n",
    "\n",
    "In this section, we instantiate the `HyDERetriever` and demonstrate how it enhances retrieval quality by generating a *hypothetical document* based on the user query. This synthetic document acts as a proxy for the user’s intent and is used to search the vector store.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- `retriever = HyDERetriever(path)`  \n",
    "  This line initializes an instance of our previously defined `HyDERetriever` class. It loads and encodes the PDF at the given path into a FAISS vectorstore using OpenAI embeddings, while also preparing the LLM-based HyDE prompt chain for generating hypothetical documents.\n",
    "\n",
    "- `test_query = \"What is the main cause of climate change?\"`  \n",
    "  We define a natural language query that a user might ask. This is the input to test our HyDE retrieval pipeline.\n",
    "\n",
    "- `results, hypothetical_doc = retriever.retrieve(test_query)`  \n",
    "  This is the key step of the HyDE method:\n",
    "  1. The model first generates a detailed hypothetical answer using the `gpt-4o-mini` LLM.\n",
    "  2. This generated document is then used as a *semantic anchor* to search the FAISS vector store.\n",
    "  3. The top-k similar chunks (default `k=3`) are retrieved based on cosine similarity between their embeddings and the embedding of the hypothetical document.\n",
    "\n",
    "- `docs_content = [doc.page_content for doc in results]`  \n",
    "  We extract just the text content of the retrieved documents, discarding metadata. This makes it easier to visualize and compare them with the hypothetical generation.\n",
    "\n",
    "- `print(text_wrap(hypothetical_doc))`  \n",
    "  Prints the hypothetical document with word wrapping for better readability. This helps developers and evaluators understand how well the synthetic document captures the intent of the query.\n",
    "\n",
    "- `show_context(docs_content)`  \n",
    "  A utility function (defined earlier in your notebook) that formats and displays the retrieved contexts. It helps visualize whether the retrieved chunks are truly relevant to the hypothetical document — an important part of evaluating HyDE-based retrieval systems.\n",
    "\n",
    "\n",
    "\n",
    "Together, this flow demonstrates how HyDE retrieves semantically aligned information using generated knowledge as a bridge — especially useful when the user query is short, vague, or underspecified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = HyDERetriever(path)\n",
    "\n",
    "\n",
    "# Demonstrate on a use case\n",
    "test_query = \"What is the main cause of climate change?\"\n",
    "results, hypothetical_doc = retriever.retrieve(test_query)\n",
    "\n",
    "\n",
    "#Plot the hypothetical document and the retrieved documnets\n",
    "docs_content = [doc.page_content for doc in results]\n",
    "\n",
    "print(\"hypothetical_doc:\\n\")\n",
    "print(text_wrap(hypothetical_doc)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ab623",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📘 Summary & Credits\n",
    "\n",
    "This notebook is based on the excellent open-source repository [RAG_Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques).  \n",
    "I referred to that work to understand how the pipeline is structured and then reimplemented the same concept in a **fully self-contained** way, but using recent models — as part of my personal learning journey.\n",
    "\n",
    "The purpose of this notebook is purely **educational**:  \n",
    "- To deepen my understanding of Retrieval-Augmented Generation systems  \n",
    "- To keep a clean, trackable log of what I’ve built and learned  \n",
    "- And to serve as a future reference for myself or others starting from scratch\n",
    "\n",
    "To support that, I’ve added clear, concise markdowns throughout the notebook — explaining *why* each package was installed, *why* each line of code exists, and *how* each component fits into the overall RAG pipeline. It’s designed to help anyone (including my future self) grasp the **how** and the **why**, not just the **what**.\n",
    "\n",
    "\n",
    "## 🔍 Why Use HyDE in RAG?\n",
    "\n",
    "Traditional RAG systems rely on embedding the **user query directly** and retrieving chunks based on cosine similarity. But when queries are short, abstract, or under-specified, this can limit retrieval quality.\n",
    "\n",
    "**HyDE (Hypothetical Document Embeddings)** solves this by:\n",
    "- ✨ **Expanding the query into a detailed, in-domain pseudo-document**\n",
    "- 📚 **Embedding that document** to improve semantic alignment with relevant chunks\n",
    "\n",
    "This leads to more **targeted, context-rich retrieval**, especially useful when user queries are vague or exploratory.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What’s New in This Version?\n",
    "\n",
    "This implementation of HyDE offers:\n",
    "\n",
    "- 🤖 **GPT-4o-based hypothetical generation** — Fast, smart pseudo-docs generated in real-time  \n",
    "- 🧱 **Self-contained architecture** — All logic defined within the notebook, no external wrappers  \n",
    "- 📐 **Customizable prompt & chunk size** — Fine control over generation and embedding quality  \n",
    "- 🔍 **Plug-and-play retriever** — HyDERetriever class is reusable across any PDF corpus  \n",
    "\n",
    "You can drop it into most LangChain or OpenAI pipelines with minimal modifications.\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Inferences & Key Takeaways\n",
    "\n",
    "Running HyDE on real queries like *\"What is the main cause of climate change?\"* shows:\n",
    "\n",
    "- 📄 The generated hypothetical doc captures nuanced concepts and keywords  \n",
    "- 📥 FAISS retrievals are noticeably more relevant, even without query rewriting  \n",
    "- 💡 Combining HyDE with chunk optimization leads to **significant gains in grounding**  \n",
    "\n",
    "It’s a lightweight yet powerful way to **boost RAG accuracy without retraining or new data**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 What Could Be Added Next?\n",
    "\n",
    "For a production-grade HyDE-powered RAG:\n",
    "\n",
    "- 🔁 **Use it in parallel with traditional queries** — Ensembling often yields the best results  \n",
    "- 🧪 **Evaluate hallucination & relevancy deltas** — Track how HyDE improves grounding  \n",
    "- 🧠 **Swap GPT-4o with smaller models** — Try Claude Haiku or Phi-3 for faster inference  \n",
    "- 🧵 **Chain it with query rewriting** — Rewrite → HyDE → Retrieve for maximal alignment  \n",
    "- 🔌 **Deploy with pgvector, Chroma, or Elastic** — Adapt vectorstore backends for scalability  \n",
    "\n",
    "---\n",
    "## 💡 Final Word\n",
    "\n",
    "This notebook is part of my larger personal project: **RAG100x** — a challenge to build and log my journney in RAG from 0 100 in the coming months.\n",
    "\n",
    "It’s not built to impress — it’s built to **progress**.  \n",
    "Everything here is structured to enable **daily iteration**, focused experimentation, and clean documentation.\n",
    "\n",
    "If you're exploring RAG from first principles, feel free to use this as a scaffold for your own builds. And of course — check out the original repository for broader implementations and ideas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
