{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5e4a53",
   "metadata": {},
   "source": [
    "## üß† **Propositional Chunking for Precision Retrieval** | **RAG100X**\n",
    "\n",
    "This notebook introduces a high-precision technique for improving RAG systems: **Propositional Chunking**. Unlike traditional methods that split documents by tokens or characters, this approach breaks content into clear, fact-based statements ‚Äî called *propositions* ‚Äî that are atomic, complete, and self-contained. These propositions are then filtered and stored in a vector database for accurate and contextually relevant retrieval.\n",
    "\n",
    "‚úÖ **Key Capabilities**  \n",
    "*This notebook builds and compares two parallel RAG pipelines:*\n",
    "\n",
    "- Token-based chunking as a baseline\n",
    "- Propositional chunking using LLMs to extract and filter factual statements\n",
    "- Quality scoring of each proposition before indexing\n",
    "- Dual vectorstores using FAISS for side-by-side comparisons\n",
    "- Retrieval and generation based on both chunk types to evaluate answer quality\n",
    "\n",
    "> üõ†Ô∏è **Why this matters in production:**  \n",
    "When building RAG for real-world apps, noisy or vague chunks can easily lead to hallucinated or off-topic answers. Propositional chunking helps you **index only the most useful and truthful units of meaning** ‚Äî giving you more precision, better grounding, and faster retrieval for QA tasks, chatbots, and document agents.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ **How This Fits into RAG100X**\n",
    "\n",
    "In this RAG100x journey so far, we‚Äôve tackled diverse use cases:\n",
    "\n",
    "1. PDF-based RAG for unstructured scanned documents  \n",
    "2. CSV-based RAG to query structured tables  \n",
    "3. Web blog-based RAG with hallucination detection  \n",
    "4. Chunk-size experiments to analyze retrieval vs. generation tradeoffs  \n",
    "\n",
    "Now, in **Day 5**, we go deeper ‚Äî exploring how **the *type* of chunk** you store in your vector DB impacts the quality of answers. Propositional chunking is one such strategy, and others like **semantic filtering**, **redundancy reduction**, or **fact-confidence scoring** can further help refine your index.\n",
    "\n",
    "> üí° This notebook isn‚Äôt just a new pipeline ‚Äî it‚Äôs a practical lesson in how to design better **retrieval units** for more trustworthy, production-ready RAG systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc0fce",
   "metadata": {},
   "source": [
    "### üì¶ Installing Core Libraries\n",
    "- **`langchain` & `langchain-community`**  \n",
    "  Provides standardized interfaces for document loaders, splitters, embedding models, vectorstores, and LLM chains ‚Äî including community-maintained integrations.\n",
    "\n",
    "- **`python-dotenv`**  \n",
    "  Helps manage API credentials securely by loading them from a `.env` file into environment variables.\n",
    "\n",
    "> We intentionally keep dependencies lightweight and modular to retain full control over the pipeline and ensure reproducibility in future experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install faiss-cpu langchain langchain-community python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0481cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLMs\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from '.env' file\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY') # For LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2344632f",
   "metadata": {},
   "source": [
    "**Test Document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dffc3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_content = \"\"\"Paul Graham's essay \"Founder Mode,\" published in September 2024, challenges conventional wisdom about scaling startups, arguing that founders should maintain their unique management style rather than adopting traditional corporate practices as their companies grow.\n",
    "Conventional Wisdom vs. Founder Mode\n",
    "The essay argues that the traditional advice given to growing companies‚Äîhiring good people and giving them autonomy‚Äîoften fails when applied to startups.\n",
    "This approach, suitable for established companies, can be detrimental to startups where the founder's vision and direct involvement are crucial. \"Founder Mode\" is presented as an emerging paradigm that is not yet fully understood or documented, contrasting with the conventional \"manager mode\" often advised by business schools and professional managers.\n",
    "Unique Founder Abilities\n",
    "Founders possess unique insights and abilities that professional managers do not, primarily because they have a deep understanding of their company's vision and culture.\n",
    "Graham suggests that founders should leverage these strengths rather than conform to traditional managerial practices. \"Founder Mode\" is an emerging paradigm that is not yet fully understood or documented, with Graham hoping that over time, it will become as well-understood as the traditional manager mode, allowing founders to maintain their unique approach even as their companies scale.\n",
    "Challenges of Scaling Startups\n",
    "As startups grow, there is a common belief that they must transition to a more structured managerial approach. However, many founders have found this transition problematic, as it often leads to a loss of the innovative and agile spirit that drove the startup's initial success.\n",
    "Brian Chesky, co-founder of Airbnb, shared his experience of being advised to run the company in a traditional managerial style, which led to poor outcomes. He eventually found success by adopting a different approach, influenced by how Steve Jobs managed Apple.\n",
    "Steve Jobs' Management Style\n",
    "Steve Jobs' management approach at Apple served as inspiration for Brian Chesky's \"Founder Mode\" at Airbnb. One notable practice was Jobs' annual retreat for the 100 most important people at Apple, regardless of their position on the organizational chart\n",
    ". This unconventional method allowed Jobs to maintain a startup-like environment even as Apple grew, fostering innovation and direct communication across hierarchical levels. Such practices emphasize the importance of founders staying deeply involved in their companies' operations, challenging the traditional notion of delegating responsibilities to professional managers as companies scale.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2664308",
   "metadata": {},
   "source": [
    "### üìö Baseline Chunking & Index Construction\n",
    "\n",
    "Before we explore propositional chunking, we begin by building a baseline RAG pipeline using **standard token-based chunking**. This helps us establish a reference point for evaluating the improvements offered by more advanced strategies.\n",
    "\n",
    "#### üîπ What This Does\n",
    "\n",
    "- **Loads a sample document** (Paul Graham‚Äôs ‚ÄúFounder Mode‚Äù essay) as a LangChain `Document`.\n",
    "- **Splits the content** using `RecursiveCharacterTextSplitter`, which breaks text into overlapping 200-token chunks ‚Äî preserving sentence boundaries as much as possible.\n",
    "- **Embeds the chunks** using the `nomic-embed-text` model via Ollama.\n",
    "- **Stores them in a FAISS vector index** for fast retrieval.\n",
    "\n",
    "#### ‚öôÔ∏è Why This Matters for Production RAG\n",
    "\n",
    "- **Token-based splitting** is widely used in simple RAG systems because it‚Äôs fast and compatible with most embedding models.\n",
    "- However, it often produces *fragmented or redundant chunks*, especially when documents don‚Äôt follow clean sentence structures.\n",
    "- This setup gives us a baseline to later compare with **propositional chunks**, which are cleaner, more meaningful, and better suited for grounding generation in facts.\n",
    "\n",
    "> üîç **Note:** Using FAISS for local vector storage keeps the retrieval system fast and self-contained ‚Äî ideal for lightweight, production-ready applications with moderate-scale document sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c1036",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build Index\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Set embeddings\n",
    "embedding_model = OllamaEmbeddings(model='nomic-embed-text:v1.5', show_progress=True)\n",
    "\n",
    "# docs\n",
    "docs_list = [Document(page_content=sample_content, metadata={\"Title\": \"Paul Graham's Founder Mode Essay\", \"Source\": \"https://www.perplexity.ai/page/paul-graham-s-founder-mode-ess-t9TCyvkqRiyMQJWsHr0fnQ\"})]\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=200, chunk_overlap=50\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2067e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding chunk ID \n",
    "for i, doc in enumerate(doc_splits):\n",
    "    doc.metadata['chunk_id'] = i+1 ### adding chunk id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603df59",
   "metadata": {},
   "source": [
    "### ‚úÇÔ∏è Generating Propositional Chunks from Raw Text\n",
    "\n",
    "Instead of splitting text into random token-based chunks, this block transforms a document into **fact-based, self-contained propositions** ‚Äî ideal for grounding answers in real, checkable information.\n",
    "\n",
    "#### üß† What This Block Does\n",
    "\n",
    "- **Defines a schema** (`GeneratePropositions`) that expects a list of factual statements (propositions) extracted from a document.\n",
    "- **Initializes a large LLM** (`llama-3.1-70b-versatile` via Groq) capable of producing structured outputs that match the schema.\n",
    "- **Adds few-shot examples** to guide the LLM with good-quality outputs ‚Äî critical for consistency across different inputs.\n",
    "- **Builds a system prompt** with 5 clear instructions for how to extract high-quality propositions:\n",
    "  - One fact per sentence\n",
    "  - No pronouns or vague references\n",
    "  - Include names, dates, context\n",
    "  - Keep it self-contained and precise\n",
    "\n",
    "> üßæ Example:  \n",
    "> From a sentence like *\"In 1969, Neil Armstrong became the first person to walk on the Moon...\"*, we generate:\n",
    "> - \"Neil Armstrong was an astronaut.\"\n",
    "> - \"The Apollo 11 mission occurred in 1969.\"\n",
    "> - \"Neil Armstrong walked on the Moon during the Apollo 11 mission.\"\n",
    "\n",
    "#### üß± Why This Matters for Production RAG\n",
    "\n",
    "- **Cleaner inputs ‚Üí better retrieval**: Semantic, factual chunks align better with user questions than arbitrary token splits.\n",
    "- **Improves grounding**: Each chunk is an atomic, verifiable claim ‚Äî ideal for truth-checking and answer justification.\n",
    "- **Modular pipeline**: Easy to swap in different LLMs, prompts, or chunking logic based on task needs or infrastructure (e.g., GPU vs Groq vs API).\n",
    "\n",
    "> üöÄ Once generated, these propositions are embedded and indexed ‚Äî forming the backbone of a **precision-first RAG system**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be41305",
   "metadata": {},
   "outputs": [],
   "source": [
    "Generate Propositions\n",
    "from typing import List\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Data model\n",
    "class GeneratePropositions(BaseModel):\n",
    "    \"\"\"List of all the propositions in a given document\"\"\"\n",
    "\n",
    "    propositions: List[str] = Field(\n",
    "        description=\"List of propositions (factual, self-contained, and concise information)\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatGroq(model=\"llama-3.1-70b-versatile\", temperature=0)\n",
    "structured_llm= llm.with_structured_output(GeneratePropositions)\n",
    "\n",
    "# Few shot prompting --- We can add more examples to make it good\n",
    "proposition_examples = [\n",
    "    {\"document\": \n",
    "        \"In 1969, Neil Armstrong became the first person to walk on the Moon during the Apollo 11 mission.\", \n",
    "     \"propositions\": \n",
    "        \"['Neil Armstrong was an astronaut.', 'Neil Armstrong walked on the Moon in 1969.', 'Neil Armstrong was the first person to walk on the Moon.', 'Neil Armstrong walked on the Moon during the Apollo 11 mission.', 'The Apollo 11 mission occurred in 1969.']\"\n",
    "    },\n",
    "]\n",
    "\n",
    "example_proposition_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{document}\"),\n",
    "        (\"ai\", \"{propositions}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_proposition_prompt,\n",
    "    examples = proposition_examples,\n",
    ")\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"Please break down the following text into simple, self-contained propositions. Ensure that each proposition meets the following criteria:\n",
    "\n",
    "    1. Express a Single Fact: Each proposition should state one specific fact or claim.\n",
    "    2. Be Understandable Without Context: The proposition should be self-contained, meaning it can be understood without needing additional context.\n",
    "    3. Use Full Names, Not Pronouns: Avoid pronouns or ambiguous references; use full entity names.\n",
    "    4. Include Relevant Dates/Qualifiers: If applicable, include necessary dates, times, and qualifiers to make the fact precise.\n",
    "    5. Contain One Subject-Predicate Relationship: Focus on a single subject and its corresponding action or attribute, without conjunctions or multiple clauses.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{document}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "proposition_generator = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d74ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing all propositions from document \n",
    "propositions = [] # Store all the propositions from the document\n",
    "\n",
    "for i in range(len(doc_splits)):\n",
    "    response = proposition_generator.invoke({\"document\": doc_splits[i].page_content}) # Creating proposition\n",
    "    for proposition in response.propositions:\n",
    "        propositions.append(Document(page_content=proposition, metadata={\"Title\": \"Paul Graham's Founder Mode Essay\", \"Source\": \"https://www.perplexity.ai/page/paul-graham-s-founder-mode-ess-t9TCyvkqRiyMQJWsHr0fnQ\", \"chunk_id\": i+1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa50f87",
   "metadata": {},
   "source": [
    "### ‚úÖ Quality Checking Generated Propositions\n",
    "\n",
    "Once we generate factual propositions from the text, it‚Äôs crucial to ensure they are **accurate, clear, complete, and concise** before indexing them for retrieval. This section sets up an automatic quality evaluation step using the LLM.\n",
    "\n",
    "#### üß† What This Code Does\n",
    "\n",
    "- **Defines a detailed grading model** (`GradePropositions`) with four categories:\n",
    "  - **Accuracy:** How well the proposition matches the original text.\n",
    "  - **Clarity:** How easy it is to understand without extra context.\n",
    "  - **Completeness:** Whether it includes all necessary details (dates, qualifiers).\n",
    "  - **Conciseness:** Whether it‚Äôs brief without losing meaning.\n",
    "\n",
    "- **Configures the LLM** (`llama-3.1-70b-versatile`) to produce structured output matching this grading model.\n",
    "\n",
    "- **Builds a clear prompt** instructing the model to rate each proposition on these criteria from 1 to 10.\n",
    "\n",
    "- **Includes an example evaluation** using the Neil Armstrong propositions, showing perfect scores for well-formed facts.\n",
    "\n",
    "> üîç This automated grading ensures only **high-quality propositions** are kept, improving the reliability of downstream retrieval and generation.\n",
    "\n",
    "#### ‚öôÔ∏è Why It Matters for Production-Ready RAG\n",
    "\n",
    "- Automatically filtering out low-quality or ambiguous propositions helps maintain **trustworthy and precise responses**.\n",
    "- The detailed multi-criteria evaluation balances completeness with conciseness ‚Äî critical for minimizing noise while preserving essential facts.\n",
    "- Integrating quality checks into the pipeline supports **scalable, maintainable RAG systems** without relying solely on manual review.\n",
    "\n",
    "> Next, we‚Äôll use these graded propositions to build a clean, high-quality vector store optimized for accurate information retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018de68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradePropositions(BaseModel):\n",
    "    \"\"\"Grade a given proposition on accuracy, clarity, completeness, and conciseness\"\"\"\n",
    "\n",
    "    accuracy: int = Field(\n",
    "        description=\"Rate from 1-10 based on how well the proposition reflects the original text.\"\n",
    "    )\n",
    "    \n",
    "    clarity: int = Field(\n",
    "        description=\"Rate from 1-10 based on how easy it is to understand the proposition without additional context.\"\n",
    "    )\n",
    "\n",
    "    completeness: int = Field(\n",
    "        description=\"Rate from 1-10 based on whether the proposition includes necessary details (e.g., dates, qualifiers).\"\n",
    "    )\n",
    "\n",
    "    conciseness: int = Field(\n",
    "        description=\"Rate from 1-10 based on whether the proposition is concise without losing important information.\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatGroq(model=\"llama-3.1-70b-versatile\", temperature=0)\n",
    "structured_llm= llm.with_structured_output(GradePropositions)\n",
    "\n",
    "# Prompt\n",
    "evaluation_prompt_template = \"\"\"\n",
    "Please evaluate the following proposition based on the criteria below:\n",
    "- **Accuracy**: Rate from 1-10 based on how well the proposition reflects the original text.\n",
    "- **Clarity**: Rate from 1-10 based on how easy it is to understand the proposition without additional context.\n",
    "- **Completeness**: Rate from 1-10 based on whether the proposition includes necessary details (e.g., dates, qualifiers).\n",
    "- **Conciseness**: Rate from 1-10 based on whether the proposition is concise without losing important information.\n",
    "\n",
    "Example:\n",
    "Docs: In 1969, Neil Armstrong became the first person to walk on the Moon during the Apollo 11 mission.\n",
    "\n",
    "Propositons_1: Neil Armstrong was an astronaut.\n",
    "Evaluation_1: \"accuracy\": 10, \"clarity\": 10, \"completeness\": 10, \"conciseness\": 10\n",
    "\n",
    "Propositons_2: Neil Armstrong walked on the Moon in 1969.\n",
    "Evaluation_3: \"accuracy\": 10, \"clarity\": 10, \"completeness\": 10, \"conciseness\": 10\n",
    "\n",
    "Propositons_3: Neil Armstrong was the first person to walk on the Moon.\n",
    "Evaluation_3: \"accuracy\": 10, \"clarity\": 10, \"completeness\": 10, \"conciseness\": 10\n",
    "\n",
    "Propositons_4: Neil Armstrong walked on the Moon during the Apollo 11 mission.\n",
    "Evaluation_4: \"accuracy\": 10, \"clarity\": 10, \"completeness\": 10, \"conciseness\": 10\n",
    "\n",
    "Propositons_5: The Apollo 11 mission occurred in 1969.\n",
    "Evaluation_5: \"accuracy\": 10, \"clarity\": 10, \"completeness\": 10, \"conciseness\": 10\n",
    "\n",
    "Format:\n",
    "Proposition: \"{proposition}\"\n",
    "Original Text: \"{original_text}\"\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", evaluation_prompt_template),\n",
    "        (\"human\", \"{proposition}, {original_text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "proposition_evaluator = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6eb078",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Evaluating and Filtering Propositions\n",
    "\n",
    "After defining how to grade propositions, this code runs the **actual evaluation loop** to filter out low-quality statements based on predefined thresholds.\n",
    "\n",
    "#### üß† What This Code Does\n",
    "\n",
    "- **Sets evaluation categories and thresholds:**  \n",
    "  We consider four key aspects ‚Äî accuracy, clarity, completeness, and conciseness ‚Äî each requiring a minimum score of 7 to pass.\n",
    "\n",
    "- **Defines `evaluate_proposition()` function:**  \n",
    "  Sends each proposition and its original chunk of text to the LLM evaluator and retrieves the detailed scores.\n",
    "\n",
    "- **Defines `passes_quality_check()` function:**  \n",
    "  Checks if all evaluation scores meet or exceed the set thresholds; only propositions passing all criteria are accepted.\n",
    "\n",
    "- **Iterates through all generated propositions:**  \n",
    "  For each, it runs evaluation, keeps the good ones, and prints out those that fail for potential manual review.\n",
    "\n",
    "> ‚ö° This filtering step ensures that **only high-quality, reliable propositions** move forward into the retrieval index ‚Äî a critical practice for production-grade RAG systems where answer precision matters.\n",
    "\n",
    "#### ‚öôÔ∏è Production Ready Insight\n",
    "\n",
    "- Automatic thresholding reduces noise and prevents misleading or incomplete facts from polluting your knowledge base.\n",
    "- Having an explicit review mechanism (print/fail logs) supports iterative improvement and auditing.\n",
    "- This step balances recall (keeping enough info) with precision (quality over quantity), enhancing downstream answer grounding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312593ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation categories and thresholds\n",
    "evaluation_categories = [\"accuracy\", \"clarity\", \"completeness\", \"conciseness\"]\n",
    "thresholds = {\"accuracy\": 7, \"clarity\": 7, \"completeness\": 7, \"conciseness\": 7}\n",
    "\n",
    "# Function to evaluate proposition\n",
    "def evaluate_proposition(proposition, original_text):\n",
    "    response = proposition_evaluator.invoke({\"proposition\": proposition, \"original_text\": original_text})\n",
    "    \n",
    "    # Parse the response to extract scores\n",
    "    scores = {\"accuracy\": response.accuracy, \"clarity\": response.clarity, \"completeness\": response.completeness, \"conciseness\": response.conciseness}  # Implement function to extract scores from the LLM response\n",
    "    return scores\n",
    "\n",
    "# Check if the proposition passes the quality check\n",
    "def passes_quality_check(scores):\n",
    "    for category, score in scores.items():\n",
    "        if score < thresholds[category]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "evaluated_propositions = [] # Store all the propositions from the document\n",
    "\n",
    "# Loop through generated propositions and evaluate them\n",
    "for idx, proposition in enumerate(propositions):\n",
    "    scores = evaluate_proposition(proposition.page_content, doc_splits[proposition.metadata['chunk_id'] - 1].page_content)\n",
    "    if passes_quality_check(scores):\n",
    "        # Proposition passes quality check, keep it\n",
    "        evaluated_propositions.append(proposition)\n",
    "    else:\n",
    "        # Proposition fails, discard or flag for further review\n",
    "        print(f\"{idx+1}) Propostion: {proposition.page_content} \\n Scores: {scores}\")\n",
    "        print(\"Fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf1104",
   "metadata": {},
   "source": [
    "### üì¶ Embedding Propositions into a Vector Store for Retrieval\n",
    "\n",
    "This step converts the **filtered, high-quality propositions** into vector embeddings and stores them in a FAISS index for efficient similarity search.\n",
    "\n",
    "#### üß† What This Code Does\n",
    "\n",
    "- **Creates a FAISS vector store** from the evaluated propositions using the Ollama embedding model.  \n",
    "  This transforms each proposition into a dense numerical vector representing its semantic meaning.\n",
    "\n",
    "- **Sets up a retriever** on the vector store configured for similarity search, returning the top 4 most relevant propositions per query.\n",
    "\n",
    "> üîç Using FAISS enables fast, scalable retrieval of relevant facts during query time, essential for real-time RAG applications.\n",
    "\n",
    "#### ‚öôÔ∏è Production Ready Insight\n",
    "\n",
    "- Vector stores like FAISS are lightweight and easy to integrate, perfect for moderate-sized datasets in production.\n",
    "- Embedding propositions (small, factual chunks) rather than larger text blocks improves retrieval precision and answer grounding.\n",
    "- Configurable retrieval parameters (like `k=4`) let you balance between recall and response conciseness depending on application needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d48caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to vectorstore\n",
    "vectorstore_propositions = FAISS.from_documents(evaluated_propositions, embedding_model)\n",
    "retriever_propositions = vectorstore_propositions.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={'k': 4}, # number of documents to retrieve\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fa32ba",
   "metadata": {},
   "source": [
    "### üîç Querying the Proposition-Based Retriever\n",
    "\n",
    "Here, we test the retriever by issuing a natural language query:\n",
    "\n",
    "- **Query:** Who inspired Brian Chesky‚Äôs \"Founder Mode\" management approach at Airbnb?  \n",
    "- **Action:** The retriever searches the proposition vector store for the top relevant factual statements related to the query.\n",
    "\n",
    "> This demonstrates how granular, proposition-based retrieval can surface precise facts to answer targeted questions ‚Äî a key strength for production-grade RAG systems aiming for accuracy and transparency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00816df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who's management approach served as inspiartion for Brian Chesky's \\\"Founder Mode\\\" at Airbnb?\"\n",
    "res_proposition = retriever_propositions.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints the chunk ID and content of the retrived ones\n",
    "for i, r in enumerate(res_proposition):\n",
    "    print(f\"{i+1}) Content: {r.page_content} --- Chunk_id: {r.metadata['chunk_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf284fd",
   "metadata": {},
   "source": [
    "### ‚öñÔ∏è Comparing with Larger Chunk-Based Retrieval\n",
    "\n",
    "To understand the benefits of proposition chunking, we build a second retriever using **larger, token-based chunks**:\n",
    "\n",
    "- **Vector Store:** Embeds and indexes the original 200-token chunks (`doc_splits`) instead of fine-grained propositions.\n",
    "- **Retriever:** Configured similarly to retrieve the top 4 relevant chunks based on similarity to the query.\n",
    "\n",
    "> This setup lets us directly compare **granular proposition retrieval** versus **broader chunk retrieval**‚Äîkey for evaluating precision, context richness, and practical utility in production RAG workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de9af90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to vectorstore_larger_\n",
    "vectorstore_larger = FAISS.from_documents(doc_splits, embedding_model)\n",
    "retriever_larger = vectorstore_larger.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={'k': 4}, # number of documents to retrieve\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0073c74e",
   "metadata": {},
   "source": [
    "### üîç Retrieving Results from Larger Chunk Retriever\n",
    "\n",
    "We run the same query through the larger chunk-based retriever to fetch relevant document chunks.\n",
    "\n",
    "This allows us to compare the quality and relevance of results returned by the broader chunk retrieval approach versus the more precise proposition-based retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712da5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res_larger = retriever_larger.invoke(query)\n",
    "\n",
    "# Prints the chunk ID and content of the retrived ones\n",
    "\n",
    "for i, r in enumerate(res_larger):\n",
    "    print(f\"{i+1}) Content: {r.page_content} --- Chunk_id: {r.metadata['chunk_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f98f476",
   "metadata": {},
   "source": [
    "### üß™ Test Query 1: Understanding the Essay's Theme\n",
    "\n",
    "We test both retrievers with a broad question:  \n",
    "**\"What is the essay 'Founder Mode' about?\"**\n",
    "\n",
    "- **`retriever_propositions.invoke()`** queries the proposition-based index to get precise, fact-based snippets.  \n",
    "- **`retriever_larger.invoke()`** queries the larger chunk index for broader context.\n",
    "\n",
    "The output prints the retrieved content along with their chunk IDs to analyze which method provides clearer and more focused answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67944f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_query_1 = \"what is the essay \\\"Founder Mode\\\" about?\"\n",
    "res_proposition = retriever_propositions.invoke(test_query_1)\n",
    "res_larger = retriever_larger.invoke(test_query_1)\n",
    "\n",
    "# Prints the chunk ID and content of the retrived ones from propositional chunking \n",
    "for i, r in enumerate(res_proposition):\n",
    "    print(f\"{i+1}) Content: {r.page_content} --- Chunk_id: {r.metadata['chunk_id']}\")\n",
    "\n",
    "\n",
    "# Prints the chunk ID and content of the retrived ones from large chunks\n",
    "for i, r in enumerate(res_larger):\n",
    "    print(f\"{i+1}) Content: {r.page_content} --- Chunk_id: {r.metadata['chunk_id']}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad8a8ae",
   "metadata": {},
   "source": [
    "### üß™ Test Query 2: Specific Fact Retrieval\n",
    "\n",
    "This test asks a focused factual question:  \n",
    "**\"Who is the co-founder of Airbnb?\"**\n",
    "\n",
    "- The proposition-based retriever aims to return concise, targeted facts about Airbnb's co-founder.  \n",
    "- The larger chunk retriever provides broader context that may include the answer but with more surrounding information.\n",
    "\n",
    "Printing results with chunk IDs helps us compare the precision and relevance of both retrieval approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fa9e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query_2 = \"who is the co-founder of Airbnb?\"\n",
    "res_proposition = retriever_propositions.invoke(test_query_2)\n",
    "res_larger = retriever_larger.invoke(test_query_2)\n",
    "\n",
    "# Prints the chunk ID and content of the retrived ones from propositional chunking \n",
    "for i, r in enumerate(res_proposition):\n",
    "    print(f\"{i+1}) Content: {r.page_content} --- Chunk_id: {r.metadata['chunk_id']}\")\n",
    "\n",
    "\n",
    "# Prints the chunk ID and content of the retrived ones from large chunks\n",
    "for i, r in enumerate(res_larger):\n",
    "    print(f\"{i+1}) Content: {r.page_content} --- Chunk_id: {r.metadata['chunk_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fe3db5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìò Summary & Credits\n",
    "\n",
    "This notebook is based on the excellent open-source repository [RAG_Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques).  \n",
    "I referred to that work to understand how the pipeline is structured and then reimplemented the same concept in a **fully self-contained** way, but using recent models ‚Äî as part of my personal learning journey.\n",
    "\n",
    "The purpose of this notebook is purely **educational**:  \n",
    "- To deepen my understanding of Retrieval-Augmented Generation systems  \n",
    "- To keep a clean, trackable log of what I‚Äôve built and learned  \n",
    "- And to serve as a future reference for myself or others starting from scratch\n",
    "\n",
    "To support that, I‚Äôve added clear, concise markdowns throughout the notebook ‚Äî explaining *why* each package was installed, *why* each line of code exists, and *how* each component fits into the overall RAG pipeline. It‚Äôs designed to help anyone (including my future self) grasp the **how** and the **why**, not just the **what**.\n",
    "\n",
    "\n",
    "In this notebook, we explored a specialized Retrieval-Augmented Generation (RAG) technique called **propositional chunking** ‚Äî breaking down documents into atomic, factual propositions rather than relying solely on larger text chunks.\n",
    "\n",
    "### What We Did\n",
    "\n",
    "- Split a complex essay into **smaller, self-contained propositions** that capture precise facts.  \n",
    "- Used an LLM to **generate** these propositions from standard token chunks.  \n",
    "- Implemented a **quality check** step where each proposition is rated on accuracy, clarity, completeness, and conciseness, ensuring only high-quality data is embedded.  \n",
    "- Embedded these validated propositions into a **FAISS vectorstore** for fast similarity search.  \n",
    "- Compared retrieval results against a traditional baseline using **larger chunks** from the same document.  \n",
    "- Ran targeted queries to evaluate which method yields more precise and relevant answers.\n",
    "\n",
    "### Why This Matters for Production-Ready RAG\n",
    "\n",
    "- **Fine-Grained Retrieval:** Propositional chunking delivers highly focused answers by isolating factual nuggets, reducing noise from unrelated context.  \n",
    "- **Quality Assurance:** The proposition quality evaluation step is critical in production to avoid hallucinations and ensure grounded, reliable responses.  \n",
    "- **Trade-offs in Context:** While propositions improve precision and efficiency, they may lose broader context or narrative flow, which is sometimes necessary for complex queries.  \n",
    "- **Modularity:** This method can be combined with traditional chunking or other retrieval strategies (hybrid retrieval) to balance specificity and context coverage.  \n",
    "- **Scalability:** Smaller chunks mean larger vector indexes but faster relevance ranking, which can be optimized with index sharding or approximate nearest neighbor search in production.  \n",
    "\n",
    "---\n",
    "\n",
    "## üìä Results & Comparison\n",
    "\n",
    "| Aspect                | Proposition-Based Retrieval                   | Simple Chunk Retrieval                         |\n",
    "|-----------------------|----------------------------------------------|-----------------------------------------------|\n",
    "| **Precision in Response**  | High: Delivers focused and direct answers.    | Medium: Provides more context but some noise. |\n",
    "| **Clarity and Brevity**    | High: Clear, concise, no fluff.               | Medium: More verbose, can overwhelm.          |\n",
    "| **Contextual Richness**    | Low: Focused on isolated facts, less context. | High: Retains broader context and details.    |\n",
    "| **Comprehensiveness**      | Low: May miss supplementary info.             | High: More complete coverage of document.     |\n",
    "| **Narrative Flow**         | Medium: Can feel fragmented.                    | High: Maintains logical flow and coherence.   |\n",
    "| **Information Overload**   | Low: Minimal extraneous information.           | High: Risk of too much info in results.        |\n",
    "| **Use Case Suitability**   | Best for quick, factual queries.                | Best for complex or exploratory queries.       |\n",
    "| **Efficiency**             | High: Faster, focused retrieval.                | Medium: More processing needed to sift data.  |\n",
    "| **Specificity**            | High: Very targeted results.                     | Medium: Broader but less precise.              |\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Inference\n",
    "\n",
    "Propositional chunking shines in scenarios requiring **precise, trustworthy answers** ‚Äî ideal for fact-checking, compliance, and Q&A systems where brevity and accuracy matter most. For tasks demanding richer narrative or context-aware insights (e.g., summarization, exploratory research), traditional chunking remains valuable.\n",
    "\n",
    "In production, combining both approaches within a hybrid retrieval system can maximize coverage and precision, balancing user needs with system performance.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps for Production RAG\n",
    "\n",
    "- Implement **source attribution** to trace answers back to specific propositions or chunks.  \n",
    "- Add **confidence scores** for retrieval and answer faithfulness.  \n",
    "- Explore **multi-hop reasoning** over chained propositions.  \n",
    "- Build **interactive UI** for real-time querying and feedback.  \n",
    "- Integrate **hybrid retrieval** combining propositional and chunk-based indexes.  \n",
    "\n",
    "This notebook offers a practical foundation for building precise, reliable RAG pipelines using propositional chunking ‚Äî a promising step toward robust, production-ready AI systems.\n",
    "\n",
    "## üí° Final Word\n",
    "\n",
    "This notebook is part of my larger personal project: **RAG100x** ‚Äî a challenge to build and log my journney in RAG from 0 100 in the coming months.\n",
    "\n",
    "It‚Äôs not built to impress ‚Äî it‚Äôs built to **progress**.  \n",
    "Everything here is structured to enable **daily iteration**, focused experimentation, and clean documentation.\n",
    "\n",
    "If you're exploring RAG from first principles, feel free to use this as a scaffold for your own builds. And of course ‚Äî check out the original repository for broader implementations and ideas.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
