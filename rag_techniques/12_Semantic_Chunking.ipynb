{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fabd3f0",
   "metadata": {},
   "source": [
    "## ğŸ§  Semantic Chunking for Coherent Retrieval | RAG100X\n",
    "\n",
    "This notebook implements **Semantic Chunking** â€” a technique that intelligently splits documents at **meaningful breakpoints**, resulting in **context-preserving chunks** for downstream LLM retrieval and generation.\n",
    "\n",
    "Unlike traditional chunking (e.g., every 500 characters), semantic chunking leverages **embedding-based similarity** to detect where ideas naturally begin and end â€” leading to more coherent text segments.\n",
    "\n",
    "The result? Chunks that reflect complete thoughts, better retrieval alignment, and answers that actually make sense in context.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… What Youâ€™ll Learn\n",
    "\n",
    "- Why fixed-size chunking often breaks up sentences mid-thought  \n",
    "- How to use LangChainâ€™s `SemanticChunker` to split by semantic shifts  \n",
    "- The difference between percentile, standard deviation, and IQR breakpoints  \n",
    "- How OpenAI embeddings guide both chunking and retrieval  \n",
    "- Why semantically coherent chunks boost RAG performance  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Real-world Analogy\n",
    "\n",
    "Imagine splitting a novel into chunks every 200 words â€” youâ€™d likely cut across scenes, dialogues, or even mid-sentence.  \n",
    "Now imagine chunking where the **story naturally pauses** â€” at paragraph ends, chapter breaks, or scene changes.\n",
    "\n",
    "âœ… **Semantic Chunking works like that â€” using embeddings to find conceptual \"jumps\" between sentences and split there.**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¬ How Semantic Chunking Works Under the Hood\n",
    "\n",
    "Letâ€™s break it down step-by-step:\n",
    "\n",
    "| Step                        | What Happens                                                                 |\n",
    "|-----------------------------|------------------------------------------------------------------------------|\n",
    "| 1. PDF Extraction           | The full PDF is converted into a raw text string                            |\n",
    "| 2. Sentence Embeddings      | Each sentence is converted into an OpenAI embedding vector                   |\n",
    "| 3. Semantic Distance Calc   | The system computes differences between adjacent sentence embeddings         |\n",
    "| 4. Breakpoint Detection     | When a distance exceeds a threshold (e.g., 90th percentile), a split occurs  |\n",
    "| 5. Chunk Assembly           | Sentences between breakpoints are grouped into coherent chunks               |\n",
    "| 6. Embedding & Indexing     | Chunks are embedded again and stored in a FAISS vector store                 |\n",
    "| 7. Query & Retrieval        | Queries are matched against these semantic chunks using vector similarity    |\n",
    "\n",
    "ğŸ§  This process ensures each chunk contains a **complete idea**, reducing the risk of retrieving incomplete or confusing fragments.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª Why This Works So Well\n",
    "\n",
    "- âœ‚ï¸ **No arbitrary cutoffs**: Chunks are split where semantic shifts actually happen  \n",
    "- ğŸ§© **Improved coherence**: Each chunk is likely to contain a full concept or argument  \n",
    "- ğŸ” **Better retrieval**: Embedding-aligned chunks match query intent more precisely  \n",
    "- ğŸ¤– **LLM-friendly**: Language models handle semantically rich text better than fragmented ones  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ—ï¸ Why This Matters in Production\n",
    "\n",
    "Fixed-size chunks often lead to:\n",
    "\n",
    "- âŒ Mid-sentence breaks  \n",
    "- âŒ Fragmented ideas  \n",
    "- âŒ Poor retrieval quality  \n",
    "\n",
    "But with semantic chunking, imagine retrieving this instead:\n",
    "\n",
    "> *â€œClimate change is primarily driven by greenhouse gas emissions, especially COâ‚‚ from fossil fuels. This has been confirmed by decades of atmospheric research.â€*\n",
    "\n",
    "âœ… **Complete idea. Self-contained. Easy to ground an answer.**\n",
    "\n",
    "Thatâ€™s the power of semantic chunking â€” giving your RAG system **intelligent building blocks** to work with.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”„ Where This Fits in RAG100X\n",
    "\n",
    "In earlier projects, youâ€™ve explored:\n",
    "\n",
    "1. Vanilla chunking + retrieval (PDF, CSV, WEB Articles)\n",
    "2. Chunk size sensitivity studies \n",
    "3. Propositional Chunking \n",
    "4. Query Enhancement (Query Transformations, HyDE, HyPE)  \n",
    "5. Context Enrichment (RSE, CCH, CEW)  \n",
    "\n",
    "\n",
    "Now in **Day 12**, we introduce **semantic awareness at the chunking level**:  \n",
    "> ğŸ’¡ **Let the *ideas* decide where to split â€” not the character count.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232d21ad",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7cb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain-experimental langchain-openai python-dotenv\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea19b38",
   "metadata": {},
   "source": [
    "### Define the Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726c2fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f51033",
   "metadata": {},
   "source": [
    "### Read PDF to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edca6499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def read_pdf_to_string(path):\n",
    "    \"\"\"\n",
    "    Read a PDF document from the specified path and return its content as a string.\n",
    "\n",
    "    Args:\n",
    "        path (str): The file path to the PDF document.\n",
    "\n",
    "    Returns:\n",
    "        str: The concatenated text content of all pages in the PDF document.\n",
    "\n",
    "    The function uses the 'fitz' library (PyMuPDF) to open the PDF document, iterate over each page,\n",
    "    extract the text content from each page, and append it to a single string.\n",
    "    \"\"\"\n",
    "    # Open the PDF document located at the specified path\n",
    "    doc = fitz.open(path)\n",
    "    content = \"\"\n",
    "    # Iterate over each page in the document\n",
    "    for page_num in range(len(doc)):\n",
    "        # Get the current page\n",
    "        page = doc[page_num]\n",
    "        # Extract the text content from the current page and append it to the content string\n",
    "        content += page.get_text()\n",
    "    return content\n",
    "\n",
    "content = read_pdf_to_string(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d11dd",
   "metadata": {},
   "source": [
    "### âš™ï¸ Breakpoint Strategies in Semantic Chunking\n",
    "\n",
    "At the heart of semantic chunking lies the decision of **where to split**. Unlike arbitrary character counts, this method relies on **semantic distances** â€” the difference in meaning between neighboring sentences, measured using their embedding vectors.\n",
    "\n",
    "LangChainâ€™s `SemanticChunker` offers three strategies for identifying these semantic breakpoints:\n",
    "\n",
    "- `'percentile'`: Calculate all pairwise sentence distances. Split where the distance exceeds the Xth percentile (e.g., top 10% most abrupt meaning shifts).\n",
    "- `'standard_deviation'`: Identify sentence pairs whose semantic gap is X standard deviations above the mean.\n",
    "- `'interquartile'`: Use the interquartile range (IQR) of distances. Sentences beyond this typical range are considered \"semantic jumps\".\n",
    "\n",
    "**What does this mean?**  \n",
    "â¡ï¸ *â€œSplit the text where semantic difference between adjacent sentences falls in the top 10% of all differences.â€*\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ§  What Happens Under the Hood?\n",
    "\n",
    "1. **Sentence Splitting**  \n",
    "   The document is first broken down into individual sentences.\n",
    "\n",
    "2. **Embedding Sentences**  \n",
    "   Each sentence is converted into a high-dimensional vector using OpenAIâ€™s embedding model.\n",
    "\n",
    "3. **Compute Semantic Distance**  \n",
    "   For every pair of adjacent sentences, a semantic distance (typically cosine distance) is calculated.  \n",
    "   These distances measure how much the meaning changes from one sentence to the next.\n",
    "\n",
    "4. **Detect Breakpoints**  \n",
    "   The splitter identifies points where the semantic shift is in the **top 10%** of all measured shifts.\n",
    "\n",
    "5. **Chunk Formation**  \n",
    "   Sentences between these breakpoints are grouped into coherent chunks, preserving the flow of ideas.\n",
    "\n",
    "\n",
    "### ğŸ“š Simple Analogy\n",
    "\n",
    "Think of reading a textbook:\n",
    "\n",
    "- Sometimes, ideas flow smoothly â€” one sentence builds on the last.\n",
    "- Other times, thereâ€™s a **clear topic shift** â€” like a new section, concept, or argument.\n",
    "\n",
    "**Semantic distances act like a \"topic change detector.\"**\n",
    "\n",
    "When a big enough shift occurs, itâ€™s as if the algorithm says:\n",
    "\n",
    "> ğŸ›‘ *\"Okay, time to start a new paragraph.\"*\n",
    "\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Semantic chunking uses **vector math instead of intuition** to create chunks:\n",
    "\n",
    "- **Fewer, broader chunks:** Use a higher threshold (e.g., 95th percentile).\n",
    "- **Finer-grained splits:** Use a lower threshold (e.g., 75th percentile).\n",
    "\n",
    "\n",
    "### ğŸ¯ The Goal\n",
    "\n",
    "> **Group sentences into logically self-contained chunks that preserve meaning â€” ideal for retrieval and generation in RAG systems.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(OpenAIEmbeddings(), breakpoint_threshold_type='percentile', breakpoint_threshold_amount=90) # chose which embeddings and breakpoint type and threshold to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c19fbd",
   "metadata": {},
   "source": [
    "### Split original text to semantic chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f7f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = text_splitter.create_documents([content])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d50ea90",
   "metadata": {},
   "source": [
    "### Create vector store and retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec13e08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "chunks_query_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0dcfbf",
   "metadata": {},
   "source": [
    "\n",
    "### Test the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d6b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context_per_question(question, chunks_query_retriever):\n",
    "    \"\"\"\n",
    "    Retrieves relevant context and unique URLs for a given question using the chunks query retriever.\n",
    "\n",
    "    Args:\n",
    "        question: The question for which to retrieve context and URLs.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - A string with the concatenated content of relevant documents.\n",
    "        - A list of unique URLs from the metadata of the relevant documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve relevant documents for the given question\n",
    "    docs = chunks_query_retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Concatenate document content\n",
    "    # context = \" \".join(doc.page_content for doc in docs)\n",
    "    context = [doc.page_content for doc in docs]\n",
    "\n",
    "    return context\n",
    "\n",
    "def show_context(context):\n",
    "    \"\"\"\n",
    "    Display the contents of the provided context list.\n",
    "\n",
    "    Args:\n",
    "        context (list): A list of context items to be displayed.\n",
    "\n",
    "    Prints each context item in the list with a heading indicating its position.\n",
    "    \"\"\"\n",
    "    for i, c in enumerate(context):\n",
    "        print(f\"Context {i + 1}:\")\n",
    "        print(c)\n",
    "        print(\"\\n\")\n",
    "\n",
    "test_query = \"What is the main cause of climate change?\"\n",
    "context = retrieve_context_per_question(test_query, chunks_query_retriever)\n",
    "show_context(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33927f85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“˜ Summary & Credits\n",
    "\n",
    "This notebook is based on the excellent open-source repository [RAG_Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques).  \n",
    "I referred to that work to understand how the pipeline is structured and then reimplemented the same concept in a **fully self-contained** way, but using recent models â€” as part of my personal learning journey.\n",
    "\n",
    "The purpose of this notebook is purely **educational**:  \n",
    "- To deepen my understanding of Retrieval-Augmented Generation systems  \n",
    "- To keep a clean, trackable log of what Iâ€™ve built and learned  \n",
    "- And to serve as a future reference for myself or others starting from scratch\n",
    "\n",
    "To support that, Iâ€™ve added clear, concise markdowns throughout the notebook â€” explaining *why* each package was installed, *why* each line of code exists, and *how* each component fits into the overall RAG pipeline. Itâ€™s designed to help anyone (including my future self) grasp the **how** and the **why**, not just the **what**.\n",
    "\n",
    "## ğŸ” Why Use Semantic Chunking in RAG?\n",
    "\n",
    "Standard chunking methods (like fixed character or token windows) often split text mid-sentence or mid-idea â€” hurting both retrieval relevance and answer quality.\n",
    "\n",
    "**Semantic Chunking** addresses this by:\n",
    "- âœ‚ï¸ Splitting text at **natural meaning shifts**, not arbitrary lengths  \n",
    "- ğŸ§  Creating **coherent, self-contained chunks** that represent full ideas  \n",
    "- ğŸ¯ Improving retrieval grounding and reducing fragmented or ambiguous answers  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Whatâ€™s New in This Version?\n",
    "\n",
    "This implementation includes:\n",
    "\n",
    "- ğŸ“ **Embedding-based sentence segmentation** using OpenAI embeddings  \n",
    "- ğŸ” Flexible **breakpoint strategies**: percentile, standard deviation, or IQR  \n",
    "- âš™ï¸ Clean integration with FAISS + LangChainâ€™s `SemanticChunker`  \n",
    "- ğŸ“¦ End-to-end pipeline from PDF â†’ semantic chunks â†’ retriever  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ Inferences & Key Takeaways\n",
    "\n",
    "- âœ… Meaning-aware chunking improves **semantic alignment** during retrieval  \n",
    "- ğŸ”— Better suited for **complex or structured documents** like reports, research papers, legal content  \n",
    "- ğŸ¤– Enhances downstream LLM performance by providing **richer, more coherent context**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ What Could Be Added Next?\n",
    "\n",
    "- ğŸ§ª Compare against fixed-size chunking with relevance + faithfulness metrics  \n",
    "- ğŸ“Š Visualize sentence embedding distances and breakpoint distributions  \n",
    "- ğŸ”„ Extend to support **hybrid chunking**: semantic + metadata-based + structure-aware  \n",
    "- ğŸ§© Integrate with rerankers or long-context models for improved QA\n",
    "\n",
    "---\n",
    "## ğŸ’¡ Final Word\n",
    "\n",
    "This notebook is part of my larger personal project: **RAG100x** â€” a challenge to build and log my journney in RAG from 0 100 in the coming months.\n",
    "\n",
    "Itâ€™s not built to impress â€” itâ€™s built to **progress**.  \n",
    "Everything here is structured to enable **daily iteration**, focused experimentation, and clean documentation.\n",
    "\n",
    "If you're exploring RAG from first principles, feel free to use this as a scaffold for your own builds. And of course â€” check out the original repository for broader implementations and ideas.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
