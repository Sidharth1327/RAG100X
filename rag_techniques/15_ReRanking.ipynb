{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f3ca79",
   "metadata": {},
   "source": [
    "## ğŸ” Reranking for Smarter Retrieval | RAG100X\n",
    "\n",
    "This notebook implements **Reranking** â€” a powerful technique that refines the output of initial retrieval by **reassessing document relevance** using more sophisticated models like **LLMs** or **Cross-Encoders**.\n",
    "\n",
    "Instead of blindly trusting the first batch of documents retrieved via vector similarity, reranking helps prioritize **truly relevant** content â€” filtering out noise and boosting the overall quality of inputs sent to the LLM.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… What Youâ€™ll Learn\n",
    "\n",
    "- Why initial vector retrieval may return weak or generic matches  \n",
    "- How to apply **LLM-based scoring** to rate document relevance  \n",
    "- How to use **Cross-Encoder models** for fine-grained query-doc comparisons  \n",
    "- How to create custom retrievers that plug reranking into LangChain workflows  \n",
    "- When and why reranking improves final answer quality in RAG systems  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Real-world Analogy\n",
    "\n",
    "Imagine you ask a librarian for books on \"climate change impacts on biodiversity\":\n",
    "\n",
    "> ğŸ” The librarian gives you 15 random books about climate, nature, ecosystemsâ€¦  \n",
    "> ğŸ§  You skim each one and pick the **3 most useful** for your question  \n",
    "\n",
    "âœ… Thatâ€™s reranking â€” **you refine the initial list based on deeper understanding**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  How Reranking Works Under the Hood\n",
    "\n",
    "| Step                 | What Happens                                                                 |\n",
    "|----------------------|------------------------------------------------------------------------------|\n",
    "| 1. Initial Retrieval | Use a vector store to fetch top-K relevant chunks                            |\n",
    "| 2. Pairing           | Form (query, document) pairs for relevance assessment                        |\n",
    "| 3. Scoring           | Use LLM or Cross-Encoder to assign relevance scores to each pair             |\n",
    "| 4. Sorting           | Rank documents by score in descending order                                  |\n",
    "| 5. Selection         | Keep only the top-N most relevant documents                                  |\n",
    "\n",
    "ğŸ’¡ This ensures that only the **most meaningful context** reaches the LLM for generation.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Why Reranking Makes a Difference\n",
    "\n",
    "- ğŸ§  **Deeper understanding**: Goes beyond cosine similarity to semantic alignment  \n",
    "- ğŸ” **Fine-grained filtering**: Discards off-topic or tangential results  \n",
    "- âš™ï¸ **Model flexibility**: Works with both general LLMs and pretrained relevance models  \n",
    "- ğŸ¯ **Precision boost**: Especially helpful for nuanced, domain-heavy queries  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ—ï¸ Why This Matters in Production\n",
    "\n",
    "Basic vector search might rank irrelevant results high just due to word overlap:\n",
    "\n",
    "> Query: â€œWhat are the biodiversity threats from climate change?â€  \n",
    "> Initial result: â€œThe weather in France is unpredictable in spring.â€ âŒ  \n",
    "\n",
    "Reranking ensures:\n",
    "\n",
    "âœ… â€œWarming leads to habitat shifts and species loss in fragile ecosystems.â€  \n",
    "gets prioritized â€” **leading to better answers**.\n",
    "\n",
    "This is critical for:\n",
    "\n",
    "- Knowledge-dense domains (e.g. healthcare, policy, research)  \n",
    "- Use cases where precision matters (e.g. chatbots, summarizers, assistants)  \n",
    "- Scenarios with large corpora and high retrieval noise  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”„ Where This Fits in RAG100X\n",
    "\n",
    "In earlier projects, youâ€™ve built:\n",
    "\n",
    "1. Basic vector retrieval from various sources  \n",
    "2. Chunking and compression enhancements  \n",
    "3. Retrieval enrichment via query reformulation, CEW, HyDE, etc.  \n",
    "\n",
    "Now, in **Day 15**, you take it further:\n",
    "\n",
    "> ğŸ’¡ **Donâ€™t just retrieve â€” rethink what you keep.**  \n",
    "> Reranking boosts quality at the final mile of retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579104bf",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaaafb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required packages\n",
    "!pip install langchain langchain-openai python-dotenv sentence-transformers\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.docstore.document import Document\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c006bf",
   "metadata": {},
   "source": [
    "### Define the document's path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee3cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2624200",
   "metadata": {},
   "source": [
    "### Create a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47062a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# Cleaning the document\n",
    "\n",
    "def replace_t_with_space(list_of_documents):\n",
    "    \"\"\"\n",
    "    Replaces all tab characters ('\\t') with spaces in the page content of each document\n",
    "\n",
    "    Args:\n",
    "        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n",
    "\n",
    "    Returns:\n",
    "        The modified list of documents with tab characters replaced by spaces.\n",
    "    \"\"\"\n",
    "\n",
    "    for doc in list_of_documents:\n",
    "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
    "    return list_of_documents\n",
    "\n",
    "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded book content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load PDF documents\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    # Create embeddings and vector store\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "vectorstore = encode_pdf(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81861374",
   "metadata": {},
   "source": [
    "### ğŸ§  Building a Custom LLM-Based Reranker\n",
    "\n",
    "In this section, we implement a **custom reranking function** using a **Large Language Model (LLM)**.  \n",
    "The idea is simple: rather than trusting the initial similarity-based retrieval, we ask an LLM to **rate the relevance** of each document **on a scale of 1 to 10** â€” then sort and select the top ones.\n",
    "\n",
    "\n",
    "### ğŸ’¡ Why Use an LLM to Score Relevance?\n",
    "\n",
    "Traditional retrieval systems (like FAISS) rank documents based on **vector similarity**.  \n",
    "But this doesnâ€™t always capture the **true meaning or intent** of a query. For example:\n",
    "\n",
    "> Query: *\"What are the threats to biodiversity due to climate change?\"*  \n",
    "> Vector result: *\"Climate policies affect economic behavior...\"* âŒ (related but not relevant)\n",
    "\n",
    "By contrast, an LLM can **understand context**, **reason about meaning**, and score based on **true relevance**.\n",
    "\n",
    "### âš™ï¸ How This Reranker Works Step-by-Step\n",
    "\n",
    "| Step | What Happens |\n",
    "|------|---------------|\n",
    "| 1ï¸âƒ£   | A prompt template is created to ask the LLM: \"Rate how relevant this document is to the query (1â€“10)\" |\n",
    "| 2ï¸âƒ£   | For every document in the initial results, a query-doc pair is fed to the LLM |\n",
    "| 3ï¸âƒ£   | The LLM returns a structured `relevance_score` |\n",
    "| 4ï¸âƒ£   | All scores are collected and sorted from high to low |\n",
    "| 5ï¸âƒ£   | The top N most relevant documents are returned |\n",
    "\n",
    "This method turns the LLM into a **relevance judge** â€” helping your system choose the most meaningful evidence before generating answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ef91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Define the expected structure of the LLM's output (must return a float score)\n",
    "class RatingScore(BaseModel):\n",
    "    relevance_score: float = Field(..., description=\"The relevance score of a document to a query.\")\n",
    "\n",
    "# Main reranking function using an LLM\n",
    "def rerank_documents(query: str, docs: List[Document], top_n: int = 3) -> List[Document]:\n",
    "    \n",
    "    # Define the prompt that asks the LLM to score each document\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"query\", \"doc\"],\n",
    "        template=\"\"\"On a scale of 1-10, rate the relevance of the following document to the query. \n",
    "        Consider the specific context and intent of the query, not just keyword matches.\n",
    "        \n",
    "        Query: {query}\n",
    "        Document: {doc}\n",
    "        Relevance Score:\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Initialize the LLM (GPT-4o in this case) with structured output\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
    "    llm_chain = prompt_template | llm.with_structured_output(RatingScore)\n",
    "    \n",
    "    scored_docs = []\n",
    "    \n",
    "    # Go through each retrieved document and get a relevance score from the LLM\n",
    "    for doc in docs:\n",
    "        input_data = {\"query\": query, \"doc\": doc.page_content}\n",
    "        score = llm_chain.invoke(input_data).relevance_score\n",
    "        \n",
    "        # If LLM returns something non-numeric, assign 0 as fallback\n",
    "        try:\n",
    "            score = float(score)\n",
    "        except ValueError:\n",
    "            score = 0\n",
    "        \n",
    "        # Store the document with its score\n",
    "        scored_docs.append((doc, score))\n",
    "    \n",
    "    # Sort documents by score in descending order\n",
    "    reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return only the top N highest-scoring documents\n",
    "    return [doc for doc, _ in reranked_docs[:top_n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7e93d1",
   "metadata": {},
   "source": [
    "### ğŸ” Comparing Initial vs. Reranked Documents â€” Example Run\n",
    "\n",
    "Now that weâ€™ve defined our custom reranking function, letâ€™s see it in action using a **real-world query**:\n",
    "\n",
    "> **Query:** \"What are the impacts of climate change on biodiversity?\"\n",
    "\n",
    "We'll compare:\n",
    "\n",
    "- ğŸ”¹ The **top 3 results** from the initial vector similarity search  \n",
    "- ğŸ”¸ The **top reranked documents** after applying the LLM-based scorer\n",
    "\n",
    "This gives you a clear picture of how reranking boosts retrieval quality by filtering out noise and emphasizing true relevance.\n",
    "\n",
    "\n",
    "### ğŸ§ª Whatâ€™s Happening Here?\n",
    "\n",
    "| Step | What We Do |\n",
    "|------|------------|\n",
    "| 1ï¸âƒ£   | Use `vectorstore.similarity_search()` to retrieve top 15 initial matches  \n",
    "| 2ï¸âƒ£   | Pass them to `rerank_documents()` to get relevance scores via the LLM  \n",
    "| 3ï¸âƒ£   | Sort and select the top N (default = 3) most relevant results  \n",
    "| 4ï¸âƒ£   | Print both sets for visual comparison  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7120177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a real-world query\n",
    "query = \"What are the impacts of climate change on biodiversity?\"\n",
    "\n",
    "# Step 1: Retrieve documents using vector similarity\n",
    "initial_docs = vectorstore.similarity_search(query, k=15)\n",
    "\n",
    "# Step 2: Rerank those documents using the LLM-based scorer\n",
    "reranked_docs = rerank_documents(query, initial_docs)\n",
    "\n",
    "# Step 3: Print the first few from the original results\n",
    "print(\"Top initial documents:\")\n",
    "for i, doc in enumerate(initial_docs[:3]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Show first 200 characters for brevity\n",
    "\n",
    "# Step 4: Print the reranked results\n",
    "print(f\"\\nQuery: {query}\\n\")\n",
    "print(\"Top reranked documents:\")\n",
    "for i, doc in enumerate(reranked_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952c31f0",
   "metadata": {},
   "source": [
    "### ğŸ§  Wrapping Reranking into a Custom Retriever\n",
    "\n",
    "Now that weâ€™ve built and tested our LLM-based reranker, letâ€™s integrate it **seamlessly into a LangChain pipeline** by creating a **custom retriever**.\n",
    "\n",
    "This allows us to use reranking as a drop-in replacement for any standard retriever â€” making it easier to plug into `RetrievalQA` or other chains.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ§© Why Build a Custom Retriever?\n",
    "\n",
    "LangChainâ€™s retrieval pipeline expects any retriever to implement a method called `get_relevant_documents(query)`.  \n",
    "To use our reranking logic with LangChain, we need to:\n",
    "\n",
    "1. Start with an initial set of documents from a vector store  \n",
    "2. Rerank them using our LLM scoring logic  \n",
    "3. Return the top-N most relevant ones  \n",
    "\n",
    "ğŸ’¡ So we wrap this logic inside a class that inherits from `BaseRetriever`.\n",
    "\n",
    "\n",
    "### âš™ï¸ What Happens Under the Hood?\n",
    "\n",
    "| Step | What It Does |\n",
    "|------|---------------|\n",
    "| 1ï¸âƒ£   | `CustomRetriever` fetches 30 documents via standard vector search  \n",
    "| 2ï¸âƒ£   | It passes them into `rerank_documents()` to get LLM relevance scores  \n",
    "| 3ï¸âƒ£   | It returns only the **top N** reranked documents (e.g. top 2 or 3)  \n",
    "| 4ï¸âƒ£   | These documents are used by the LLM to generate the final answer  \n",
    "\n",
    "This setup is now compatible with any LangChain workflow â€” just like a default retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935278c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "# Step 1: Create a custom retriever class that applies reranking\n",
    "class CustomRetriever(BaseRetriever, BaseModel):\n",
    "    \n",
    "    # Pass in a vectorstore (e.g., FAISS or Chroma)\n",
    "    vectorstore: Any = Field(description=\"Vector store for initial retrieval\")\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True  # Allows storing non-standard types like FAISS\n",
    "\n",
    "    def get_relevant_documents(self, query: str, num_docs=2) -> List[Document]:\n",
    "        # Retrieve more documents than we need (for reranking to work well)\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=30)\n",
    "        \n",
    "        # Apply the reranking logic and return the top N\n",
    "        return rerank_documents(query, initial_docs, top_n=num_docs)\n",
    "\n",
    "# Step 2: Instantiate the custom retriever with our vectorstore\n",
    "custom_retriever = CustomRetriever(vectorstore=vectorstore)\n",
    "\n",
    "# Step 3: Create the language model (for answer generation, not reranking)\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "\n",
    "# Step 4: Connect everything into a RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Simple chain type that concatenates documents\n",
    "    retriever=custom_retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bdcb52",
   "metadata": {},
   "source": [
    "### Example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ed5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(f\"\\nQuestion: {query}\")\n",
    "print(f\"Answer: {result['result']}\")\n",
    "print(\"\\nRelevant source documents:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2725fda",
   "metadata": {},
   "source": [
    "### ğŸ§  Method 2: Reranking with Cross-Encoder Models\n",
    "\n",
    "In this section, weâ€™ll use a **Cross-Encoder model** for reranking retrieved documents â€” an **alternative to LLM-based scoring**.\n",
    "\n",
    "Cross-Encoders are especially useful in production setups when:\n",
    "- You want faster and cheaper reranking (compared to GPT)\n",
    "- You care about precise similarity scoring\n",
    "- You donâ€™t need deep reasoning â€” just **textual relevance**\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ¤– What Is a Cross-Encoder?\n",
    "\n",
    "Unlike **bi-encoders** (used in vector search) that encode query and doc separately,  \n",
    "**Cross-Encoders jointly encode a (query, document) pair** and predict a **relevance score**.\n",
    "\n",
    "This makes them **slower but more accurate** at matching.\n",
    "\n",
    "We use the model:  \n",
    "> `cross-encoder/ms-marco-MiniLM-L-6-v2` â€” a lightweight and fast option.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ§± Under the Hood: How It Works\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 1ï¸âƒ£   | Retrieve top `k` documents using a vector search  \n",
    "| 2ï¸âƒ£   | Form `(query, document)` pairs for each result  \n",
    "| 3ï¸âƒ£   | Run them through the cross-encoder model  \n",
    "| 4ï¸âƒ£   | Rank documents by predicted relevance scores  \n",
    "| 5ï¸âƒ£   | Return the top-N documents for the final answer  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7128352",
   "metadata": {},
   "source": [
    "### âš™ï¸ Why We Only Implemented the Synchronous Retriever\n",
    "\n",
    "#### 1. `CrossEncoder.predict()` is a Synchronous, CPU/GPU-bound Operation\n",
    "\n",
    "- The `predict()` method from the ğŸ¤— `sentence-transformers` library is a **blocking**, CPU/GPU-intensive operation.\n",
    "- It does **not have an async version** like `await cross_encoder.apredict(...)` â€” such a method doesnâ€™t exist.\n",
    "- This is because the model runs **locally on your machine** and performs **matrix computations**, not I/O operations (e.g., API calls, file reads, or DB queries).\n",
    "- As a result, using async would **not provide any performance gain** â€” it would still block the CPU while executing.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. LangChain Supports Both Sync and Async Retriever Interfaces\n",
    "\n",
    "LangChain provides two interfaces for custom retrievers:\n",
    "\n",
    "- `get_relevant_documents(query: str)` â†’ **Synchronous**\n",
    "- `aget_relevant_documents(query: str)` â†’ **Asynchronous**\n",
    "\n",
    "In this implementation, we only define the **synchronous version** because:\n",
    "\n",
    "- Both the **vectorstore** and **CrossEncoder model** are synchronous.\n",
    "- Async is only helpful when dealing with **I/O-bound tasks**, such as:\n",
    "  - Calling OpenAI or Groq LLM APIs\n",
    "  - Querying remote vector databases\n",
    "  - Fetching data from external APIs or files\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  TL;DR\n",
    "\n",
    "- We donâ€™t implement `aget_relevant_documents()` because there's **no async benefit** â€” our reranking pipeline is **fully local and CPU-bound**.\n",
    "- If needed, you *can* add an async wrapper using `asyncio.run_in_executor(...)`, but thatâ€™s mainly useful in **web server contexts** (like FastAPI apps).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load a pre-trained cross-encoder model for relevance scoring\n",
    "# This model takes (query, document) pairs and outputs a score indicating semantic similarity\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "\n",
    "class CrossEncoderRetriever(BaseRetriever, BaseModel):\n",
    "    \"\"\"\n",
    "    A custom retriever that performs initial dense retrieval from a vectorstore,\n",
    "    then reranks the results using a cross-encoder model.\n",
    "    \"\"\"\n",
    "\n",
    "    vectorstore: Any = Field(description=\"Vector store backend (e.g., FAISS, Chroma) for initial similarity search\")\n",
    "    cross_encoder: Any = Field(description=\"Cross-encoder model that scores (query, doc) pairs for reranking\")\n",
    "    \n",
    "    k: int = Field(default=5, description=\"Number of documents to retrieve in initial search\")\n",
    "    rerank_top_k: int = Field(default=3, description=\"Number of top documents to keep after reranking\")\n",
    "\n",
    "    class Config:\n",
    "        # Allows the use of complex, non-standard types like FAISS or SentenceTransformer\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Main retrieval method:\n",
    "        - Retrieves top-k documents using vector similarity (bi-encoder)\n",
    "        - Reranks those documents based on semantic matching using a cross-encoder\n",
    "        \"\"\"\n",
    "        \n",
    "        # STEP 1: Retrieve initial set of documents using vector similarity\n",
    "        # Here we use a standard vector search to quickly find approximate matches\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=self.k)\n",
    "\n",
    "        # STEP 2: Create input pairs (query, document) for each result\n",
    "        # The cross-encoder requires both query and doc as input together to assess their match\n",
    "        pairs = [[query, doc.page_content] for doc in initial_docs]\n",
    "\n",
    "        # STEP 3: Score each (query, doc) pair using the cross-encoder\n",
    "        # The model returns a float score for each pair representing relevance\n",
    "        scores = self.cross_encoder.predict(pairs)\n",
    "\n",
    "        # ğŸªœ STEP 4: Zip the documents with their scores and sort by score (descending)\n",
    "        # Higher scores indicate better semantic alignment with the query\n",
    "        scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # STEP 5: Return top N reranked documents\n",
    "        # These are the most relevant documents as judged by the cross-encoder\n",
    "        return [doc for doc, _ in scored_docs[:self.rerank_top_k]]\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # This retriever currently only supports synchronous execution\n",
    "        raise NotImplementedError(\"Async retrieval not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513024fa",
   "metadata": {},
   "source": [
    "### ğŸ”§ Creating and Using the CrossEncoderRetriever\n",
    "Now that weâ€™ve defined the `CrossEncoderRetriever` class, letâ€™s put it into action by:\n",
    "\n",
    "1. **Instantiating the retriever**\n",
    "2. **Creating a `RetrievalQA` chain with a GPT-4o LLM**\n",
    "3. **Running a sample query to see reranked results**\n",
    "\n",
    "####  Instantiate `CrossEncoderRetriever`\n",
    "\n",
    "We create an instance of our custom retriever by passing:\n",
    "- A `vectorstore` for initial retrieval\n",
    "- A `cross_encoder` model for reranking\n",
    "- `k=10` â€” Retrieve 10 documents initially from the vectorstore\n",
    "- `rerank_top_k=5` â€” Rerank them and keep only the top 5\n",
    "\n",
    "This modular design makes our retriever easily pluggable into LangChain pipelines.\n",
    "\n",
    "\n",
    "\n",
    "#### Set Up the LLM (GPT-4o)\n",
    "\n",
    "We use OpenAI's **GPT-4o** model via the `ChatOpenAI` wrapper with a temperature of 0 for deterministic output.\n",
    "\n",
    "\n",
    "#### Build the `RetrievalQA` Chain\n",
    "\n",
    "The chain uses:\n",
    "\n",
    "- The **CrossEncoderRetriever** for smarter, reranked retrieval\n",
    "- The **\"stuff\"** chain type (concatenates all retrieved documents)\n",
    "- `return_source_documents=True` so we can see what evidence was used\n",
    "\n",
    "\n",
    "#### Run an Example Query\n",
    "\n",
    "We use a relevant query:\n",
    "> *\"What are the impacts of climate change on biodiversity?\"*\n",
    "\n",
    "The model uses reranked results to answer and also shows the top documents that influenced its response.\n",
    "\n",
    "### ğŸ§ª Example Output\n",
    "This helps verify that the reranking logic meaningfully improves the relevance of context used for LLM generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c33dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the CrossEncoderRetriever\n",
    "cross_encoder_retriever = CrossEncoderRetriever(\n",
    "    vectorstore=vectorstore,               # Initial retrieval using vector similarity\n",
    "    cross_encoder=cross_encoder,           # Reranker model (cross-encoder)\n",
    "    k=10,                                  # Retrieve top 10 candidates\n",
    "    rerank_top_k=5                         # Keep top 5 after reranking\n",
    ")\n",
    "\n",
    "# Set up the LLM using GPT-4o (deterministic output with temperature=0)\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "# Create the RetrievalQA pipeline using LangChain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",                        # Concatenates all retrieved docs into one prompt\n",
    "    retriever=cross_encoder_retriever,         # Our custom reranking retriever\n",
    "    return_source_documents=True               # Enables access to source documents\n",
    ")\n",
    "\n",
    "# Run an example query\n",
    "query = \"What are the impacts of climate change on biodiversity?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nQuestion: {query}\")\n",
    "print(f\"Answer: {result['result']}\")\n",
    "\n",
    "# Display the top reranked source documents\n",
    "print(\"\\nRelevant source documents:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Show first 200 characters of each\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb09502",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“˜ Summary & Credits\n",
    "\n",
    "This notebook is based on the excellent open-source repository [RAG_Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques).  \n",
    "I referred to that work to understand how the pipeline is structured and then reimplemented the same concept in a **fully self-contained** way, but using recent models â€” as part of my personal learning journey.\n",
    "\n",
    "The purpose of this notebook is purely **educational**:  \n",
    "- To deepen my understanding of Retrieval-Augmented Generation systems  \n",
    "- To keep a clean, trackable log of what Iâ€™ve built and learned  \n",
    "- And to serve as a future reference for myself or others starting from scratch\n",
    "\n",
    "To support that, Iâ€™ve added clear, concise markdowns throughout the notebook â€” explaining *why* each package was installed, *why* each line of code exists, and *how* each component fits into the overall RAG pipeline. Itâ€™s designed to help anyone (including my future self) grasp the **how** and the **why**, not just the **what**.\n",
    "\n",
    "## ğŸ” Why Use Cross-Encoder Reranking in RAG?\n",
    "\n",
    "Vector similarity retrieval is fast but imprecise â€” it only considers embeddings, not the actual query-token alignment.\n",
    "\n",
    "**Cross-Encoder Reranking** upgrades your RAG pipeline by:\n",
    "- ğŸ§  Performing **deep pairwise comparison** between query and each candidate document  \n",
    "- ğŸ“ Scoring based on **actual token-level attention**, not just vector proximity  \n",
    "- ğŸ§¹ Filtering out semantically close but contextually irrelevant chunks  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Whatâ€™s New in This Version?\n",
    "\n",
    "This implementation includes:\n",
    "\n",
    "- ğŸ¤– A custom `CrossEncoderRetriever` compatible with LangChain  \n",
    "- ğŸ” Initial recall with **FAISS vectorstore**, followed by CrossEncoder reranking  \n",
    "- ğŸ§® **Local reranking** using `sentence-transformers` without external API calls  \n",
    "- âš™ï¸ Configurable `k` and `rerank_top_k` for control over retrieval and precision  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ Inferences & Key Takeaways\n",
    "\n",
    "- âœ… Cross-encoders significantly **improve retrieval precision** by modeling query-doc interactions  \n",
    "- ğŸ§  Best used when you need **fewer but highly relevant chunks**  \n",
    "- âš¡ Reranking is CPU/GPU-bound â€” great for small-to-mid-sized corpora  \n",
    "- ğŸ§© Easily pluggable into LangChainâ€™s `RetrievalQA` pipeline  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ What Could Be Added Next?\n",
    "\n",
    "- ğŸ“Š Evaluate the impact of reranking using faithfulness and relevancy metrics  \n",
    "- ğŸ§ª Compare performance vs. embedding-only retrievers like HyDE or HyPE  \n",
    "- ğŸ”— Try **Hybrid Fusion + Reranking** for improved recall *and* precision  \n",
    "- ğŸŒ Serve CrossEncoder via a lightweight API for use in web apps or agents  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ Final Word\n",
    "\n",
    "This notebook is part of my larger personal project: **RAG100x** â€” a challenge to build and log my journney in RAG from 0 100 in the coming months.\n",
    "\n",
    "Itâ€™s not built to impress â€” itâ€™s built to **progress**.  \n",
    "Everything here is structured to enable **daily iteration**, focused experimentation, and clean documentation.\n",
    "\n",
    "If you're exploring RAG from first principles, feel free to use this as a scaffold for your own builds. And of course â€” check out the original repository for broader implementations and ideas."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
