{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f61131eb",
   "metadata": {},
   "source": [
    "## 📊 **Evaluating Chunk Size Impact in RAG Pipelines** | **RAG100x**\n",
    "\n",
    "This notebook introduces an automated system to **evaluate the quality of RAG pipelines** by analyzing how different **chunk sizes** affect performance — a key step for building **production-ready** RAG systems.\n",
    "\n",
    "Unlike the previous three notebooks where we focused on **building RAG applications** from various data sources (PDFs, CSVs, web articles), here we take a step further by asking:  \n",
    "**“How do we know if our RAG system is actually working well?”**\n",
    "\n",
    "We use **LlamaIndex**, **OpenAI embeddings**, and structured **GPT-based evaluators** to:\n",
    "\n",
    "- ✅ Benchmark RAG output across chunk sizes\n",
    "- ✅ Score for faithfulness (hallucination detection)\n",
    "- ✅ Measure relevance and answer quality\n",
    "- ✅ Track retrieval + generation latency\n",
    "\n",
    "This transition from building to **evaluating and optimizing** makes the system more **robust, testable, and production-aware**.\n",
    "\n",
    "> 🛠️ All components are implemented inline for clarity, transparency, and customization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d44303",
   "metadata": {},
   "source": [
    "### 📦 Installing Core Libraries\n",
    "- **`langchain` & `langchain-community`**  \n",
    "  Provides standardized interfaces for document loaders, splitters, embedding models, vectorstores, and LLM chains — including community-maintained integrations.\n",
    "\n",
    "- **`python-dotenv`**  \n",
    "  Helps manage API credentials securely by loading them from a `.env` file into environment variables.\n",
    "\n",
    "> We intentionally keep dependencies lightweight and modular to retain full control over the pipeline and ensure reproducibility in future experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43335c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install llama-index openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a1b874",
   "metadata": {},
   "source": [
    "## 🧰 Getting Things Ready (Libraries + API Setup)\n",
    "\n",
    "Before we start building anything, we need to load a few tools and make sure our environment is set up properly. Here's what each part does:\n",
    "\n",
    "- **`nest_asyncio`**  \n",
    "  This is a helper that allows certain parts of Python to work better inside Jupyter notebooks or Google Colab. Some tools (like LLMs or web loaders) use background tasks called *\"async\"*, and this makes sure they don't crash or conflict in a notebook environment.\n",
    "\n",
    "- **`dotenv` and `os`**  \n",
    "  Instead of typing your OpenAI API key directly in the notebook (which is risky), we keep it in a hidden `.env` file. These two libraries help us *load that key securely* so we can access OpenAI’s models safely.\n",
    "\n",
    "- **`llama_index`**  \n",
    "  This is a powerful tool that helps connect documents (like blog posts) with large language models (LLMs). We use it to:\n",
    "  - Read and organize the text from files  \n",
    "  - Break long text into smaller pieces (chunks)  \n",
    "  - Build a searchable index  \n",
    "  - Generate questions and evaluate the answers  \n",
    "  - Customize how the model should respond using special prompts  \n",
    "\n",
    "- **`openai`**  \n",
    "  This library talks directly to OpenAI’s models like GPT-3.5 and GPT-4. We use it behind the scenes for generating answers and checking how accurate or relevant they are.\n",
    "\n",
    "- **`load_dotenv()` + `openai.api_key = os.getenv(...)`**  \n",
    "  These lines *load your secret API key* from the `.env` file and make it available to the notebook. Without this, none of the OpenAI features would work.\n",
    "\n",
    "> 🟢 In short: This block makes sure everything is set up correctly — the tools are imported, your OpenAI key is safely loaded, and the notebook is ready to run RAG workflows with LlamaIndex and OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc676233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import random\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "import openai\n",
    "import time\n",
    "import os\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d9509e",
   "metadata": {},
   "source": [
    "## 📂 Load the Source Documents\n",
    "\n",
    "- **What this does**  \n",
    "  This step loads all the source documents that our RAG system will use. These documents are stored in a `data/` folder and could include text, JSON, or other readable formats.\n",
    "\n",
    "- **`data_dir = \"../data\"`**  \n",
    "  This specifies the directory containing the documents we want to load. In this case, it's a folder named `data` located one level above the notebook.\n",
    "\n",
    "- **`SimpleDirectoryReader(data_dir).load_data()`**  \n",
    "  A utility from **LlamaIndex** that automatically reads all supported files from the specified directory and loads them as a list of text documents. This makes it easy to bring in external knowledge without manual parsing.\n",
    "\n",
    "> 📎 This is the entry point for your knowledge base. Once loaded, these documents can be embedded, indexed, and retrieved during question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "documents = SimpleDirectoryReader(data_dir).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a07465d",
   "metadata": {},
   "source": [
    "## 🧪 Generate Evaluation Questions Automatically\n",
    "\n",
    "- **What this does**  \n",
    "  Instead of writing evaluation questions manually, we use LlamaIndex's built-in `DatasetGenerator` to auto-generate realistic questions from our documents. This is useful for benchmarking how well the RAG pipeline performs across different queries.\n",
    "\n",
    "- **`eval_documents = documents[0:20]`**  \n",
    "  Selects the first 20 documents from our dataset for generating questions. Using a subset keeps evaluation lightweight while still being meaningful.\n",
    "\n",
    "- **`DatasetGenerator.from_documents(...)`**  \n",
    "  This initializes a generator that scans the given documents and prepares to create evaluation-ready questions.\n",
    "\n",
    "- **`generate_questions_from_nodes()`**  \n",
    "  Actually generates question prompts based on the selected documents' content. These questions simulate real user queries grounded in the source material.\n",
    "\n",
    "- **`random.sample(..., num_eval_questions)`**  \n",
    "  From the full list of generated questions, we randomly pick a specified number (`num_eval_questions = 25`) to create a diverse and manageable evaluation set.\n",
    "\n",
    "> 🎯 This step helps us create a synthetic QA benchmark for evaluating our RAG system’s accuracy and grounding — a critical practice before deploying any real-world RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32282e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_questions = 25\n",
    "\n",
    "eval_documents = documents[0:20]\n",
    "data_generator = DatasetGenerator.from_documents(eval_documents)\n",
    "eval_questions = data_generator.generate_questions_from_nodes()\n",
    "k_eval_questions = random.sample(eval_questions, num_eval_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232725d9",
   "metadata": {},
   "source": [
    "## 📏 Define Evaluation Metrics using GPT-4\n",
    "\n",
    "To ensure our RAG system is not only retrieving relevant information but also generating factually grounded responses, we use two evaluation metrics:\n",
    "\n",
    "### ✅ 1. **Faithfulness Evaluation**\n",
    "- Checks **if the generated answer is directly supported by the retrieved context**.\n",
    "- Based on GPT-4’s reasoning, it gives a **binary judgment**: `YES` (faithful) or `NO` (hallucinated).\n",
    "- We define a **custom prompt template** with clear examples to guide the LLM. This ensures more **robust and interpretable evaluations** compared to a generic instruction.\n",
    "\n",
    "> 🎯 Why customize the faithfulness prompt?  \n",
    "Default evaluators may be too vague. By explicitly stating how GPT-4 should evaluate grounding and providing examples (e.g., about apple pie or Paris), we make evaluations **more consistent and reliable**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 2. **Relevancy Evaluation**\n",
    "- Judges whether the retrieved context is **relevant to the input query**.\n",
    "- Also uses GPT-4 (`gpt-4o`) to score how well the documents match the user question.\n",
    "- Helps measure **retriever performance** independently of the final answer quality.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Model & Settings\n",
    "- We configure all evaluations to use **`gpt-4o`**, a faster and more cost-efficient variant of GPT-4 with strong reasoning skills.\n",
    "- This model is injected into LlamaIndex’s global `Settings.llm`, ensuring all evaluators default to this model without re-specifying it each time.\n",
    "\n",
    "> 🛠️ These evaluators help **quantify hallucinations and retrieval mismatch**, making the RAG system more trustworthy and production-ready.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddcb1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use GPT-4 for evaluating the responses\n",
    "gpt4 = OpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "# Set appropriate settings for the LLM\n",
    "Settings.llm = gpt4\n",
    "\n",
    "# Define Faithfulness Evaluators which are based on GPT-4\n",
    "faithfulness_gpt4 = FaithfulnessEvaluator()\n",
    "\n",
    "faithfulness_new_prompt_template = PromptTemplate(\"\"\" Please tell if a given piece of information is directly supported by the context.\n",
    "    You need to answer with either YES or NO.\n",
    "    Answer YES if any part of the context explicitly supports the information, even if most of the context is unrelated. If the context does not explicitly support the information, answer NO. Some examples are provided below.\n",
    "\n",
    "    Information: Apple pie is generally double-crusted.\n",
    "    Context: An apple pie is a fruit pie in which the principal filling ingredient is apples.\n",
    "    Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard, or cheddar cheese.\n",
    "    It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
    "    Answer: YES\n",
    "\n",
    "    Information: Apple pies taste bad.\n",
    "    Context: An apple pie is a fruit pie in which the principal filling ingredient is apples.\n",
    "    Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard, or cheddar cheese.\n",
    "    It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
    "    Answer: NO\n",
    "\n",
    "    Information: Paris is the capital of France.\n",
    "    Context: This document describes a day trip in Paris. You will visit famous landmarks like the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\n",
    "    Answer: NO\n",
    "\n",
    "    Information: {query_str}\n",
    "    Context: {context_str}\n",
    "    Answer:\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "faithfulness_gpt4.update_prompts({\"your_prompt_key\": faithfulness_new_prompt_template}) # Update the prompts dictionary with the new prompt template\n",
    "\n",
    "# Define Relevancy Evaluators which are based on GPT-4\n",
    "relevancy_gpt4 = RelevancyEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb1782b",
   "metadata": {},
   "source": [
    "## 🧪 Evaluate Chunk Size Impact on RAG Accuracy & Speed\n",
    "\n",
    "To assess how **different chunk sizes** affect both **retrieval quality** and **response latency**, we define a custom evaluation function.\n",
    "\n",
    "### 🔧 Function: `evaluate_response_time_and_accuracy(chunk_size, eval_questions)`\n",
    "\n",
    "This function:\n",
    "- Runs a RAG pipeline using the specified `chunk_size`.\n",
    "- Uses **GPT-3.5-Turbo** for generating answers (cost-efficient and fast).\n",
    "- Uses **GPT-4** to evaluate each answer for:\n",
    "  - **Faithfulness** (is it grounded in the retrieved context?)\n",
    "  - **Relevancy** (is the context relevant to the query?)\n",
    "- Measures the **response time** per query.\n",
    "\n",
    "### 📊 What it Measures:\n",
    "- **⏱️ Average Response Time** — Time taken to generate answers.\n",
    "- **✅ Faithfulness Score** — How factual the generated responses are, based on retrieved context.\n",
    "- **🔍 Relevancy Score** — How relevant the retrieved context is to the query.\n",
    "\n",
    "> 💡 Why this matters:  \n",
    "In real-world RAG systems, there's always a **tradeoff between chunk size and performance**:\n",
    "- Large chunks = richer context but slower and harder to retrieve precisely.\n",
    "- Small chunks = faster and more targeted, but may lack enough info.\n",
    "\n",
    "This function helps us **quantify that tradeoff** and choose optimal chunk sizes for production use.\n",
    "\n",
    "---\n",
    "\n",
    "📌 *Note:*  \n",
    "While `BatchEvalRunner` from LlamaIndex can evaluate faster in bulk, we use a **for-loop** here to individually measure **response time** for each question, which is crucial for latency benchmarking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n",
    "# We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it.\n",
    "def evaluate_response_time_and_accuracy(chunk_size, eval_questions):\n",
    "    \"\"\"\n",
    "    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\n",
    "    \n",
    "    Parameters:\n",
    "    chunk_size (int): The size of data chunks being processed.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    total_response_time = 0\n",
    "    total_faithfulness = 0\n",
    "    total_relevancy = 0\n",
    "\n",
    "    # create vector index\n",
    "    llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "    Settings.llm = llm\n",
    "    Settings.chunk_size = chunk_size\n",
    "    Settings.chunk_overlap = chunk_size // 5 \n",
    "\n",
    "    vector_index = VectorStoreIndex.from_documents(eval_documents)\n",
    "    \n",
    "    # build query engine\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k=5)\n",
    "    num_questions = len(eval_questions)\n",
    "\n",
    "    # Iterate over each question in eval_questions to compute metrics.\n",
    "    # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\n",
    "    # we're using a loop here to specifically measure response time for different chunk sizes.\n",
    "    for question in eval_questions:\n",
    "        start_time = time.time()\n",
    "        response_vector = query_engine.query(question)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        faithfulness_result = faithfulness_gpt4.evaluate_response(\n",
    "            response=response_vector\n",
    "        ).passing\n",
    "        \n",
    "        relevancy_result = relevancy_gpt4.evaluate_response(\n",
    "            query=question, response=response_vector\n",
    "        ).passing\n",
    "\n",
    "        total_response_time += elapsed_time\n",
    "        total_faithfulness += faithfulness_result\n",
    "        total_relevancy += relevancy_result\n",
    "\n",
    "    average_response_time = total_response_time / num_questions\n",
    "    average_faithfulness = total_faithfulness / num_questions\n",
    "    average_relevancy = total_relevancy / num_questions\n",
    "\n",
    "    return average_response_time, average_faithfulness, average_relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ea5b12",
   "metadata": {},
   "source": [
    "## 🧪 Benchmarking Different Chunk Sizes\n",
    "\n",
    "In this step, we test how different **chunk sizes** impact:\n",
    "\n",
    "- 📉 **Latency** (response time)\n",
    "- ✅ **Faithfulness** (factual correctness)\n",
    "- 🔍 **Relevancy** (how on-topic the retrieved context is)\n",
    "\n",
    "### 🧪 Chunk Sizes Tested:\n",
    "We loop over a list of chunk sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a640b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunk_sizes = [128, 256]\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "  avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, k_eval_questions)\n",
    "  print(f\"Chunk size {chunk_size} - Average Response time: {avg_response_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95c93a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📘 Summary & Credits\n",
    "\n",
    "This notebook is based on the excellent open-source repository [RAG_Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques).  \n",
    "I referred to that work to understand how the pipeline is structured and then reimplemented the same concept in a **fully self-contained** way, but using recent models — as part of my personal learning journey.\n",
    "\n",
    "The purpose of this notebook is purely **educational**:  \n",
    "- To deepen my understanding of Retrieval-Augmented Generation systems  \n",
    "- To keep a clean, trackable log of what I’ve built and learned  \n",
    "- And to serve as a future reference for myself or others starting from scratch\n",
    "\n",
    "To support that, I’ve added clear, concise markdowns throughout the notebook — explaining *why* each package was installed, *why* each line of code exists, and *how* each component fits into the overall RAG pipeline. It’s designed to help anyone (including my future self) grasp the **how** and the **why**, not just the **what**.\n",
    "\n",
    "## 📊 Why Evaluate Chunk Sizes in RAG?\n",
    "\n",
    "Chunking is one of the **most underrated performance levers** in Retrieval-Augmented Generation (RAG). The size of the text chunks directly impacts:\n",
    "\n",
    "- 🧠 **Context relevance** — Too small, and context gets fragmented. Too large, and noise overwhelms signal.\n",
    "- ⚡ **Inference latency** — Smaller chunks are faster to retrieve and rank. Larger chunks can slow down LLM processing.\n",
    "- 🎯 **Accuracy** — Faithfulness and relevancy both fluctuate depending on how well the chunks align with the query intent.\n",
    "\n",
    "In this notebook, I:\n",
    "- Loaded a document corpus and generated **25 evaluation questions**\n",
    "- Used **GPT-3.5** to generate answers and **GPT-4** to grade them\n",
    "- Evaluated how response **latency, faithfulness, and relevancy** vary across different chunk sizes (128, 256)\n",
    "- Built a **custom evaluator function** to measure all 3 metrics for any chunk size\n",
    "\n",
    "This approach gives **quantitative evidence** to guide chunk tuning in real-world RAG systems.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What’s New in This Version?\n",
    "\n",
    "Compared to my previous RAG builds, this version focuses heavily on **automated evaluation pipelines** using modern LLM tools:\n",
    "\n",
    "- 📐 **Metric-driven architecture tuning** — Measures real-world tradeoffs using structured prompts and eval loops  \n",
    "- 🧪 **Faithfulness and Relevancy grading with GPT-4** — Ensures model answers are grounded in retrieved chunks  \n",
    "- 🔁 **Iterative chunking experiments** — Runs the same evaluation logic over multiple chunk sizes for easy comparison  \n",
    "- 🔍 **Custom faithfulness prompt** — Replaces the default LlamaIndex template with a stricter, context-sensitive scoring rubric  \n",
    "- 📦 **All logic in-notebook** — No reliance on external helper modules — everything is transparent and reproducible\n",
    "\n",
    "This project brings me one step closer to building **production-grade, evaluation-first RAG systems**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 What Could Be Added Next?\n",
    "\n",
    "Here are some high-impact ideas to build on this foundation:\n",
    "\n",
    "- 🧩 **More granular chunking strategies** — Try sentence-based splits, semantic splitting, or recursive chunking with windowed overlap  \n",
    "  *Go beyond static sizes — explore dynamic methods like sentence boundaries or chunk re-ranking.*\n",
    "\n",
    "- 🔁 **Multiple eval rounds per chunk size** — Average metrics over several random samples for higher statistical confidence  \n",
    "  *Improves robustness of conclusions — less noise from specific question sets.*\n",
    "\n",
    "- 📈 **Visualize results** — Plot latency vs. accuracy to find the ideal operating point  \n",
    "  *Use matplotlib or seaborn to produce tradeoff curves and identify sweet spots.*\n",
    "\n",
    "- 🧠 **Expand eval questions** — Use LLMs to generate multi-hop, open-ended, and factoid queries  \n",
    "  *Stress-test the retriever + generator stack with more diverse question types.*\n",
    "\n",
    "- ✅ **Integrate LLM-as-a-Judge frameworks** — Use OpenAI function-calling or LangChain’s `LLMGraderChain` for cleaner eval orchestration  \n",
    "  *This would standardize evaluation across multiple projects.*\n",
    "\n",
    "- 🖼️ **Deploy with UI** — Add a Streamlit dashboard to control chunk size, ask queries, and view live metrics  \n",
    "  *Useful for showcasing internal benchmarks or making decisions interactively.*\n",
    "\n",
    "This notebook sets the stage for rigorous, benchmark-driven RAG optimization — ideal for research and production readiness alike.\n",
    "\n",
    "## 💡 Final Word\n",
    "\n",
    "This notebook is part of my larger personal project: **RAG100x** — a challenge to build and log my journney in RAG from 0 100 in the coming months.\n",
    "\n",
    "It’s not built to impress — it’s built to **progress**.  \n",
    "Everything here is structured to enable **daily iteration**, focused experimentation, and clean documentation.\n",
    "\n",
    "If you're exploring RAG from first principles, feel free to use this as a scaffold for your own builds. And of course — check out the original repository for broader implementations and ideas."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
