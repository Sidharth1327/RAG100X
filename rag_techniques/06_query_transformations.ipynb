{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "561b3bc5",
   "metadata": {},
   "source": [
    "## üîÅ **Query Transformations for Smarter Retrieval** | **RAG100X**\n",
    "\n",
    "This notebook explores how transforming user queries can dramatically improve the quality of retrieval in RAG systems. Instead of relying only on the original (often vague or complex) query, we use LLMs to rewrite, broaden, or break it down ‚Äî helping the retriever fetch **more accurate and complete results**.\n",
    "\n",
    "‚úÖ **Key Techniques Covered**\n",
    "We build and test three query transformation strategies:\n",
    "\n",
    "- **Query Rewriting** ‚Üí Makes vague queries more specific  \n",
    "- **Step-back Prompting** ‚Üí Broadens narrow queries for better context  \n",
    "- **Sub-query Decomposition** ‚Üí Splits complex queries into simpler ones\n",
    "\n",
    "Each transformed query is run through a retriever to compare against the original, helping us analyze which method gives the most useful chunks.\n",
    "\n",
    "> üõ†Ô∏è **Why this matters in production:**  \n",
    "In real-world RAG systems (e.g., customer support or legal research), users often ask unclear or overloaded questions. These techniques help reframe the query behind the scenes ‚Äî so your system doesn‚Äôt miss critical documents or generate half-baked answers.\n",
    "\n",
    "For example, a legal chatbot asked *\"What did the 2022 ruling say about tax law in California?\"* might miss key background. But a **step-back prompt** like *\"What are the major 2022 tax rulings in California?\"* ensures broader, more relevant context gets retrieved.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ **How This Fits into RAG100X**\n",
    "\n",
    "So far, RAG100x has covered:\n",
    "\n",
    "1. PDF-based document QA  \n",
    "2. CSV-based retrieval from structured data  \n",
    "3. Blog-based RAG with hallucination checks  \n",
    "4. Chunk-size tuning for better retrieval  \n",
    "5. Propositional chunking for precision\n",
    "\n",
    "Now in **Day 6**, we shift focus from *what* you retrieve to *how* you ask for it. Query transformation is a lightweight, modular layer you can add to almost any RAG system ‚Äî and it works especially well when users submit complex or ambiguous inputs.\n",
    "\n",
    "> üí° Think of this as upgrading the \"search intent\" of your RAG ‚Äî so your system starts from a better question, not just a better index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0e9319",
   "metadata": {},
   "source": [
    "## üì¶ Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bcf881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5812e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61354e64",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è1. Query Rewriting for Better Retrieval\n",
    "\n",
    "In production RAG systems, user queries are often **too vague** or ambiguous, which leads to poor or irrelevant document retrieval. Query rewriting improves this by **rephrasing queries to be more detailed and retrieval-friendly**.\n",
    "\n",
    "We use GPT-4o (via OpenAI) to automatically rewrite the user's original question using a tailored prompt.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Key Components Explained\n",
    "\n",
    "- **`ChatOpenAI`**  \n",
    "  Loads the GPT-4o model for deterministic generation. Setting `temperature=0` ensures consistent outputs, which is important for reproducible retrieval.\n",
    "\n",
    "- **`PromptTemplate`**  \n",
    "  A reusable template that takes in the original query and formats it into a prompt that tells the LLM *exactly how to rewrite it*.\n",
    "\n",
    "- **`|` Operator (LangChain's chain pipe)**  \n",
    "  Chains the prompt and the model into a single callable object ‚Äî this is syntactic sugar for connecting stages of an LLM pipeline.\n",
    "\n",
    "- **`.invoke()`**  \n",
    "  This runs the chained prompt + model on the given input and returns the LLM's response.\n",
    "\n",
    "---\n",
    "\n",
    "Now let‚Äôs define the rewriting chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6535a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT-4o model with deterministic behavior\n",
    "re_write_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
    "\n",
    "# Create a prompt template that tells the model how to rewrite the query\n",
    "query_rewrite_template = \"\"\"You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. \n",
    "Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n",
    "\n",
    "Original query: {original_query}\n",
    "\n",
    "Rewritten query:\"\"\"\n",
    "\n",
    "# Wrap the prompt into a LangChain template object\n",
    "query_rewrite_prompt = PromptTemplate(\n",
    "    input_variables=[\"original_query\"],\n",
    "    template=query_rewrite_template\n",
    ")\n",
    "\n",
    "# Chain the prompt and LLM using LangChain's pipe syntax\n",
    "query_rewriter = query_rewrite_prompt | re_write_llm\n",
    "\n",
    "# Define a simple function that takes a user query and returns the rewritten version\n",
    "def rewrite_query(original_query):\n",
    "    \"\"\"\n",
    "    Rewrite the original query to improve retrieval.\n",
    "    \n",
    "    Args:\n",
    "    original_query (str): The original user query\n",
    "    \n",
    "    Returns:\n",
    "    str: The rewritten query\n",
    "    \"\"\"\n",
    "    response = query_rewriter.invoke(original_query)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9466a76d",
   "metadata": {},
   "source": [
    "## üß™ Demonstration: Query Rewriting in Action\n",
    "\n",
    "Let‚Äôs see how our query rewriting chain performs on a real-world example from the **‚ÄúUnderstanding Climate Change‚Äù** dataset.\n",
    "\n",
    "The goal is to show how a vague or general query can be rewritten into a **more specific and information-rich version**, which increases the chance of retrieving relevant chunks.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Why this matters\n",
    "\n",
    "RAG pipelines are only as good as the queries they receive. If a user asks something too broad like _\"How does climate change affect the environment?\"_, the retriever might return irrelevant or generic chunks.\n",
    "\n",
    "By rewriting the query to something like:\n",
    "\n",
    "> ‚ÄúWhat are the specific environmental consequences of rising global temperatures due to climate change, such as sea-level rise, extreme weather, or biodiversity loss?‚Äù\n",
    "\n",
    "...we give the retriever a **sharper signal** ‚Äî and improve both recall and answer grounding.\n",
    "\n",
    "Let‚Äôs try it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb68ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query from the \"Understanding Climate Change\" dataset\n",
    "original_query = \"What are the impacts of climate change on the environment?\"\n",
    "\n",
    "# Use the rewrite function defined earlier\n",
    "rewritten_query = rewrite_query(original_query)\n",
    "\n",
    "# Print the before and after\n",
    "print(\"Original query:\", original_query)\n",
    "print(\"\\nRewritten query:\", rewritten_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d87fc8",
   "metadata": {},
   "source": [
    "## üîÑ2. Step-back Prompting for Broader Context\n",
    "\n",
    "Another powerful query transformation technique is **Step-back Prompting**.  \n",
    "Instead of making a query more specific (like query rewriting), this strategy does the opposite ‚Äî it **zooms out**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Why Step-back Prompting?\n",
    "\n",
    "Sometimes users ask **narrow or overly specific questions**, which makes it hard for the retriever to return relevant background info. Step-back prompting generates a **more general version** of the query that can surface **high-level context or foundational facts**.\n",
    "\n",
    "This is especially useful when:\n",
    "- The user skips context because they assume the system knows it.\n",
    "- You want to combine fine-grained and broad retrievals to improve answer grounding.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Code Breakdown\n",
    "\n",
    "- **`step_back_llm = ChatOpenAI(...)`**  \n",
    "  Initializes a GPT-4o model with deterministic outputs (temperature=0) to ensure consistent step-back generation.\n",
    "\n",
    "- **`step_back_template`**  \n",
    "  Defines the prompt used to guide the LLM. It tells the model to generate a **more general version** of the user‚Äôs original query. This helps retrieve wider context from the vector store.\n",
    "\n",
    "- **`PromptTemplate(...)`**  \n",
    "  Converts the above text into a reusable format. We insert the user‚Äôs original query into `{original_query}`.\n",
    "\n",
    "- **`step_back_chain = step_back_prompt | step_back_llm`**  \n",
    "  Chains the template with the LLM ‚Äî meaning we now have a ready-to-use component that takes a user query and outputs a broadened version.\n",
    "\n",
    "- **`generate_step_back_query()`**  \n",
    "  A helper function that invokes the LLM chain and extracts the broader query. This is what we‚Äôll call during inference.\n",
    "\n",
    "> üõ†Ô∏è **In practice**:  \n",
    "A narrow query like _‚ÄúHow does climate change affect coral reefs?‚Äù_ might be hard to answer directly if your database lacks reef-specific chunks. A step-back query like _‚ÄúHow does climate change affect marine ecosystems?‚Äù_ increases the chance of finding relevant passages ‚Äî even if the original term doesn‚Äôt exist in the index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5cb9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the LLM (GPT-4o) with low temperature for consistent outputs\n",
    "step_back_llm = ChatOpenAI(\n",
    "    temperature=0,            # Makes output deterministic (same input = same output)\n",
    "    model_name=\"gpt-4o\",      # High-quality OpenAI model for reasoning and rewriting\n",
    "    max_tokens=4000           # Generous token limit for flexibility in generation\n",
    ")\n",
    "\n",
    "# 2. Define the prompt template used to generate step-back queries\n",
    "step_back_template = \"\"\"You are an AI assistant tasked with generating broader, more general queries to improve context retrieval in a RAG system.\n",
    "Given the original query, generate a step-back query that is more general and can help retrieve relevant background information.\n",
    "\n",
    "Original query: {original_query}\n",
    "\n",
    "Step-back query:\"\"\"\n",
    "\n",
    "# 3. Create a PromptTemplate object\n",
    "# This allows us to format the above prompt dynamically with the user query\n",
    "step_back_prompt = PromptTemplate(\n",
    "    input_variables=[\"original_query\"],\n",
    "    template=step_back_template\n",
    ")\n",
    "\n",
    "# 4. Create an LLMChain by combining the prompt and LLM\n",
    "# This allows us to treat it like a function: pass a query and get a step-back result\n",
    "step_back_chain = step_back_prompt | step_back_llm\n",
    "\n",
    "# 5. Wrap the chain in a Python function for reusability\n",
    "def generate_step_back_query(original_query):\n",
    "    \"\"\"\n",
    "    Generate a step-back query to retrieve broader context.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): The original user query\n",
    "    \n",
    "    Returns:\n",
    "        str: A broader, more general version of the query\n",
    "    \"\"\"\n",
    "    response = step_back_chain.invoke(original_query)\n",
    "    return response.content  # Extract and return the LLM's output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c225ea1",
   "metadata": {},
   "source": [
    "## üß™ Demonstration: Step-back Prompting for Broader Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306cd2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ Try step-back prompting on a climate-related question\n",
    "\n",
    "# Original user query\n",
    "original_query = \"What are the impacts of climate change on the environment?\"\n",
    "\n",
    "# Generate a more general version of this query for background retrieval\n",
    "step_back_query = generate_step_back_query(original_query)\n",
    "\n",
    "# Show both queries side-by-side\n",
    "print(\"Original query:\", original_query)\n",
    "print(\"\\nStep-back query:\", step_back_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01e4363",
   "metadata": {},
   "source": [
    "### üîç3. Sub-query Decomposition: Breaking Down Complex Queries\n",
    "\n",
    "When users ask complex, multi-part questions, a single query often misses important context during retrieval.  \n",
    "**Sub-query decomposition** is a technique where we break a long query into 2‚Äì4 focused, simpler questions ‚Äî each targeting a different aspect of the original intent.\n",
    "\n",
    "This has three major benefits:\n",
    "- ‚úÖ Improves recall by retrieving relevant documents for each sub-question\n",
    "- ‚úÖ Enables structured, multi-hop reasoning\n",
    "- ‚úÖ Leads to richer, more complete answers\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† Why It Works\n",
    "\n",
    "Think of it like asking a research assistant:\n",
    "\n",
    "> \"Tell me everything about climate change's environmental impact\"\n",
    "\n",
    "...vs...\n",
    "\n",
    "> - \"How does climate change affect oceans?\"  \n",
    "> - \"What are the effects on agriculture?\"  \n",
    "> - \"What about biodiversity and human health?\"\n",
    "\n",
    "By decomposing, you improve both **retrieval** and **answer generation**.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è Key Components Explained\n",
    "\n",
    "- **`PromptTemplate`**  \n",
    "  Instructs the LLM to rephrase a query into multiple sub-queries using a clear example. Ensures output consistency.\n",
    "\n",
    "- **`LLMChain` with GPT-4o**  \n",
    "  Chains the prompt and LLM together. GPT-4o is used here for its reasoning ability and structured outputs.\n",
    "\n",
    "- **`decompose_query()`**  \n",
    "  This helper function runs the chain and returns a clean list of sub-queries by parsing the model‚Äôs output line by line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71af344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Load a GPT-4o model to perform sub-query decomposition\n",
    "sub_query_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
    "\n",
    "# üìù Prompt template to instruct the model on how to break down queries\n",
    "subquery_decomposition_template = \"\"\"You are an AI assistant tasked with breaking down complex queries into simpler sub-queries for a RAG system.\n",
    "Given the original query, decompose it into 2-4 simpler sub-queries that, when answered together, would provide a comprehensive response to the original query.\n",
    "\n",
    "Original query: {original_query}\n",
    "\n",
    "example: What are the impacts of climate change on the environment?\n",
    "\n",
    "Sub-queries:\n",
    "1. What are the impacts of climate change on biodiversity?\n",
    "2. How does climate change affect the oceans?\n",
    "3. What are the effects of climate change on agriculture?\n",
    "4. What are the impacts of climate change on human health?\"\"\"\n",
    "\n",
    "# üì¶ Wrap the prompt using LangChain's PromptTemplate\n",
    "subquery_decomposition_prompt = PromptTemplate(\n",
    "    input_variables=[\"original_query\"],\n",
    "    template=subquery_decomposition_template\n",
    ")\n",
    "\n",
    "# üîó Create an LLMChain that ties the prompt to the LLM\n",
    "subquery_decomposer_chain = subquery_decomposition_prompt | sub_query_llm\n",
    "\n",
    "# üîç Function to decompose a query into sub-queries\n",
    "def decompose_query(original_query: str):\n",
    "    \"\"\"\n",
    "    Decompose the original query into simpler sub-queries.\n",
    "    \n",
    "    Args:\n",
    "    original_query (str): The original complex query\n",
    "    \n",
    "    Returns:\n",
    "    List[str]: A list of simpler sub-queries\n",
    "    \"\"\"\n",
    "    # Invoke the LLM chain\n",
    "    response = subquery_decomposer_chain.invoke(original_query).content\n",
    "\n",
    "    # Clean and extract each sub-query line\n",
    "    sub_queries = [q.strip() for q in response.split('\\n') if q.strip() and not q.strip().startswith('Sub-queries:')]\n",
    "    return sub_queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad3740",
   "metadata": {},
   "source": [
    "## üß™ Demonstration: Sub-query Decomposition: Breaking Down Complex Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a93568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example query over the understanding climate change dataset\n",
    "original_query = \"What are the impacts of climate change on the environment?\"\n",
    "sub_queries = decompose_query(original_query)\n",
    "print(\"\\nSub-queries:\")\n",
    "for i, sub_query in enumerate(sub_queries, 1):\n",
    "    print(sub_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb78e27d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìò Summary & Credits\n",
    "\n",
    "This notebook is based on the excellent open-source repository [RAG_Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques).  \n",
    "I referred to that work to understand how the pipeline is structured and then reimplemented the same concept in a **fully self-contained** way, but using recent models ‚Äî as part of my personal learning journey.\n",
    "\n",
    "The purpose of this notebook is purely **educational**:  \n",
    "- To deepen my understanding of Retrieval-Augmented Generation systems  \n",
    "- To keep a clean, trackable log of what I‚Äôve built and learned  \n",
    "- And to serve as a future reference for myself or others starting from scratch\n",
    "\n",
    "To support that, I‚Äôve added clear, concise markdowns throughout the notebook ‚Äî explaining *why* each package was installed, *why* each line of code exists, and *how* each component fits into the overall RAG pipeline. It‚Äôs designed to help anyone (including my future self) grasp the **how** and the **why**, not just the **what**.\n",
    "\n",
    "## üîÑ Why Improve Queries in RAG?\n",
    "\n",
    "Most RAG pipelines focus on chunking and retrieval, but **query quality is the real first bottleneck**. Poorly phrased queries lead to irrelevant or incomplete results ‚Äî even if your chunks and embeddings are perfect.\n",
    "\n",
    "This notebook explores **three advanced query transformation techniques** to supercharge retrieval:\n",
    "\n",
    "- ‚úçÔ∏è **Query Rewriting** ‚Äî Refines vague queries into better-formed, search-optimized versions\n",
    "- üîÅ **Step-back Prompting** ‚Äî Adds background context by generating broader, higher-level queries\n",
    "- üß© **Sub-query Decomposition** ‚Äî Splits complex queries into smaller, atomic sub-queries for multi-hop retrieval\n",
    "\n",
    "Each method is:\n",
    "- Powered by **GPT-4o via LangChain**\n",
    "- Implemented as **modular, reusable LLM chains**\n",
    "- Tested on the *Understanding Climate Change* dataset\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What‚Äôs New in This Version?\n",
    "\n",
    "Compared to earlier RAG builds, this version focuses on **query-centric improvements**, offering:\n",
    "\n",
    "- üß† **LLM-first pre-retrieval transformation** ‚Äî Enhance queries *before* sending them to the retriever  \n",
    "- üéØ **Custom prompt templates** ‚Äî Fine-tuned instructions for consistent, high-quality output  \n",
    "- üß™ **Chainable logic** ‚Äî Each transformation is self-contained and composable with other RAG tools  \n",
    "- üßº **Production-friendly design** ‚Äî No reliance on external modules, all logic lives inside the notebook for easy reproducibility\n",
    "\n",
    "This design philosophy enables **plug-and-play enhancements** for production-grade retrieval systems.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Inferences & Key Takeaways\n",
    "\n",
    "From running the transformations on real examples:\n",
    "\n",
    "- üîç **Rewriting consistently improves keyword alignment**, making FAISS and Chroma retrievals more relevant  \n",
    "- üåê **Step-back prompting adds essential context** ‚Äî especially for under-specified or abstract queries  \n",
    "- üß© **Sub-query decomposition boosts completeness** ‚Äî great for multi-hop or reasoning-heavy questions  \n",
    "- üìä Overall, combining these techniques leads to **higher-quality retrieval inputs**, which translate into **more accurate, grounded LLM outputs**\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ What Could Be Added Next?\n",
    "\n",
    "To evolve this system into a full production-ready layer:\n",
    "\n",
    "- üîÅ **Evaluate each method automatically** ‚Äî Use LLM graders to compare retrieval relevance and answer faithfulness across transformed queries  \n",
    "- üß† **Train a lightweight transformer rewriter** ‚Äî Fine-tune a small model (e.g. DistilBERT) on rewritten vs. original queries for offline use  \n",
    "- üîå **Plug into real retrievers** ‚Äî Connect to Elastic, Weaviate, or pgvector to observe improvements at scale  \n",
    "- üìä **Add scoring dashboards** ‚Äî Track how each transformation impacts hit rate, relevancy, and latency  \n",
    "- üß± **Stack transformations dynamically** ‚Äî Learn when to apply which technique (e.g. rewrite only if the query is vague, decompose only if it's long)\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Final Word\n",
    "\n",
    "This notebook is part of my larger personal project: **RAG100x** ‚Äî a challenge to build and log my journney in RAG from 0 100 in the coming months.\n",
    "\n",
    "It‚Äôs not built to impress ‚Äî it‚Äôs built to **progress**.  \n",
    "Everything here is structured to enable **daily iteration**, focused experimentation, and clean documentation.\n",
    "\n",
    "If you're exploring RAG from first principles, feel free to use this as a scaffold for your own builds. And of course ‚Äî check out the original repository for broader implementations and ideas.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
