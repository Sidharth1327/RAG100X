{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afe5a17a",
   "metadata": {},
   "source": [
    "## üß† Contextual Compression for Focused Retrieval | RAG100X\n",
    "\n",
    "This notebook implements **Contextual Compression** ‚Äî a technique that filters and compresses the retrieved chunks based on a query, ensuring only the **most relevant information** is passed to the LLM for answer generation.\n",
    "\n",
    "Unlike traditional RAG pipelines that return full chunks (which might include irrelevant fluff), contextual compression **uses a secondary LLM to extract only the essential segments**. This leads to more focused, efficient, and faithful answers ‚Äî especially in large or noisy documents.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What You‚Äôll Learn\n",
    "\n",
    "- Why raw chunk retrieval often includes unnecessary or noisy context  \n",
    "- How to build a **ContextualCompressionRetriever** with LangChain  \n",
    "- How the **LLMChainExtractor** works to distill relevant info  \n",
    "- How to combine vector search + compression for more effective RAG  \n",
    "- How compression improves latency, accuracy, and token efficiency  \n",
    "\n",
    "---\n",
    "\n",
    "### üîç Real-world Analogy\n",
    "\n",
    "Imagine you ask a friend to summarize a 100-page report. Instead of reading the whole thing to you, they:\n",
    "\n",
    "> üîç Skim only the sections related to your question  \n",
    "> ‚úÇÔ∏è Extract just the key paragraphs  \n",
    "> üß† Answer using only the most relevant insights  \n",
    "\n",
    "‚úÖ That‚Äôs contextual compression ‚Äî **retrieval with built-in summarization**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† How Contextual Compression Works Under the Hood\n",
    "\n",
    "Here‚Äôs the step-by-step pipeline:\n",
    "\n",
    "| Step                          | What Happens                                                                 |\n",
    "|-------------------------------|------------------------------------------------------------------------------|\n",
    "| 1. PDF Ingestion              | A PDF document is loaded and chunked                                        |\n",
    "| 2. Vector Embedding           | Chunks are embedded using OpenAI and stored in a FAISS vector store         |\n",
    "| 3. Base Retrieval             | Initial top-K chunks are retrieved based on query similarity                |\n",
    "| 4. LLM Compression            | A `LLMChainExtractor` distills only relevant parts from each retrieved chunk|\n",
    "| 5. QA Chain                   | The final compressed context is used by an LLM to generate the answer       |\n",
    "\n",
    "üß† You get **precision-focused retrieval**, reducing both token usage and noise.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Why Contextual Compression Works So Well\n",
    "\n",
    "- üß† **Relevance-driven**: Chunks are trimmed to what actually matters for the query  \n",
    "- üßπ **Noise reduction**: Removes irrelevant surrounding context  \n",
    "- üìâ **Lower token cost**: Only important bits are passed to the LLM  \n",
    "- ‚ö° **Better answers**: Higher grounding and answer quality  \n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è Why This Matters in Production\n",
    "\n",
    "Traditional vector retrieval might return:\n",
    "\n",
    "> ‚Äú...Climate models suggest a variety of trends over the coming decades. One factor is CO‚ÇÇ...‚Äù\n",
    "\n",
    "Whereas compression gives:\n",
    "\n",
    "> ‚ÄúClimate change is primarily driven by CO‚ÇÇ emissions, confirmed by decades of atmospheric research.‚Äù\n",
    "\n",
    "‚úÖ **Cleaner. Direct. Grounded.**\n",
    "\n",
    "This is crucial for applications with:\n",
    "\n",
    "- Tight latency/token budgets  \n",
    "- Complex or large document corpora  \n",
    "- High demand for factual accuracy  \n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Where This Fits in RAG100X\n",
    "\n",
    "In earlier projects, you explored:\n",
    "\n",
    "1. Vanilla PDF/CSV/Web retrieval  \n",
    "2. Chunking strategies and enhancements (Propositional, Semantic, Contextual Headers)  \n",
    "3. Query Transformation and HyDE/HyPE  \n",
    "4. Segment-based and windowed retrieval techniques  \n",
    "\n",
    "Now, in **Day 13**, you compress at the **retrieval level**, post-vector search:  \n",
    "> üí° **Find relevant chunks ‚Äî then shrink them down to what actually matters.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8602ee",
   "metadata": {},
   "source": [
    "## üì¶ Installation & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e83c95",
   "metadata": {},
   "source": [
    "### üß© Key LangChain Components Explained\n",
    "\n",
    "- **`LLMChainExtractor`**  \n",
    "  A document compressor that uses an LLM to extract only the most relevant parts from each retrieved chunk.  \n",
    "  ‚úÖ Filters out noise and keeps only what's useful for answering the query.  \n",
    "  üîç Under the hood: it takes each chunk and the query, runs a prompt over them using an LLM, and returns a compressed version focused on the query.\n",
    "\n",
    "- **`ContextualCompressionRetriever`**  \n",
    "  A special retriever that adds a compression layer on top of your base retriever (like FAISS).  \n",
    "  ‚úÖ First, it retrieves the usual top-k results. Then, it applies `LLMChainExtractor` to each chunk.  \n",
    "  üîç This gives you shorter, query-focused snippets instead of long raw chunks.\n",
    "\n",
    "- **`RetrievalQA`**  \n",
    "  A standard LangChain QA chain that handles the full process: retrieve ‚Üí generate answer.  \n",
    "  ‚úÖ In this setup, it pulls compressed, context-aware chunks from the `ContextualCompressionRetriever`.  \n",
    "  üîç This reduces token usage and improves the relevance and faithfulness of the final answer.\n",
    "\n",
    "\n",
    "These components together create a **retrieval pipeline with built-in semantic filtering**, helping the LLM focus only on the information that truly matters for the user's query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain python-dotenv\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e911618",
   "metadata": {},
   "source": [
    "### Document Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ad738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0bd310",
   "metadata": {},
   "source": [
    "### Creating the Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682f3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Cleaning the document\n",
    "\n",
    "def replace_t_with_space(list_of_documents):\n",
    "    \"\"\"\n",
    "    Replaces all tab characters ('\\t') with spaces in the page content of each document\n",
    "\n",
    "    Args:\n",
    "        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n",
    "\n",
    "    Returns:\n",
    "        The modified list of documents with tab characters replaced by spaces.\n",
    "    \"\"\"\n",
    "\n",
    "    for doc in list_of_documents:\n",
    "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
    "    return list_of_documents\n",
    "\n",
    "#Encoding the pdf into vector store using OpenAI Embeddings\n",
    "\n",
    "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded book content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load PDF documents\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    # Create embeddings and vector store\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "vector_store = encode_pdf(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e87461",
   "metadata": {},
   "source": [
    "### üîß Building a Semantically Compressed Retrieval Pipeline\n",
    "\n",
    "This block sets up a complete retrieval + compression + QA system using LangChain.\n",
    "\n",
    "\n",
    "#### üìå Step-by-step Breakdown:\n",
    "\n",
    "- **`vector_store.as_retriever()`**  \n",
    "  Converts the FAISS vector store into a retriever.  \n",
    "  üîç When given a query, it returns the top-k most similar chunks based on embedding similarity.\n",
    "\n",
    "- **`LLMChainExtractor.from_llm(llm)`**  \n",
    "  Creates a compressor that uses an LLM to extract only the relevant parts from each chunk.  \n",
    "  üîç It takes each chunk + query, and returns a shorter, query-focused version using `gpt-4o-mini`.\n",
    "\n",
    "- **`ContextualCompressionRetriever(...)`**  \n",
    "  Combines the base retriever with the compressor.  \n",
    "  üîç It first retrieves top-k chunks, then compresses each using the LLM to retain only the most relevant info.\n",
    "\n",
    "- **`RetrievalQA.from_chain_type(...)`**  \n",
    "  Builds a RetrievalQA chain using the compressed retriever.  \n",
    "  üîç The chain retrieves compressed chunks, sends them to the LLM, and returns an answer + source docs.\n",
    "\n",
    "\n",
    "üéØ **Why This Setup?**\n",
    "\n",
    "- ‚úÖ Reduces token usage by trimming unnecessary text.\n",
    "- ‚úÖ Focuses on chunks actually useful for answering the query.\n",
    "- ‚úÖ Produces answers that are more relevant, grounded, and concise.\n",
    "\n",
    "This is especially helpful in long-document scenarios where raw chunks might contain lots of irrelevant filler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7168eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Convert the FAISS vector store into a retriever\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Load the LLM that will be used for compressing the chunks\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "\n",
    "# Create a compressor that uses the LLM to extract only relevant info from each chunk\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# Combine the retriever with the compressor\n",
    "#    This retrieves top-k documents and compresses them using the LLM\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "# Build a QA chain that uses the compressed retriever\n",
    "#    The chain sends the compressed chunks to the LLM and returns an answer\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=compression_retriever,\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4316cc3",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e56031",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the main topic of the document?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "print(result[\"result\"])\n",
    "print(\"Source documents:\", result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88776dd0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìò Summary & Credits\n",
    "\n",
    "This notebook is based on the excellent open-source repository [RAG_Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques).  \n",
    "I referred to that work to understand how the pipeline is structured and then reimplemented the same concept in a **fully self-contained** way, but using recent models ‚Äî as part of my personal learning journey.\n",
    "\n",
    "The purpose of this notebook is purely **educational**:  \n",
    "- To deepen my understanding of Retrieval-Augmented Generation systems  \n",
    "- To keep a clean, trackable log of what I‚Äôve built and learned  \n",
    "- And to serve as a future reference for myself or others starting from scratch\n",
    "\n",
    "To support that, I‚Äôve added clear, concise markdowns throughout the notebook ‚Äî explaining *why* each package was installed, *why* each line of code exists, and *how* each component fits into the overall RAG pipeline. It‚Äôs designed to help anyone (including my future self) grasp the **how** and the **why**, not just the **what**.\n",
    "\n",
    "## üîç Why Use Contextual Compression in RAG?\n",
    "\n",
    "Retrieving full chunks can overload the LLM with irrelevant or verbose context ‚Äî reducing precision and increasing latency.\n",
    "\n",
    "**Contextual Compression** solves this by:\n",
    "- ‚úÇÔ∏è Filtering out **irrelevant content** from retrieved documents before generation  \n",
    "- üß† Passing only the **most salient segments** to the LLM  \n",
    "- ‚ö° Making RAG more efficient, focused, and cost-effective  \n",
    "\n",
    "---\n",
    "\n",
    "## üß† What‚Äôs New in This Version?\n",
    "\n",
    "This implementation includes:\n",
    "\n",
    "- üß± A retriever + compressor pipeline using LangChain‚Äôs `ContextualCompressionRetriever`  \n",
    "- üß† **LLMChainExtractor** to extract meaningful context spans using GPT-4o-mini  \n",
    "- üîÑ Seamless plug-in with existing FAISS vectorstore retrievers  \n",
    "- ‚úÖ A full pipeline: vector retriever ‚Üí compressor ‚Üí QA chain  \n",
    "\n",
    "---\n",
    "\n",
    "## üìà Inferences & Key Takeaways\n",
    "\n",
    "- ‚úÖ Compressed context reduces **noise and token load** for the LLM  \n",
    "- üß† Enables more **focused and accurate answers** by removing irrelevant filler  \n",
    "- üîÑ Flexible architecture ‚Äî can plug in any base retriever or compressor  \n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ What Could Be Added Next?\n",
    "\n",
    "- üìä Compare performance with and without compression using eval metrics  \n",
    "- ü§ñ Try advanced compressors like rerankers or summarization chains  \n",
    "- üß† Experiment with chunk merging before compression for longer context  \n",
    "- üõ†Ô∏è Add UI toggles to switch between raw vs. compressed retrieval  \n",
    "\n",
    "---\n",
    "## üí° Final Word\n",
    "\n",
    "This notebook is part of my larger personal project: **RAG100x** ‚Äî a challenge to build and log my journney in RAG from 0 100 in the coming months.\n",
    "\n",
    "It‚Äôs not built to impress ‚Äî it‚Äôs built to **progress**.  \n",
    "Everything here is structured to enable **daily iteration**, focused experimentation, and clean documentation.\n",
    "\n",
    "If you're exploring RAG from first principles, feel free to use this as a scaffold for your own builds. And of course ‚Äî check out the original repository for broader implementations and ideas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
