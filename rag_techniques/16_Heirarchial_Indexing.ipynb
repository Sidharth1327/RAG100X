{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49700db9",
   "metadata": {},
   "source": [
    "## ğŸ—‚ï¸ Hierarchical Indexing for Structured Retrieval | RAG100X\n",
    "\n",
    "This notebook implements **Hierarchical Indexing** â€” a retrieval strategy designed to reflect the **multi-level structure** of complex content (like documents, chapters, sections) and perform **multi-stage retrieval** that mirrors human information search patterns.\n",
    "\n",
    "Rather than treating all chunks equally, hierarchical indexing helps navigate large knowledge bases **top-down** â€” from high-level summaries to fine-grained details â€” improving both **precision and relevance**.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… What Youâ€™ll Learn\n",
    "\n",
    "- How to design **multi-level indexes** (e.g., section-level, paragraph-level)  \n",
    "- How to use **two-stage retrieval**: first coarse-grained, then fine-grained  \n",
    "- How to integrate **LangChain retrievers** to query across hierarchical levels  \n",
    "- When hierarchical indexing is better than flat vector search  \n",
    "- How to implement parent-child relationships between document chunks  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Real-world Analogy\n",
    "\n",
    "Imagine youâ€™re looking for a recipe in a cookbook:\n",
    "\n",
    "> ğŸ“– First, you check the **table of contents** for \"Desserts\"  \n",
    "> ğŸ“„ Then you jump to that chapter and read the **exact recipe for Tiramisu**\n",
    "\n",
    "âœ… Thatâ€™s hierarchical retrieval â€” **start broad, drill down smart**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  How Hierarchical Indexing Works Under the Hood\n",
    "\n",
    "| Step                    | What Happens                                                                 |\n",
    "|-------------------------|------------------------------------------------------------------------------|\n",
    "| 1. Document Structuring | Split documents into hierarchical units (chapters â†’ sections â†’ paragraphs)  |\n",
    "| 2. Indexing             | Create separate vector indexes for higher-level and lower-level chunks      |\n",
    "| 3. Coarse Retrieval     | Retrieve top nodes from the **higher-level index** (e.g., sections)         |\n",
    "| 4. Focused Retrieval    | Drill down into associated **child chunks** (e.g., paragraphs within a section) |\n",
    "| 5. Fusion (Optional)    | Combine results from both levels or rerank them for final context selection |\n",
    "\n",
    "ğŸ’¡ This allows retrieval to mirror **natural information flow** â€” top-down and context-aware.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Why Hierarchical Indexing Matters\n",
    "\n",
    "- ğŸ§  **Preserves document structure**: Honors section-topic boundaries  \n",
    "- ğŸ” **Improves precision**: Narrows search to relevant subsections  \n",
    "- âš™ï¸ **Scalable**: Works well with long-form or structured content  \n",
    "- ğŸ’¬ **Better context windows**: Reduces irrelevant token bloat for LLMs  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ—ï¸ Use Cases Where It Shines\n",
    "\n",
    "- Long documents (e.g., research papers, policy manuals)  \n",
    "- Websites or blogs with structured layouts (e.g., guides, FAQs)  \n",
    "- Legal or financial documents with nested hierarchies  \n",
    "- Any scenario where context granularity improves retrieval quality  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”„ Where This Fits in RAG100X\n",
    "\n",
    "In earlier projects, youâ€™ve built:\n",
    "\n",
    "1. Flat retrieval systems with vector stores  \n",
    "2. Query and context enrichment techniques (HyDE, CEW, Sub-query Decomp)  \n",
    "3. Reranking pipelines for smarter final selection  \n",
    "\n",
    "Now, in **Day 16**, you take a structural leap:\n",
    "\n",
    "> ğŸ’¡ **Not all chunks are equal â€” organize before you retrieve.**  \n",
    "> Hierarchical Indexing brings structure to semantic search.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b598f2",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab02c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-openai python-dotenv\n",
    "import asyncio\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.summarize.chain import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eeb12d",
   "metadata": {},
   "source": [
    "### Define document path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65401a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4900bf3a",
   "metadata": {},
   "source": [
    "## Function to Encode both chunk levels and summary\n",
    "\n",
    "This function implements a **two-level vectorization pipeline** for PDFs â€” capturing both **high-level summaries** and **fine-grained content**. Why is this useful?\n",
    "\n",
    "### ğŸ” Why Hierarchical Embedding?\n",
    "\n",
    "When users query your RAG system, sometimes they need **broad context** (e.g., â€œWhatâ€™s the chapter about?â€), and other times they need **precise details** (e.g., â€œWhatâ€™s the definition of entropy?â€). A single-level embedding (just chunks) either:\n",
    "- Misses global context (if chunks are too narrow), or  \n",
    "- Loses precision (if summaries are too abstract).  \n",
    "\n",
    "**Hierarchical vectorization solves this** by storing both:\n",
    "1. **Summarized representations** of each page (or document section)  \n",
    "2. **Detailed chunks** of the actual text  \n",
    "\n",
    "This way, your retriever can serve **both birds with one stone** â€” high-level overviews and pinpoint answers.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ How the Function Works\n",
    "\n",
    "Hereâ€™s a step-by-step explanation of what the function `encode_pdf_hierarchical()` does:\n",
    "\n",
    "1. **ğŸ“„ Load PDF pages**:\n",
    "   - If `is_string=False`, it loads from a real PDF file using `PyPDFLoader`.\n",
    "   - If `is_string=True`, it assumes the input is raw text (used for testing/debugging).\n",
    "\n",
    "2. **ğŸ“ Summarize Each Page**:\n",
    "   - Uses **GPT-4o-mini** to generate a concise summary of each page.\n",
    "   - Handles API rate limits using **exponential backoff** and batching to stay compliant.\n",
    "\n",
    "3. **ğŸ§© Create Detailed Chunks**:\n",
    "   - Uses `RecursiveCharacterTextSplitter` to break each page into overlapping chunks.\n",
    "   - Adds metadata like chunk index and page number.\n",
    "\n",
    "4. **ğŸ”— Embed Both Levels**:\n",
    "   - Summaries and chunks are embedded **separately** using `OpenAIEmbeddings`.\n",
    "   - Two **FAISS vector stores** are created in parallel:\n",
    "     - One for summaries\n",
    "     - One for chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a642c",
   "metadata": {},
   "source": [
    "## ğŸ” Retry with Exponential Backoff | Rate Limit Resilience for Async Calls\n",
    "\n",
    "In production RAG pipelines, API rate limits are a common bottleneck â€” especially when using services like OpenAI. A **rate limit error** occurs when you send too many requests in a short period, and the server tells you to slow down.\n",
    "\n",
    "To handle this gracefully, we implement a **retry strategy with exponential backoff**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ What is Exponential Backoff?\n",
    "\n",
    "Exponential backoff means:  \n",
    "> *â€œWait a little longer after each failed attempt before retrying â€” and double the wait time each time.â€*\n",
    "\n",
    "This helps reduce the pressure on the server and increases the chance of a successful retry.\n",
    "\n",
    "Example retry delays:  \n",
    "1st retry â†’ wait 1s  \n",
    "2nd retry â†’ wait 2s  \n",
    "3rd retry â†’ wait 4s  \n",
    "... and so on.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ› ï¸ Function Purpose\n",
    "\n",
    "The function `retry_with_exponential_backoff(coroutine)` wraps **any async coroutine** (e.g. calling OpenAI, summarization, embedding) and retries it if it hits a `RateLimitError`.\n",
    "\n",
    "- âœ… It tries multiple times (default 5).\n",
    "- ğŸ§  After each failure, it waits longer before retrying (exponentially).\n",
    "- ğŸ›‘ If all retries fail, it raises the last exception.\n",
    "\n",
    "This makes your system **more reliable** and protects against temporary spikes in API usage.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Why Async?\n",
    "\n",
    "The function is `async` so it can `await` both:\n",
    "- The coroutine you're running.\n",
    "- The pause/wait time using `await asyncio.sleep(...)` inside `exponential_backoff()`.\n",
    "\n",
    "This is crucial in concurrent systems â€” so you **donâ€™t block** the event loop while waiting.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Usage Example\n",
    "\n",
    "Suppose you're summarizing documents using OpenAI's API:\n",
    "```python\n",
    "summary = await retry_with_exponential_backoff(summarize_doc(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b664bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import RateLimitError\n",
    "import random\n",
    "\n",
    "## Helper Functions \n",
    "async def exponential_backoff(attempt):\n",
    "    \"\"\"\n",
    "    Implements exponential backoff with jitter.\n",
    "\n",
    "    Args:\n",
    "        attempt: The current retry attempt number (starts from 0).\n",
    "\n",
    "    This function calculates a dynamic wait time before retrying a failed operation.\n",
    "    It increases wait time exponentially with each retry and adds jitter to prevent\n",
    "    thundering herd problems (many clients retrying at the same fixed interval).\n",
    "    \"\"\"\n",
    "    # Step 1: Compute the exponential delay (2^attempt)\n",
    "    # e.g., for attempt = 0 â†’ 1s, attempt = 1 â†’ 2s, attempt = 2 â†’ 4s, etc.\n",
    "    base_delay = 2 ** attempt\n",
    "\n",
    "    # Step 2: Add a small random delay (jitter) to avoid simultaneous retries\n",
    "    # Jitter helps prevent all clients from hitting the server again at the same time\n",
    "    jitter = random.uniform(0, 1)\n",
    "\n",
    "    # Step 3: Total wait time = exponential delay + jitter\n",
    "    wait_time = base_delay + jitter\n",
    "\n",
    "    # Step 4: Log the wait time (optional, useful for debugging and visibility)\n",
    "    print(f\"Rate limit hit. Retrying in {wait_time:.2f} seconds...\")\n",
    "\n",
    "    # Step 5: Pause execution asynchronously for the calculated time\n",
    "    # This prevents blocking the event loop while waiting\n",
    "    await asyncio.sleep(wait_time)\n",
    "\n",
    "\n",
    "\n",
    "async def retry_with_exponential_backoff(coroutine, max_retries=5):\n",
    "    \"\"\"\n",
    "    Retries a coroutine using exponential backoff upon encountering a RateLimitError.\n",
    "    \n",
    "    Args:\n",
    "        coroutine: The coroutine to be executed.\n",
    "        max_retries: The maximum number of retry attempts.\n",
    "        \n",
    "    Returns:\n",
    "        The result of the coroutine if successful.\n",
    "        \n",
    "    Raises:\n",
    "        The last encountered exception if all retry attempts fail.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Attempt to execute the coroutine\n",
    "            return await coroutine\n",
    "        except RateLimitError as e:\n",
    "            # If the last attempt also fails, raise the exception\n",
    "            if attempt == max_retries - 1:\n",
    "                raise e\n",
    "\n",
    "            # Wait for an exponential backoff period before retrying\n",
    "            await exponential_backoff(attempt)\n",
    "\n",
    "    # If max retries are reached without success, raise an exception\n",
    "    raise Exception(\"Max retries reached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f933a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# Function: Hierarchical PDF Vectorization for RAG\n",
    "\n",
    "async def encode_pdf_hierarchical(path, chunk_size=1000, chunk_overlap=200, is_string=False):\n",
    "    \"\"\"\n",
    "    Asynchronously encodes a PDF book into a hierarchical vector store using OpenAI embeddings.\n",
    "    Produces two FAISS vectorstores:\n",
    "    - One for high-level summaries\n",
    "    - One for detailed chunks\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the PDF file or text string (if is_string=True)\n",
    "        chunk_size (int): Approximate size of each chunk\n",
    "        chunk_overlap (int): Overlap between chunks for continuity\n",
    "        is_string (bool): Whether input is plain text or a real PDF\n",
    "\n",
    "    Returns:\n",
    "        (summary_vectorstore, detailed_vectorstore): FAISS vectorstores at two levels\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the PDF as a list of documents (each page = one doc)\n",
    "    if not is_string:\n",
    "        loader = PyPDFLoader(path)\n",
    "        documents = await asyncio.to_thread(loader.load)\n",
    "    else:\n",
    "        # When working with plain text (e.g., testing), split it manually\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        documents = text_splitter.create_documents([path])\n",
    "\n",
    "    # Set up the LLM summarizer for page-level summaries\n",
    "    summary_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "    summary_chain = load_summarize_chain(summary_llm, chain_type=\"map_reduce\")\n",
    "\n",
    "    async def summarize_doc(doc):\n",
    "        \"\"\"\n",
    "        Summarizes one document (page-level) with retry logic.\n",
    "        Returns a Document object containing the summary.\n",
    "        \"\"\"\n",
    "        summary_output = await retry_with_exponential_backoff(summary_chain.ainvoke([doc]))\n",
    "        summary = summary_output['output_text']\n",
    "        return Document(\n",
    "            page_content=summary,\n",
    "            metadata={\n",
    "                \"source\": path,\n",
    "                \"page\": doc.metadata.get(\"page\", 0),\n",
    "                \"summary\": True\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Process documents in batches to avoid OpenAI rate limits\n",
    "    batch_size = 5\n",
    "    summaries = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        batch_summaries = await asyncio.gather(*[summarize_doc(doc) for doc in batch])\n",
    "        summaries.extend(batch_summaries)\n",
    "        await asyncio.sleep(1)  # Pause between batches for safety\n",
    "\n",
    "    # Now, split the full document into smaller detailed chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    detailed_chunks = await asyncio.to_thread(text_splitter.split_documents, documents)\n",
    "\n",
    "    # Add metadata to each chunk: page number, chunk ID, and label it as non-summary\n",
    "    for i, chunk in enumerate(detailed_chunks):\n",
    "        chunk.metadata.update({\n",
    "            \"chunk_id\": i,\n",
    "            \"summary\": False,\n",
    "            \"page\": int(chunk.metadata.get(\"page\", 0))\n",
    "        })\n",
    "\n",
    "    # Load the embedding model\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    # Function to create vectorstore from any set of documents\n",
    "    async def create_vectorstore(docs):\n",
    "        return await retry_with_exponential_backoff(\n",
    "            asyncio.to_thread(FAISS.from_documents, docs, embeddings)\n",
    "        )\n",
    "\n",
    "    # Create both vector stores in parallel: summaries and chunks\n",
    "    summary_vectorstore, detailed_vectorstore = await asyncio.gather(\n",
    "        create_vectorstore(summaries),\n",
    "        create_vectorstore(detailed_chunks)\n",
    "    )\n",
    "\n",
    "    return summary_vectorstore, detailed_vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed854d8",
   "metadata": {},
   "source": [
    "## ğŸ§  Encode the PDF book to both document-level summaries and detailed chunks if the vector stores do not exist\n",
    "\n",
    "This block encodes a PDF into **two levels of granularity** using a hierarchical strategy:\n",
    "\n",
    "1. **Document-Level Summaries** â€” for high-level overview and fast top-down retrieval.\n",
    "2. **Detailed Chunks** â€” fine-grained sections used for in-depth retrieval.\n",
    "\n",
    "The system first checks whether pre-computed vector stores already exist:\n",
    "- If **yes**, it loads them using FAISS and `OpenAIEmbeddings`, saving time and compute.\n",
    "- If **no**, it calls an async function `encode_pdf_hierarchical(path)` to:\n",
    "   - Split and embed the PDF into both summary and detailed levels.\n",
    "   - Save the resulting `FAISS` vector stores locally for future use.\n",
    "\n",
    "This structure allows hybrid querying:\n",
    "- Use summaries for coarse filtering or reranking.\n",
    "- Use detailed chunks for final generation or evidence extraction.\n",
    "\n",
    "> ğŸ“‚ Vector stores are saved to:  \n",
    "> `../vector_stores/summary_store` and `../vector_stores/detailed_store`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0059662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if both summary and detailed FAISS vector stores already exist locally\n",
    "if os.path.exists(\"../vector_stores/summary_store\") and os.path.exists(\"../vector_stores/detailed_store\"):\n",
    "    # Initialize the OpenAI embedding model\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    # Load the summary-level vector store from disk\n",
    "    summary_store = FAISS.load_local(\n",
    "        \"../vector_stores/summary_store\",\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True  # Required for secure deserialization\n",
    "    )\n",
    "\n",
    "    # Load the detailed-level vector store from disk\n",
    "    detailed_store = FAISS.load_local(\n",
    "        \"../vector_stores/detailed_store\",\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # If vector stores do not exist, encode the PDF hierarchically (summary + detailed)\n",
    "    summary_store, detailed_store = await encode_pdf_hierarchical(path)\n",
    "\n",
    "    # Save the summary vector store to disk for future reuse\n",
    "    summary_store.save_local(\"../vector_stores/summary_store\")\n",
    "\n",
    "    # Save the detailed vector store to disk for future reuse\n",
    "    detailed_store.save_local(\"../vector_stores/detailed_store\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a8105",
   "metadata": {},
   "source": [
    "## ğŸ” Retrieve information according to summary level, and then retrieve information from the chunk level vector store and filter according to the summary level pages\n",
    "\n",
    "This function implements a **hierarchical retrieval** mechanism using two levels of semantic granularity:\n",
    "\n",
    "1. **High-level Summary Retrieval**  \n",
    "   It first queries a vector store of **document-level summaries** to identify relevant regions in the document.\n",
    "   \n",
    "2. **Targeted Chunk Retrieval**  \n",
    "   For each top summary result, it filters and retrieves **fine-grained chunks** from the detailed vector store **within the same page** as the summary.\n",
    "\n",
    "### Why Hierarchical?\n",
    "- âœ… Reduces noise by narrowing detailed retrieval to only relevant regions.\n",
    "- âœ… Improves **precision** in large documents with diverse topics.\n",
    "- âœ… Mimics how humans search: first skim, then deep dive.\n",
    "\n",
    "### Parameters:\n",
    "- `k_summaries`: Number of top summaries to retrieve (coarse filtering).\n",
    "- `k_chunks`: Number of chunks to retrieve per summary (fine detail).\n",
    "\n",
    "> ğŸ“Œ Assumes each summary and chunk includes a `\"page\"` metadata field for page-level alignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d21ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hierarchical(query, summary_vectorstore, detailed_vectorstore, k_summaries=3, k_chunks=5):\n",
    "    \"\"\"\n",
    "    Performs a hierarchical retrieval using the query.\n",
    "\n",
    "    Args:\n",
    "        query: The search query string.\n",
    "        summary_vectorstore: Vector store containing high-level document summaries.\n",
    "        detailed_vectorstore: Vector store containing detailed text chunks.\n",
    "        k_summaries: Number of top summaries to retrieve (default is 3).\n",
    "        k_chunks: Number of detailed chunks to retrieve per summary (default is 5).\n",
    "\n",
    "    Returns:\n",
    "        A list of relevant detailed chunks aligned with the most relevant summaries.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Retrieve top-k summaries that match the query\n",
    "    top_summaries = summary_vectorstore.similarity_search(query, k=k_summaries)\n",
    "    \n",
    "    # Initialize a list to collect relevant detailed chunks\n",
    "    relevant_chunks = []\n",
    "\n",
    "    # Step 2: For each summary, retrieve detailed chunks from the same page\n",
    "    for summary in top_summaries:\n",
    "        # Extract the page number metadata from the summary\n",
    "        page_number = summary.metadata[\"page\"]\n",
    "\n",
    "        # Define a filter that matches only chunks from the same page\n",
    "        page_filter = lambda metadata: metadata[\"page\"] == page_number\n",
    "\n",
    "        # Search the detailed vector store with the same query,\n",
    "        # but restrict results to the filtered page\n",
    "        page_chunks = detailed_vectorstore.similarity_search(\n",
    "            query,\n",
    "            k=k_chunks,\n",
    "            filter=page_filter\n",
    "        )\n",
    "\n",
    "        # Add the retrieved chunks to the result list\n",
    "        relevant_chunks.extend(page_chunks)\n",
    "\n",
    "    # Return all matched detailed chunks across summaries\n",
    "    return relevant_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de481a7",
   "metadata": {},
   "source": [
    "### Demonstrate on a use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e572b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the greenhouse effect?\"\n",
    "results = retrieve_hierarchical(query, summary_store, detailed_store)\n",
    "\n",
    "# Print results\n",
    "for chunk in results:\n",
    "    print(f\"Page: {chunk.metadata['page']}\")\n",
    "    print(f\"Content: {chunk.page_content}...\")  # Print first 100 characters\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da980527",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“˜ Summary & Credits\n",
    "\n",
    "This notebook is based on the excellent open-source repository [RAG_Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques).  \n",
    "I referred to that work to understand how the pipeline is structured and then reimplemented the same concept in a **fully self-contained** way, but using recent models â€” as part of my personal learning journey.\n",
    "\n",
    "The purpose of this notebook is purely **educational**:  \n",
    "- To deepen my understanding of Retrieval-Augmented Generation systems  \n",
    "- To keep a clean, trackable log of what Iâ€™ve built and learned  \n",
    "- And to serve as a future reference for myself or others starting from scratch\n",
    "\n",
    "To support that, Iâ€™ve added clear, concise markdowns throughout the notebook â€” explaining *why* each package was installed, *why* each line of code exists, and *how* each component fits into the overall RAG pipeline. Itâ€™s designed to help anyone (including my future self) grasp the **how** and the **why**, not just the **what**.\n",
    "\n",
    "## ğŸ” Why Use Hierarchical Retrieval in RAG?\n",
    "\n",
    "In large or structured documents, querying the entire corpus at once often retrieves contextually scattered results.\n",
    "\n",
    "**Hierarchical Retrieval** addresses this by:\n",
    "- ğŸ§­ First identifying **relevant regions** (via summaries or TOC-level units)  \n",
    "- ğŸ” Then performing **focused retrieval** within those regions  \n",
    "- ğŸ§¹ Reducing noise from irrelevant sections, even if semantically similar  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Whatâ€™s New in This Version?\n",
    "\n",
    "This implementation includes:\n",
    "\n",
    "- ğŸ§± Two-level vectorstores â€” one for **summaries**, one for **detailed chunks**  \n",
    "- ğŸ—‚ï¸ Metadata-based filtering to align chunks with their summaryâ€™s **page number**  \n",
    "- ğŸ” A hierarchical flow: *summaries â filtered detailed chunks*  \n",
    "- ğŸ”Œ Easily integrable into LangChain or standalone RAG pipelines  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ Inferences & Key Takeaways\n",
    "\n",
    "- âœ… Ideal for **lengthy PDFs, multi-topic reports, or research papers**  \n",
    "- ğŸ§  Mimics how humans search â€” *skim first, deep dive later*  \n",
    "- âš¡ Reduces unnecessary token usage and LLM context pollution  \n",
    "- ğŸ” Encourages **targeted grounding**, improving factual consistency in generation  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ What Could Be Added Next?\n",
    "\n",
    "- ğŸ“Š Evaluate impact on **faithfulness and hallucination rates**  \n",
    "- ğŸ§ª Extend filtering to **multi-field metadata** (e.g., section, chapter)  \n",
    "- ğŸ”— Combine with reranking (e.g., **CrossEncoder**) for even tighter precision  \n",
    "- ğŸ§  Add fallback to global retrieval if summaries fail to locate answers  \n",
    "\n",
    "\n",
    "## ğŸ’¡ Final Word\n",
    "\n",
    "This notebook is part of my larger personal project: **RAG100x** â€” a challenge to build and log my journney in RAG from 0 100 in the coming months.\n",
    "\n",
    "Itâ€™s not built to impress â€” itâ€™s built to **progress**.  \n",
    "Everything here is structured to enable **daily iteration**, focused experimentation, and clean documentation.\n",
    "\n",
    "If you're exploring RAG from first principles, feel free to use this as a scaffold for your own builds. And of course â€” check out the original repository for broader implementations and ideas."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
