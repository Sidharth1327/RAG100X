{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2fcQ52WvjTz"
      },
      "source": [
        "## 🌐 **Web-Based RAG with Hallucination Detection** | **RAG100X**\n",
        "\n",
        "This notebook demonstrates a fully functional Retrieval-Augmented Generation (RAG) pipeline built on top of real blog articles from [DeepLearning.ai](https://www.deeplearning.ai/blog/). It extracts, indexes, and retrieves relevant sections from web content, performs generation with proper grounding, and includes advanced reasoning modules like hallucination detection and source attribution.\n",
        "\n",
        "✅ **Key Capabilities**  \n",
        "*This notebook expands the RAG capabilities by layering multiple critical stages:*\n",
        "\n",
        "- *Loads articles directly from web URLs using LangChain’s WebBaseLoader*  \n",
        "- *Splits the raw text into semantic chunks using `RecursiveCharacterTextSplitter`*  \n",
        "- *Embeds each chunk using Cohere’s high-performance embedding model*  \n",
        "- *Stores and retrieves via Chroma vectorstore for scalable search*  \n",
        "- *Uses Groq’s LLaMA 3.1 model to filter relevant chunks (document grading)*  \n",
        "- *Generates grounded answers using only the filtered, high-relevance chunks*  \n",
        "- *Detects hallucinations in the LLM output by verifying against the source text*  \n",
        "\n",
        "\n",
        "> 🛠️ **Note:** This notebook is **completely self-contained**, with all logic implemented inline for transparency, reproducibility, and easy customization. It is designed to move one step closer to production-grade RAG.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 **How This Notebook Differs from Previous RAGs**\n",
        "\n",
        "🧩 Compared to the first two notebooks in **RAG100x**, this version introduces multiple critical upgrades:\n",
        "\n",
        "| Feature                        | Day 1: PDF QA RAG | Day 2: CSV RAG | ✅ Day 3: Web RAG |\n",
        "|-------------------------------|-------------------|----------------|-------------------|\n",
        "| Data Source                   | PDFs              | CSVs           | 🌐 Web Articles   |\n",
        "| Retrieval Only                | ✅ Yes            | ❌ No          | ❌ No (includes generation) |\n",
        "| Answer Generation             | ❌ No             | ✅ Yes         | ✅ Yes            |\n",
        "| Document Relevance Filtering  | ❌ No             | ❌ No          | ✅ Yes (via LLM)  |\n",
        "| Hallucination Detection       | ❌ No             | ❌ No          | ✅ Yes            |\n",
        "| Embedding Model               | OpenAI            | OpenAI         | 🧠 Cohere         |\n",
        "| Vector DB                     | FAISS             | FAISS          | 📦 Chroma         |\n",
        "| LLM Stack                     | OpenAI GPT-4o     | GPT-4o         | 🔥 Groq (LLaMA 3.1, Mixtral) |\n",
        "\n",
        "\n",
        "This notebook moves from simple experimentation to more **real-world RAG workflows** like trust, traceability, and precision — key for production readiness.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbq2DOjIyWua"
      },
      "source": [
        "### 📦 Installing Core Libraries for Web-based RAG\n",
        "\n",
        "To get started, we install a minimal yet powerful set of libraries required for a **self-contained, production-ready RAG system** built on top of live web content:\n",
        "\n",
        "- **`langchain` & `langchain-community`**  \n",
        "  Provides standardized interfaces for document loaders, splitters, embedding models, vectorstores, and LLM chains — including community-maintained integrations.\n",
        "\n",
        "- **`python-dotenv`**  \n",
        "  Helps manage API credentials securely by loading them from a `.env` file into environment variables.\n",
        "\n",
        "> We intentionally keep dependencies lightweight and modular to retain full control over the pipeline and ensure reproducibility in future experiments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MsyFdxlyfFp"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langchain langchain-community python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2NLsDQtyjmR"
      },
      "source": [
        "### 🔐 Setting Up Environment Variables for API Access\n",
        "\n",
        "To connect with external services like **Cohere** (for embeddings) and **Groq** (for running LLMs), we need to securely load our API keys.\n",
        "\n",
        "Instead of hardcoding sensitive credentials into the notebook, we use a `.env` file combined with the `python-dotenv` package. This keeps our keys out of version control and makes the setup portable.\n",
        "\n",
        "- **`.env` file** contains your API keys in simple `KEY=value` format.\n",
        "- **`load_dotenv()`** reads that file and makes the values available to Python.\n",
        "- **`os.environ[...]`** is used to explicitly set the keys for downstream compatibility.\n",
        "\n",
        "> 📁 Make sure your `.env` file is located in the root directory and looks like this:\n",
        "> ```\n",
        "> GROQ_API_KEY=your_groq_key\n",
        "> COHERE_API_KEY=your_cohere_key\n",
        "> ```\n",
        "\n",
        "This setup allows the rest of our notebook to access LLMs and embedding models seamlessly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQw1zciizXiq"
      },
      "outputs": [],
      "source": [
        "# Load and set API keys from .env file\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load variables from the .env file into environment\n",
        "load_dotenv()\n",
        "\n",
        "# Explicitly set them in os.environ (ensures compatibility across services)\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
        "os.environ[\"COHERE_API_KEY\"] = os.getenv(\"COHERE_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0LkBNEGz8_I"
      },
      "source": [
        "### 🧱 Building the Vector Index from Web Articles\n",
        "\n",
        "We’re turning online blog posts into searchable chunks by following a few essential steps:\n",
        "\n",
        "- **`WebBaseLoader`**  \n",
        "  Loads the full article content from each URL — useful when your data lives on the web.\n",
        "\n",
        "- **`RecursiveCharacterTextSplitter`**  \n",
        "  Splits long articles into smaller chunks. This helps preserve semantic structure while making each chunk manageable for embedding.\n",
        "\n",
        "- **`CohereEmbeddings`**  \n",
        "  Converts each chunk into a dense vector using Cohere’s high-quality multilingual embedding model.\n",
        "\n",
        "- **`Chroma` Vectorstore**  \n",
        "  Stores all these vectors in a fast, local vector database that supports similarity search. A lightweight and fast vectorstore ideal for local experimentation.\n",
        "\n",
        "- **`.as_retriever()`**  \n",
        "  Creates a retriever that can return the top `k` most relevant chunks based on a semantic query.\n",
        "\n",
        "> ✅ After this step, we have a ready-to-query semantic index of the DeepLearning.ai blogs — our knowledge base for RAG!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfYgxQN-0aQh"
      },
      "outputs": [],
      "source": [
        "# Import required tools\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_cohere import CohereEmbeddings\n",
        "\n",
        "# Load Cohere embedding model\n",
        "embedding_model = CohereEmbeddings(model=\"embed-english-v3.0\")\n",
        "\n",
        "# List of DeepLearning.ai article URLs\n",
        "urls = [\n",
        "    \"https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io\",\n",
        "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/?ref=dl-staging-website.ghost.io\",\n",
        "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/?ref=dl-staging-website.ghost.io\",\n",
        "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-4-planning/?ref=dl-staging-website.ghost.io\",\n",
        "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/?ref=dl-staging-website.ghost.io\"\n",
        "]\n",
        "\n",
        "# Load all web documents\n",
        "raw_docs = []\n",
        "for url in urls:\n",
        "    raw_docs.extend(WebBaseLoader(url).load())\n",
        "\n",
        "# Split each article into smaller chunks\n",
        "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=0\n",
        ")\n",
        "chunks = splitter.split_documents(raw_docs)\n",
        "\n",
        "# Store embeddings in Chroma vectorstore\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embedding_model,\n",
        "    collection_name=\"web-rag\"\n",
        ")\n",
        "\n",
        "# Create a retriever to fetch top 4 relevant chunks\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 4}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPdme_9Z1UUK"
      },
      "source": [
        "### 🔍 Retrieving Relevant Chunks for a Question\n",
        "\n",
        "Now that we’ve built our vector index, we can test it by asking a question.\n",
        "\n",
        "- **`retriever.invoke()`**  \n",
        "  Takes a user question, embeds it, and returns the top-matching document chunks from the vectorstore.\n",
        "\n",
        "- **Document Metadata**  \n",
        "  We print the first retrieved chunk to inspect what kind of content is being returned — including its `title`, `source URL`, and actual `content`.\n",
        "\n",
        "> This step confirms that our retriever is working correctly and that the documents being pulled are semantically relevant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnfzLRpO1as5"
      },
      "outputs": [],
      "source": [
        "# Ask a question\n",
        "question = \"what are the different kinds of agentic design patterns?\"\n",
        "\n",
        "# Retrieve top-matching documents from the vector index\n",
        "docs = retriever.invoke(question)\n",
        "\n",
        "# Preview the first retrieved chunk\n",
        "print(f\"Title: {docs[0].metadata['title']}\\n\")\n",
        "print(f\"Source: {docs[0].metadata['source']}\\n\")\n",
        "print(f\"Content:\\n{docs[0].page_content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqlb4gvz2mRh"
      },
      "source": [
        "### 🧪 LLM-Based Filtering: Checking Document Relevance\n",
        "\n",
        "Retrievers aren't perfect — they might return documents that are loosely related or even irrelevant. To improve answer quality, we use an **LLM to grade each retrieved chunk** for actual relevance.\n",
        "\n",
        "Here's how this filtering works:\n",
        "\n",
        "- **Why?**  \n",
        "  To remove noise and hallucinations in later steps, we want to only pass *truly useful* documents to the answer generation step.\n",
        "\n",
        "- **What?**  \n",
        "  We create a structured grading function using **Groq’s LLaMA 3.1 model**, which checks whether a document is relevant to the given question.\n",
        "\n",
        "- **How?**  \n",
        "  1. We define a simple schema with a single binary output: `\"yes\"` or `\"no\"`.\n",
        "  2. We wrap the LLM with this schema using `with_structured_output(...)`.\n",
        "  3. We feed each retrieved chunk + question into a prompt template, and the LLM decides whether it's useful or not.\n",
        "\n",
        "- **`GradeDocuments (Pydantic)`**  \n",
        "  Defines the expected output: the model can only reply with **\"yes\"** or **\"no\"** — nothing else.\n",
        "\n",
        "- **`ChatGroq(model=\"llama-3.1-8b-instant\")`**  \n",
        "  Uses Groq’s lightning-fast LLaMA model to analyze each document.\n",
        "\n",
        "- **`.with_structured_output(...)`**  \n",
        "  Forces the model to follow our \"yes/no\" format — this keeps responses consistent and easy to use.\n",
        "\n",
        "- **`ChatPromptTemplate`**  \n",
        "  Gives the model a clear job:  \n",
        "  > “Here’s a document and a user question. Is this document relevant?”\n",
        "\n",
        "\n",
        ">| Component                   | Role                                                        |\n",
        "| --------------------------- | ----------------------------------------------------------- |\n",
        "| `retriever.invoke(...)`     | Fetches relevant-looking documents                          |\n",
        "| `retrieval_grader`          | Decides if each document is *actually* relevant or not      |\n",
        "| `\"yes\"` / `\"no\"` decision   | Filters out bad retrievals before they reach the LLM answer |\n",
        "\n",
        "> 🧠 Under the hood: This turns the LLM into a *relevance classifier*, helping us automatically discard retrieved content that isn’t grounded in the user’s query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9TZd1v93rRi"
      },
      "outputs": [],
      "source": [
        "# Imports for prompting and schema validation\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Define a structured schema for the LLM output\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "    binary_score: str = Field(\n",
        "        description=\"Is this document relevant to the user’s question? 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "# Load Groq's LLaMA model with output schema binding\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Create a prompt that tells the LLM how to assess relevance\n",
        "system_prompt = \"\"\"\n",
        "You are a grader assessing relevance of a retrieved document to a user question.\n",
        "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n",
        "It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
        "Give a binary score — 'yes' if relevant, 'no' otherwise.\n",
        "\"\"\"\n",
        "\n",
        "grade_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"human\", \"Retrieved document:\\n\\n{document}\\n\\nUser question: {question}\")\n",
        "])\n",
        "\n",
        "# Chain prompt and model into a single grader They reply: Yes or No\n",
        "retrieval_grader = grade_prompt | structured_llm_grader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xnEoI2c5FXZ"
      },
      "source": [
        "### 🧹 Filtering Out Irrelevant Documents\n",
        "\n",
        "Now that we’ve built our document grader, let’s use it to **clean the retrieval results**.\n",
        "\n",
        "We loop through the retrieved documents and ask:\n",
        "\n",
        "> 🧐 “Does this document help answer the question?”\n",
        "\n",
        "- **`retrieval_grader.invoke({...})`**  \n",
        "  Passes in the document and question to our LLM-based grader.\n",
        "\n",
        "- **Filter Logic**  \n",
        "  If the model returns `\"yes\"` (relevant), we **keep the document**.  \n",
        "  Otherwise, we drop it from further processing.\n",
        "\n",
        "- **`docs_to_use`**  \n",
        "  Stores only the documents that pass our relevance check —  \n",
        "  so the final answer is based on **trusted, filtered sources**.\n",
        "\n",
        "> This improves answer quality and reduces hallucination by eliminating off-topic content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ih-NWsgz5ZL5"
      },
      "outputs": [],
      "source": [
        "docs_to_use = []\n",
        "for doc in docs:\n",
        "    print(doc.page_content, '\\n', '-'*50)\n",
        "    res = retrieval_grader.invoke({\"question\": question, \"document\": doc.page_content})\n",
        "    print(res,'\\n')\n",
        "    if res.binary_score == 'yes':\n",
        "        docs_to_use.append(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1moowD_5cyg"
      },
      "source": [
        "### 🧠 Generating the Final Answer with LLaMA-3.1\n",
        "\n",
        "This is the final step — where the **cleaned, relevant documents** are passed to the LLM to generate a concise answer.\n",
        "\n",
        "- **`ChatPromptTemplate`**  \n",
        "  Structures the prompt with two pieces:\n",
        "  - `<docs>...</docs>` → Injects the filtered documents\n",
        "  - `<question>...</question>` → Inserts the user’s query\n",
        "\n",
        "- **`ChatGroq(model=\"llama-3.1-8b-instant\")`**  \n",
        "  Uses Groq’s blazing-fast LLaMA model to answer based on the input context.\n",
        "\n",
        "- **`format_docs()`**  \n",
        "  Wraps each document in tags (`<doc1>`, `<doc2>`, etc.) with metadata like title & source for traceability.\n",
        "\n",
        "- **`StrOutputParser()`**  \n",
        "  Ensures the final output is returned as clean, readable text.\n",
        "\n",
        "- **`rag_chain`**  \n",
        "  This chain flows:  \n",
        "  **Prompt → LLM → Output Parser** — giving us a ready-to-display answer.\n",
        "\n",
        "> ✨ In just a few lines, we’ve gone from user query → document retrieval → answer generation — all powered by LangChain + Groq.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0FOhhBK5oaB"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Prompt structure for answer generation\n",
        "system_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
        "Answer the question based on the retrieved documents and your knowledge.\n",
        "Keep it short and focused — 3 to 5 sentences max.\"\"\"\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"human\",\n",
        "     \"Retrieved documents:\\n\\n<docs>{documents}</docs>\\n\\n\"\n",
        "     \"User question: <question>{question}</question>\")\n",
        "])\n",
        "\n",
        "# LLM: Fast and deterministic\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
        "\n",
        "# Format retrieved docs with metadata\n",
        "def format_docs(docs):\n",
        "    return \"\\n\".join(\n",
        "        f\"<doc{i+1}>:\\nTitle: {doc.metadata.get('title', 'N/A')}\\n\"\n",
        "        f\"Source: {doc.metadata.get('source', 'N/A')}\\n\"\n",
        "        f\"Content: {doc.page_content}\\n</doc{i+1}>\\n\"\n",
        "        for i, doc in enumerate(docs)\n",
        "    )\n",
        "\n",
        "# Final RAG chain: Prompt → LLM → Parser\n",
        "rag_chain = qa_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "generation = rag_chain.invoke({\n",
        "    \"documents\": format_docs(docs_to_use),\n",
        "    \"question\": question\n",
        "})\n",
        "\n",
        "print(generation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T3pvBJm6x_x"
      },
      "source": [
        "---\n",
        "\n",
        "### 🧪 Hallucination Detection Using Groq + Function Calling\n",
        "\n",
        "Once the LLM generates an answer, it's important to **verify whether the answer is actually grounded in the retrieved source documents** — especially in high-stakes or trust-sensitive applications.\n",
        "\n",
        "This section introduces a **hallucination checker** using Groq’s LLaMA 3.1 model and LangChain's function calling capabilities.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🧠 What We're Doing\n",
        "\n",
        "We use an LLM to **analyze whether the generated answer is actually supported by the retrieved chunks**. This adds an extra trust layer to the RAG pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔍 Why This Matters\n",
        "\n",
        "While RAG systems are designed to stay grounded in the retrieved context, **hallucinations can still happen** when:\n",
        "\n",
        "- The retrieved context is only loosely relevant\n",
        "- The LLM fills gaps with prior knowledge\n",
        "- The question was misunderstood\n",
        "\n",
        "This step explicitly flags such situations by asking the LLM to judge its own output.\n",
        "\n",
        "---\n",
        "\n",
        "#### ⚙️ How It Works\n",
        "\n",
        "1. **`GradeHallucinations` Pydantic Model**  \n",
        "   Defines a schema with a single field: `binary_score` → either `'yes'` or `'no'`.\n",
        "\n",
        "2. **Structured Output with Groq’s LLM**  \n",
        "   We wrap Groq’s LLaMA 3.1 model with `.with_structured_output(...)` so the LLM *must* return output matching our `GradeHallucinations` schema.\n",
        "\n",
        "3. **Custom Prompt for Hallucination Scoring**  \n",
        "   A system message tells the LLM to check if the answer is grounded in the facts.  \n",
        "   A human message supplies both the `<facts>` and the `<generation>`.\n",
        "\n",
        "4. **Grading Execution**  \n",
        "   Finally, we pass the prompt and invoke the grading pipeline:\n",
        "   ```python\n",
        "   response = hallucination_grader.invoke({\n",
        "       \"documents\": format_docs(docs_to_use),\n",
        "       \"generation\": generation\n",
        "   })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHut3wSN7aUx"
      },
      "outputs": [],
      "source": [
        "# Grading schema: grounded = 'yes', hallucinated = 'no'\n",
        "class GradeHallucinations(BaseModel):\n",
        "    binary_score: str = Field(\n",
        "        ...,\n",
        "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "# Grader LLM setup\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
        "\n",
        "# Grading prompt: Does generation align with retrieved facts?\n",
        "system_prompt = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts.\n",
        "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
        "\n",
        "hallucination_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"human\",\n",
        "     \"Set of facts:\\n\\n<facts>{documents}</facts>\\n\\n\"\n",
        "     \"LLM generation: <generation>{generation}</generation>\")\n",
        "])\n",
        "\n",
        "# Combine prompt and LLM\n",
        "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
        "\n",
        "# Evaluate grounding\n",
        "response = hallucination_grader.invoke({\n",
        "    \"documents\": format_docs(docs_to_use),\n",
        "    \"generation\": generation\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNLfo99T8q3o"
      },
      "source": [
        "---\n",
        "\n",
        "## 📘 Summary & Credits\n",
        "\n",
        "This notebook is based on the excellent open-source repository [RAG_Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques).  \n",
        "I referred to that work to understand how the pipeline is structured and then reimplemented the same concept in a **fully self-contained** way, but using recent models — as part of my personal learning journey.\n",
        "\n",
        "The purpose of this notebook is purely **educational**:  \n",
        "- To deepen my understanding of Retrieval-Augmented Generation systems  \n",
        "- To keep a clean, trackable log of what I’ve built and learned  \n",
        "- And to serve as a future reference for myself or others starting from scratch\n",
        "\n",
        "To support that, I’ve added clear, concise markdowns throughout the notebook — explaining *why* each package was installed, *why* each line of code exists, and *how* each component fits into the overall RAG pipeline. It’s designed to help anyone (including my future self) grasp the **how** and the **why**, not just the **what**.\n",
        "\n",
        "\n",
        "## 🌐 Why Use Web Data?\n",
        "\n",
        "Unlike static files (like PDFs or CSVs), web content changes fast — and often contains rich, diverse context. In this notebook, I:\n",
        "- Scraped content from [DeepLearning.ai](https://www.deeplearning.ai/blog/)\n",
        "- Parsed, cleaned, and embedded the text using **Cohere Embeddings**\n",
        "- Stored it in a **Chroma vectorstore**\n",
        "- Retrieved and ranked relevant chunks based on user queries\n",
        "\n",
        "This is closer to how real production systems behave — dynamic data, fast inference, and structured outputs.\n",
        "\n",
        "## 🧠 What’s New in This Version?\n",
        "\n",
        "Compared to my previous RAG builds, this version introduces several advanced features:\n",
        "\n",
        "- 🔍 **LLM-based document filtering** — Uses a structured grader to filter out irrelevant chunks before generation  \n",
        "- 🧪 **Hallucination detection** — Adds a second grading step to validate whether the generated answer is grounded in the retrieved content  \n",
        "- ⚡ **Groq-hosted LLaMA 3.1 models** — Provides ultra-fast inference with structured outputs  \n",
        "- 🔗 **LangChain 0.2+ composability** — Built using clean expression-style chaining, no legacy `LCEL` syntax  \n",
        "- 💡 **Manual document formatting** — Ensures consistent input formatting for both generation and grading phases  \n",
        "- ✨ **No external modules** — Everything is implemented directly inside the notebook for transparency and reproducibility\n",
        "\n",
        "\n",
        "## 🚀 What Could Be Added Next?\n",
        "\n",
        "This is a strong base, but several production-ready enhancements could follow:\n",
        "\n",
        "- ✅ **Source Attribution** — Map the generated answer to specific documents or snippets  \n",
        "  *Use techniques like context windows with highlighted evidence or attach source metadata inline with answer spans. LangChain’s `stuff` or `map_reduce` chains can be adapted to include citations.*\n",
        "\n",
        "- 📈 **Confidence Scoring** — Add soft labels or scoring to the relevance and hallucination outputs  \n",
        "  *Instead of binary \"yes/no\" hallucination flags, output a confidence score between 0–1 using a regression-style prompt or token probabilities.*\n",
        "\n",
        "- 🧩 **Multi-hop Retrieval** — Support reasoning across multiple documents with scratchpad prompts  \n",
        "  *Chain together multiple retrieved documents and guide the LLM with intermediate reasoning steps (e.g. via ReAct or Tree of Thoughts-style prompting).*\n",
        "\n",
        "- 🧪 **Answer Evaluation** — Use GPT-based graders or traditional metrics to score the final answer  \n",
        "  *Apply tools like `LLM-as-a-judge` or integrate BLEU/ROUGE-style metrics if ground truths exist — useful in benchmarking system accuracy.*\n",
        "\n",
        "- 🖼️ **Streamlit / Gradio UI** — Turn this into a live tool or chatbot  \n",
        "  *Wrap the full chain into a friendly web interface where users can upload links, ask questions, and view sources, generation, and hallucination verdicts interactively.*\n",
        "\n",
        "- 🔍 **Hybrid Retrieval** — Combine keyword (BM25) and vector search for higher recall  \n",
        "  *Fuse dense embedding search (via Chroma or FAISS) with sparse keyword retrieval (via BM25 or Elasticsearch) to catch both semantic and lexical matches.*\n",
        "\n",
        "Each of these could be a new notebook in the series.\n",
        "\n",
        "## 💡 Final Word\n",
        "\n",
        "This notebook is part of my larger personal project: **RAG100x** — a challenge to build and log my journney in RAG from 0 100 in the coming months.\n",
        "\n",
        "It’s not built to impress — it’s built to **progress**.  \n",
        "Everything here is structured to enable **daily iteration**, focused experimentation, and clean documentation.\n",
        "\n",
        "If you're exploring RAG from first principles, feel free to use this as a scaffold for your own builds. And of course — check out the original repository for broader implementations and ideas."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
