{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ebecedf",
   "metadata": {},
   "source": [
    "## 🧩 Relevant Segment Extraction (RSE) for Better Context | RAG100X\n",
    "\n",
    "This notebook implements **Relevant Segment Extraction (RSE)** — a retrieval-time optimization that reconstructs the *most contextually useful* segments from your documents by selecting **contiguous runs of relevant chunks**, not just isolated ones.\n",
    "\n",
    "Instead of naively picking the Top-k highest scoring chunks, RSE uses a reranking model (like Cohere Rerank) to assign relevance scores to all chunks, and then intelligently finds the **best spans** of text — even if some relevant chunks weren’t individually top-ranked.\n",
    "\n",
    "The result? Better grounding, smoother flow, and improved performance for LLM answers.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ What You’ll Learn\n",
    "\n",
    "- Why Top-k retrieval can lead to broken context and hallucinations  \n",
    "- How RSE scores and selects the *best chunk spans*, not individual pieces  \n",
    "- How segment optimization helps recover missing context that matters  \n",
    "- When RSE outperforms vanilla retrieval in real-world QA tasks  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Real-world Analogy\n",
    "\n",
    "Imagine you’re watching a movie, and someone asks:\n",
    "\n",
    "> *\"What caused the main character’s breakdown?\"*\n",
    "\n",
    "If you only watch 3 random dramatic scenes (Top-k chunks), you’ll miss the full story.\n",
    "\n",
    "But if you instead watch a **full 5-minute clip** leading up to the breakdown — even if it includes a few “boring” moments — you’ll get the full emotional arc.\n",
    "\n",
    "✅ **RSE gives you that full arc — not just scattered scenes.**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔬 How RSE Works Under the Hood\n",
    "\n",
    "Let’s say your document is split into 500 chunks. For a given query, we want to extract the *best possible segments* — ideally, groups of neighboring chunks that flow well together and are jointly relevant.\n",
    "\n",
    "| Step               | What Happens                                                                 |\n",
    "|--------------------|------------------------------------------------------------------------------|\n",
    "| 1. Chunking        | Document is split into non-overlapping chunks using LangChain’s splitter     |\n",
    "| 2. Reranking       | Each chunk is scored using Cohere Rerank for its relevance to the query     |\n",
    "| 3. Value Mapping   | Scores are converted into “chunk values” — good chunks = +ve, bad = –ve     |\n",
    "| 4. Segmentation    | A search algorithm finds **contiguous spans** with high total chunk value   |\n",
    "| 5. Filtering       | Segments are pruned based on token budget, redundancy, and quality          |\n",
    "| 6. Output          | The selected segments are passed to the LLM as context                      |\n",
    "\n",
    "🧠 Even if a few chunks inside a span are low scoring, they might **complete a relevant section** — so we include them if the segment as a whole is valuable.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Why This Works So Well\n",
    "\n",
    "- 🧱 **Context continuity**: Relevant ideas rarely exist in isolation — they live in flow  \n",
    "- 🤖 **LLMs love coherence**: Chunks from the same topic block improve grounding and reduce hallucination  \n",
    "- 🎯 **Span-level optimization**: Instead of picking top chunks, RSE picks *top stories*  \n",
    "\n",
    "---\n",
    "\n",
    "### 🏗️ Why This Matters in Production\n",
    "\n",
    "Imagine this chunk retrieved on its own:\n",
    "\n",
    "> *“The final experiment used a dropout rate of 0.2 and achieved 87% accuracy.”*\n",
    "\n",
    "It’s informative — but out of context. What was the task? Why that setup? What came before?\n",
    "\n",
    "With RSE, we retrieve a **span** like:\n",
    "\n",
    "1. *“Dataset and Preprocessing…”*  \n",
    "2. *“Model Architecture: 3-layer BiLSTM…”*  \n",
    "3. *“Final experiment used a dropout rate…”*\n",
    "\n",
    "That full thread provides **complete reasoning**, improving both faithfulness and fluency.\n",
    "\n",
    "**RSE ensures retrieval returns meaningful *segments*, not fragmented sentences.**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Where This Fits in RAG100X\n",
    "\n",
    "So far in RAG100X, we’ve explored:\n",
    "\n",
    "1. PDF-based QA  \n",
    "2. CSV-based semantic search  \n",
    "3. DeepLearning.ai RAG QA  \n",
    "4. Chunk size & latency tuning  \n",
    "5. Proposition-aware chunking  \n",
    "6. Query rewriting + decomposition  \n",
    "7. HyDE: Embed imagined answers  \n",
    "8. HyPE: Embed imagined questions  \n",
    "9. CCH: Add intelligent titles to chunks  \n",
    "\n",
    "Now in **Day 10**, we zoom in on post-retrieval — and **optimize which chunks make it into the final context**.\n",
    "\n",
    "> 💡 **RSE doesn't just pick relevant chunks — it picks relevant *stories*.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bec1bd",
   "metadata": {},
   "source": [
    "## 📦 Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f23ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install matplotlib numpy python-dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from scipy.stats import beta\n",
    "import matplotlib.pyplot as plt\n",
    "import cohere\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "os.environ[\"CO_API_KEY\"] = os.getenv('CO_API_KEY') # Cohere API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c300a9f",
   "metadata": {},
   "source": [
    "## 🧩 Define Helper Functions for Chunk Scoring with Cohere Reranker\n",
    "\n",
    "Before we extract relevant segments, we need a way to **measure how relevant each chunk is** to a given user query.\n",
    "\n",
    "In this section:\n",
    "\n",
    "- We'll define helper functions to **split raw text into chunks**, **score them using Cohere's rerank API**, and **visualize the results**.\n",
    "- Since we're only working with a single document, we skip vector search and send all chunks directly to the reranker.\n",
    "\n",
    "### 📌 Functions Defined\n",
    "\n",
    "1. `split_into_chunks`: Uses LangChain’s `RecursiveCharacterTextSplitter` to break large documents into overlapping or non-overlapping chunks.\n",
    "2. `transform`: Applies a Beta CDF to spread Cohere's relevance scores (which are often bunched near 0 or 1) into a smoother 0–1 range.\n",
    "3. `rerank_chunks`: Calls Cohere’s `rerank` API to assign relevance scores to each chunk based on a query.\n",
    "4. `plot_relevance_scores`: Visualizes the relevance scores across all chunks.\n",
    "\n",
    "These scores will later help us **extract only the most relevant segments**, minimizing hallucinations and improving grounding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572bd2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Function to split a document into smaller chunks using LangChain's RecursiveCharacterTextSplitter\n",
    "def split_into_chunks(text: str, chunk_size: int = 800) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits a large document into overlapping or non-overlapping chunks.\n",
    "\n",
    "    Args:\n",
    "        text (str): Full text of the document.\n",
    "        chunk_size (int): Size of each chunk in characters.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of string chunks.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=0, length_function=len\n",
    "    )\n",
    "    texts = text_splitter.create_documents([text])\n",
    "    chunks = [text.page_content for text in texts]\n",
    "    return chunks\n",
    "\n",
    "# Beta transformation to smooth out Cohere's sharp relevance scores (usually close to 0 or 1)\n",
    "def transform(x: float) -> float:\n",
    "    \"\"\"\n",
    "    Transforms the sharp relevance scores from Cohere Rerank into smoother values.\n",
    "\n",
    "    Args:\n",
    "        x (float): Raw relevance score from the reranker.\n",
    "\n",
    "    Returns:\n",
    "        float: Smoothed score using the beta cumulative distribution function.\n",
    "    \"\"\"\n",
    "    a, b = 0.4, 0.4  # Tunable parameters controlling smoothness\n",
    "    return beta.cdf(x, a, b)\n",
    "\n",
    "# Use Cohere Rerank API to evaluate relevance of each chunk to the input query\n",
    "def rerank_chunks(query: str, chunks: List[str]):\n",
    "    \"\"\"\n",
    "    Reranks all chunks using Cohere's rerank model and computes relevance-weighted scores.\n",
    "\n",
    "    Args:\n",
    "        query (str): Natural language query.\n",
    "        chunks (List[str]): List of document chunks to score.\n",
    "\n",
    "    Returns:\n",
    "        similarity_scores (List[float]): Normalized similarity scores.\n",
    "        chunk_values (List[float]): Final chunk importance scores (with decay applied).\n",
    "    \"\"\"\n",
    "    model = \"rerank-english-v3.0\"\n",
    "    client = cohere.Client(api_key=os.environ[\"CO_API_KEY\"])\n",
    "    decay_rate = 30  # Controls how quickly lower-ranked chunks decay in importance\n",
    "\n",
    "    reranked_results = client.rerank(model=model, query=query, documents=chunks)\n",
    "    results = reranked_results.results\n",
    "\n",
    "    reranked_indices = [r.index for r in results]\n",
    "    raw_scores = [r.relevance_score for r in results]\n",
    "\n",
    "    similarity_scores = [0] * len(chunks)\n",
    "    chunk_values = [0] * len(chunks)\n",
    "\n",
    "    for i, index in enumerate(reranked_indices):\n",
    "        abs_score = transform(raw_scores[i])\n",
    "        similarity_scores[index] = abs_score\n",
    "        # Apply exponential decay based on the rank\n",
    "        chunk_values[index] = np.exp(-i / decay_rate) * abs_score\n",
    "\n",
    "    return similarity_scores, chunk_values\n",
    "\n",
    "# Visualize how relevant each chunk is to the input query\n",
    "def plot_relevance_scores(chunk_values: List[float], start_index: int = None, end_index: int = None) -> None:\n",
    "    \"\"\"\n",
    "    Plots a scatter graph showing how relevant each chunk is to the query.\n",
    "\n",
    "    Args:\n",
    "        chunk_values (List[float]): Importance scores per chunk.\n",
    "        start_index (int, optional): Starting chunk index for the plot.\n",
    "        end_index (int, optional): Ending chunk index for the plot.\n",
    "    \"\"\"\n",
    "    if start_index is None:\n",
    "        start_index = 0\n",
    "    if end_index is None:\n",
    "        end_index = len(chunk_values)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.title(\"🔍 Query-to-Chunk Relevance Across Document\")\n",
    "    plt.xlabel(\"Chunk Index\")\n",
    "    plt.ylabel(\"Relevance Score\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.scatter(range(start_index, end_index), chunk_values[start_index:end_index])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ab0e6",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e177bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the input document\n",
    "FILE_PATH = \"data/nike_2023_annual_report.txt\"\n",
    "\n",
    "with open(FILE_PATH, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "chunks = split_into_chunks(text, chunk_size=800)\n",
    "\n",
    "print (f\"Split the document into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21e5fda",
   "metadata": {},
   "source": [
    "## 📊 Visualize Chunk Relevance Across a Single Document\n",
    "\n",
    "Now that we’ve defined the reranker and scoring helpers, let’s test them on a real query.\n",
    "\n",
    "In this step:\n",
    "- We pass a **realistic financial query** to the reranker.\n",
    "- We **visualize how relevant each chunk** in the document is.\n",
    "- This helps us identify which parts of the document should be used to construct a grounded response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78b15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Nike consolidated financial statements\"\n",
    "similarity_scores, chunk_values = rerank_chunks(query, chunks)\n",
    "plot_relevance_scores(chunk_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4979a5",
   "metadata": {},
   "source": [
    "## 📊 Interpreting the Chunk Relevance Plot\n",
    "\n",
    "The plot above visualizes how relevant each document chunk is to the input query.\n",
    "\n",
    "- The **x-axis** shows the chunk index — starting from 0 for the first chunk in the document.\n",
    "- The **y-axis** shows the **final relevance score**, which is not just the raw similarity score from the reranker.\n",
    "\n",
    "### 🧠 How is the Relevance Score Computed?\n",
    "\n",
    "Each chunk is passed through a reranker (like Cohere), which gives:\n",
    "- A **raw relevance score** — how semantically similar the chunk is to the query.\n",
    "- A **rank** — where it stands among all chunks in terms of similarity (1st, 2nd, etc.).\n",
    "\n",
    "Instead of relying only on one of these signals, we combine them:\n",
    "- The **rank** is converted using an **exponential decay function** — so the top-ranked chunks contribute more to the final score.\n",
    "- This decay value is **multiplied with the raw relevance score**.\n",
    "- Optionally, a **beta transformation** is applied to distribute the scores more smoothly.\n",
    "\n",
    "This makes the score both **position-aware** (top results matter more) and **content-aware** (chunks still need semantic similarity).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Zooming into a Relevant Cluster\n",
    "\n",
    "We noticed that the similarity scores in the range of **chunk indices 320–333** were noticeably higher than others, indicating a dense cluster of relevance.\n",
    "\n",
    "To examine this section more closely:\n",
    "\n",
    "```python\n",
    "plot_relevance_scores(chunk_values, 320, 340)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51d401",
   "metadata": {},
   "source": [
    "## 🔍 Understanding the Retrieved Document Segment\n",
    "\n",
    "After identifying a cluster of high-relevance chunks between **chunk 320 and 340**, we inspect the actual content and find something insightful.\n",
    "\n",
    "From our observation:\n",
    "- **Chunk 323** starts the section titled **\"Consolidated Statement of Income\"**.\n",
    "- The following chunks up to **chunk 333** contain **detailed financial statements** — exactly what our query is looking for.\n",
    "\n",
    "This means that **all the chunks from 323 to 333 are semantically relevant** to the query:  \n",
    "**\"Nike consolidated financial statements\"**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🤔 Then Why Were Only Half Marked as Relevant?\n",
    "\n",
    "Here’s what this means:\n",
    "\n",
    "- The reranker model evaluates each chunk **independently**, and returns a binary decision:  \n",
    "  ✅ **Relevant** or ❌ **Not Relevant**.\n",
    "- In the case of chunks 323–333:\n",
    "  - Only about **half** were labeled as **relevant** by the reranker.\n",
    "  - The **others were marked as irrelevant**, even though they’re clearly part of the same logical section.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Why Does This Happen?\n",
    "\n",
    "The reranker doesn’t have access to **context beyond individual chunks**. So:\n",
    "\n",
    "- Some chunks may contain **section headers or highly keyword-aligned sentences** — these get marked as relevant.\n",
    "- Others may contain **numeric tables, continuation of previous content, or subtler phrasing** — these don’t match the query as explicitly, so they’re marked as irrelevant.\n",
    "\n",
    "> In short, **chunks that are contextually important but less textually aligned** might be ignored by the reranker.\n",
    "\n",
    "This leads to a situation where **important but low-signal chunks are sandwiched between high-signal ones**, and get missed in naive filtering.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Why This Matters\n",
    "\n",
    "If we blindly include only those chunks marked as relevant:\n",
    "- We may **break the logical continuity** of the document section.\n",
    "- The LLM might **miss key information**, especially in structured or tabular formats.\n",
    "\n",
    "By instead considering **clusters of adjacent relevant chunks**, and including the chunks in between, we can:\n",
    "- Provide the LLM with a **more coherent and complete context**.\n",
    "- Avoid missing content that’s **crucial but subtle**.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Takeaway\n",
    "\n",
    "- Binary relevance labels are **not enough** — proximity and continuity also matter.\n",
    "- For structured documents like financial reports, always consider **nearby chunks as potential context**, even if they weren’t ranked as top matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feea1e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_document_segment(chunks: List[str], start_index: int, end_index: int):\n",
    "    \"\"\"\n",
    "    Print the text content of a segment of the document\n",
    "\n",
    "    Args:\n",
    "        chunks (list): List of text chunks\n",
    "        start_index (int): Start index of the segment\n",
    "        end_index (int): End index of the segment (not inclusive)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Prints:\n",
    "        The text content of the specified segment of the document\n",
    "    \"\"\"\n",
    "    for i in range(start_index, end_index):\n",
    "        print(f\"\\nChunk {i}\")\n",
    "        print(chunks[i])\n",
    "\n",
    "print_document_segment(chunks, 320, 340)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147b5f88",
   "metadata": {},
   "source": [
    "## 🧩 Why Use Clusters of Relevant Chunks?\n",
    "\n",
    "When answering complex or structured queries, **individual chunks** often fail to provide enough context. But when we group **contiguous relevant chunks together as a cluster**, we preserve the **semantic continuity** of the original document — something large language models (LLMs) benefit from significantly.\n",
    "\n",
    "Instead of feeding isolated chunks to the LLM, we now aim to feed **entire segments of highly relevant, adjacent chunks**.\n",
    "\n",
    "\n",
    "## 🧠 How Do We Find These Clusters?\n",
    "\n",
    "This is where the challenge lies:  \n",
    "We need to **automatically detect groups of adjacent chunks** that together form a high-quality, coherent segment.\n",
    "\n",
    "To do this efficiently, we reframe the problem as a variation of the **maximum subarray problem**, which is a well-known algorithmic problem with efficient solutions.\n",
    "\n",
    "But how?\n",
    "\n",
    "\n",
    "\n",
    "## 🔢 Defining Chunk Values for Optimization\n",
    "\n",
    "We already have **relevance scores** (e.g., from a reranker) for each chunk — typically between `0` and `1`. To use them in an optimization algorithm, we tweak them slightly:\n",
    "\n",
    "### ➕ Relevant chunks → Positive score  \n",
    "### ➖ Irrelevant chunks → Negative score  \n",
    "\n",
    "To make this transformation:\n",
    "- We subtract a constant value (say, **0.2**) from each relevance score.\n",
    "- This shifts **low-relevance chunks below zero**, making them penalize the segment value.\n",
    "- We call this constant the `irrelevant_chunk_penalty`.\n",
    "\n",
    "This setup ensures that the more relevant chunks a segment has — and the fewer irrelevant ones — the **higher the total score**.\n",
    "\n",
    "\n",
    "\n",
    "## 🔍 What Does the Algorithm Do?\n",
    "\n",
    "The function `get_best_segments(...)` solves this modified version of the **maximum subarray problem**, with some added constraints:\n",
    "\n",
    "1. **Inputs:**\n",
    "   - A list of **relevance values** (after applying the penalty).\n",
    "   - A **max length** for a single segment.\n",
    "   - A **total length budget** for all returned segments.\n",
    "   - A **minimum score** required to consider a segment \"good\".\n",
    "\n",
    "2. **Goal:**  \n",
    "   Find non-overlapping segments of chunks that:\n",
    "   - Stay within the max length.\n",
    "   - Stay within the overall chunk budget.\n",
    "   - Have a total score above a threshold.\n",
    "   - Don’t overlap with previously selected segments.\n",
    "\n",
    "3. **Output:**  \n",
    "   A list of `(start_index, end_index)` pairs representing the best segments to include, plus their respective scores.\n",
    "\n",
    "\n",
    "## ✅ Why This Works\n",
    "\n",
    "This approach ensures:\n",
    "- **High-density clusters of relevance** are selected, not just isolated peaks.\n",
    "- **Adjacent useful chunks are grouped**, preserving context.\n",
    "- **Irrelevant noise is penalized**, making the output cleaner.\n",
    "\n",
    "This is especially powerful for documents like financial reports or technical documentation, where meaningful information spans multiple nearby chunks, and context is everything.\n",
    "\n",
    "\n",
    "## 💡 Summary\n",
    "\n",
    "By reframing segment selection as a **scoring and optimization problem**, we can smartly select the most useful and coherent parts of a document. This significantly boosts the **retrieval quality** and ultimately leads to more accurate, grounded, and complete answers from the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c435853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_segments(relevance_values: list, max_length: int, overall_max_length: int, minimum_value: float):\n",
    "    \"\"\"\n",
    "    This function takes the chunk relevance values and then runs an optimization algorithm to find the best segments. In more technical terms, it solves a constrained version of the maximum sum subarray problem.\n",
    "\n",
    "    Note: this is a simplified implementation intended for demonstration purposes. A more sophisticated implementation would be needed for production use and is available in the dsRAG library.\n",
    "\n",
    "    Args:\n",
    "        relevance_values (list): a list of relevance values for each chunk of a document\n",
    "        max_length (int): the maximum length of a single segment (measured in number of chunks)\n",
    "        overall_max_length (int): the maximum length of all segments (measured in number of chunks)\n",
    "        minimum_value (float): the minimum value that a segment must have to be considered\n",
    "\n",
    "    Returns:\n",
    "        best_segments (list): a list of tuples (start, end) that represent the indices of the best segments (the end index is non-inclusive) in the document\n",
    "        scores (list): a list of the scores for each of the best segments\n",
    "    \"\"\"\n",
    "    best_segments = []\n",
    "    scores = []\n",
    "    total_length = 0\n",
    "    while total_length < overall_max_length:\n",
    "        # find the best remaining segment\n",
    "        best_segment = None\n",
    "        best_value = -1000\n",
    "        for start in range(len(relevance_values)):\n",
    "            # skip over negative value starting points\n",
    "            if relevance_values[start] < 0:\n",
    "                continue\n",
    "            for end in range(start+1, min(start+max_length+1, len(relevance_values)+1)):\n",
    "                # skip over negative value ending points\n",
    "                if relevance_values[end-1] < 0:\n",
    "                    continue\n",
    "                # check if this segment overlaps with any of the best segments and skip if it does\n",
    "                if any(start < seg_end and end > seg_start for seg_start, seg_end in best_segments):\n",
    "                    continue\n",
    "                # check if this segment would push us over the overall max length and skip if it would\n",
    "                if total_length + end - start > overall_max_length:\n",
    "                    continue\n",
    "                \n",
    "                # define segment value as the sum of the relevance values of its chunks\n",
    "                segment_value = sum(relevance_values[start:end])\n",
    "                if segment_value > best_value:\n",
    "                    best_value = segment_value\n",
    "                    best_segment = (start, end)\n",
    "        \n",
    "        # if we didn't find a valid segment then we're done\n",
    "        if best_segment is None or best_value < minimum_value:\n",
    "            break\n",
    "\n",
    "        # otherwise, add the segment to the list of best segments\n",
    "        best_segments.append(best_segment)\n",
    "        scores.append(best_value)\n",
    "        total_length += best_segment[1] - best_segment[0]\n",
    "    \n",
    "    return best_segments, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7b2f2",
   "metadata": {},
   "source": [
    "## 🧩 Segment Optimization: Selecting the Most Valuable Context\n",
    "\n",
    "Now that we've defined how to score contiguous chunks using our relevance-based method, the next step is to **actually run the optimization** and select the most valuable text segments to pass to the LLM.\n",
    "\n",
    "\n",
    "\n",
    "### ⚙️ Setting the Optimization Parameters\n",
    "\n",
    "We first define a few important constraints and thresholds that guide the optimization process:\n",
    "\n",
    "- **Irrelevant Chunk Penalty (`0.2`)**:  \n",
    "  This value is subtracted from each chunk's relevance score to convert it into a \"value.\" Chunks with low original relevance will now have negative values, while highly relevant ones remain positive. This makes it easier to find high-density regions of relevance using a simple sum.\n",
    "\n",
    "- **Maximum Segment Length (`20` chunks)**:  \n",
    "  This restricts how long a single segment can be. It ensures we don’t create excessively large spans, which could reduce precision.\n",
    "\n",
    "- **Overall Maximum Length (`30` chunks)**:  \n",
    "  This is a hard limit on the total number of chunks that can be selected across all segments. It helps control context size and prevents the model input from overflowing.\n",
    "\n",
    "- **Minimum Value Threshold (`0.7`)**:  \n",
    "  This ensures that only meaningful, high-value segments are retained. Segments with total value below this threshold are discarded.\n",
    "\n",
    "\n",
    "\n",
    "### ➖ Converting Relevance Scores into Optimization-Friendly Values\n",
    "\n",
    "To enable the optimizer to find high-value clusters, we convert each chunk’s relevance score into a new value by subtracting a constant penalty. This transformation ensures:\n",
    "- **Positive values** indicate useful, highly relevant chunks.\n",
    "- **Negative values** indicate noise or irrelevant content.\n",
    "\n",
    "This lets us use a greedy optimization approach to find sequences of chunks with maximum total value — i.e., those that are not only relevant but tightly packed together in the document.\n",
    "\n",
    "\n",
    "\n",
    "### 🚀 Running the Optimizer\n",
    "\n",
    "We now run the segment selection algorithm. It scans through the adjusted chunk values and selects a set of **non-overlapping segments** that:\n",
    "- Respect the `max_length` and `overall_max_length` constraints,\n",
    "- Do not overlap with each other,\n",
    "- And each individually score above the `minimum_value` threshold.\n",
    "\n",
    "This results in a list of the most valuable segments from the document — clusters of chunks that are highly relevant **and** contextually cohesive.\n",
    "\n",
    "\n",
    "\n",
    "### 🧾 Interpreting the Output\n",
    "\n",
    "Once the optimizer finishes, it returns:\n",
    "- The **indices** of the best segments (start and end positions of each cluster),\n",
    "- The **value scores** of each segment (total relevance after applying the penalty).\n",
    "\n",
    "This gives us a clean and interpretable way to **identify exactly which parts of the document are worth passing to the LLM** — not based on arbitrary rules, but on quantified relevance and context density.\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Why This Matters\n",
    "\n",
    "Instead of feeding isolated chunks into the LLM — which may be individually relevant but lack surrounding context — we now extract **tight, coherent clusters**. These segments:\n",
    "- Preserve narrative flow,\n",
    "- Carry stronger semantic signals,\n",
    "- And lead to significantly better downstream answer quality in RAG pipelines.\n",
    "\n",
    "This method offers a **principled, efficient way to ground generation in the best parts of the document**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2421f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some parameters and constraints for the optimization\n",
    "irrelevant_chunk_penalty = 0.2 # empirically, something around 0.2 works well; lower values bias towards longer segments\n",
    "max_length = 20\n",
    "overall_max_length = 30\n",
    "minimum_value = 0.7\n",
    "\n",
    "# subtract constant threshold value from chunk relevance values\n",
    "relevance_values = [v - irrelevant_chunk_penalty for v in chunk_values] \n",
    "\n",
    "# run the optimization\n",
    "best_segments, scores = get_best_segments(relevance_values, max_length, overall_max_length, minimum_value)\n",
    "\n",
    "# print results\n",
    "print (\"Best segment indices\")\n",
    "print (best_segments) # indices of the best segments, with the end index non-inclusive\n",
    "print ()\n",
    "print (\"Best segment scores\")\n",
    "print (scores)\n",
    "print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c5f3f",
   "metadata": {},
   "source": [
    "## 🔎 What Happens When Only a Single Chunk is Relevant?\n",
    "\n",
    "RSE (Relevant Segment Extraction) isn’t just for finding large, dense clusters of relevant content — it’s also smart enough to handle the opposite case.\n",
    "\n",
    "When the answer to a query is contained in just **one or two isolated chunks**, RSE doesn’t try to forcefully build large segments. Instead, it gracefully falls back to behavior that resembles **top-k retrieval**, returning those specific high-scoring chunks on their own. \n",
    "\n",
    "This adaptive behavior makes RSE robust:  \n",
    "- It performs well whether the information is **spread across multiple nearby chunks**  \n",
    "- Or **concentrated in just a single chunk**\n",
    "\n",
    "If you visualize the chunk relevance values for such a query, you’ll typically see isolated spikes instead of smooth clusters — and RSE will correctly identify and return those spikes.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Performance Impact (Based on Public Benchmarks)\n",
    "\n",
    "Although we haven’t run the evaluations ourselves, existing benchmarks suggest that RSE leads to **significant improvements in answer quality** over standard top-k retrieval — even when cost and context size are kept roughly constant.\n",
    "\n",
    "Some key takeaways from the evaluations:\n",
    "- RSE consistently outperforms top-k retrieval across **diverse datasets** — including long PDFs, markdown handbooks, court opinions, and 10-K filings.\n",
    "- Average quality scores improved by over **40%** in certain settings.\n",
    "- When paired with techniques like **Contextual Chunk Headers (CCH)**, RSE has shown strong gains on finance-focused QA tasks as well.\n",
    "\n",
    "These results reinforce the core value of RSE:  \n",
    "By focusing not just on individual chunk scores but also on **how relevant chunks cluster together**, RSE provides the LLM with richer, more coherent context — leading to more accurate answers.\n",
    "\n",
    "---\n",
    "\n",
    "✅ Whether you're dealing with scattered facts or dense documents, RSE adapts intelligently to return the best possible input for your RAG system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9de05c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📘 Summary & Credits\n",
    "\n",
    "This notebook is based on the excellent open-source repository [RAG_Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques).  \n",
    "I referred to that work to understand how the pipeline is structured and then reimplemented the same concept in a **fully self-contained** way, but using recent models — as part of my personal learning journey.\n",
    "\n",
    "The purpose of this notebook is purely **educational**:  \n",
    "- To deepen my understanding of Retrieval-Augmented Generation systems  \n",
    "- To keep a clean, trackable log of what I’ve built and learned  \n",
    "- And to serve as a future reference for myself or others starting from scratch\n",
    "\n",
    "To support that, I’ve added clear, concise markdowns throughout the notebook — explaining *why* each package was installed, *why* each line of code exists, and *how* each component fits into the overall RAG pipeline. It’s designed to help anyone (including my future self) grasp the **how** and the **why**, not just the **what**.\n",
    "\n",
    "## 🔍 Why Use RSE in RAG?\n",
    "\n",
    "Standard top-k retrieval often returns scattered chunks, missing the fact that relevant information is **clustered** in documents.\n",
    "\n",
    "**Relevant Segment Extraction (RSE)** solves this by:\n",
    "- 📊 Scoring chunks in context using a **sliding window**\n",
    "- 🧱 Extracting **coherent high-relevance segments** instead of isolated chunks\n",
    "- 🎯 Improving **context quality** for LLMs with minimal extra cost\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What’s New in This Version?\n",
    "\n",
    "This RSE implementation includes:\n",
    "\n",
    "- 📊 **Smoothed relevance scoring** across neighbors  \n",
    "- 🧠 **Local maxima-based segment detection**  \n",
    "- ⚙️ **Greedy selection of best non-overlapping spans**  \n",
    "- 📦 **Lightweight, reusable logic** for RAG pipelines  \n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Inferences & Key Takeaways\n",
    "\n",
    "- 🚀 RSE improves grounding and relevance over top-k retrieval  \n",
    "- 🧠 Helps especially on **long, dense, or structured documents**  \n",
    "- 🔄 Falls back to top-k behavior when only isolated chunks are relevant  \n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 What Could Be Added Next?\n",
    "\n",
    "- 🤖 Plug into rerankers like Cohere or GPT-4o  \n",
    "- 📊 Evaluate on datasets like KITE or FinanceBench  \n",
    "- 🔧 Wrap as a LangChain retriever for modular use  \n",
    "\n",
    "---\n",
    "## 💡 Final Word\n",
    "\n",
    "This notebook is part of my larger personal project: **RAG100x** — a challenge to build and log my journney in RAG from 0 100 in the coming months.\n",
    "\n",
    "It’s not built to impress — it’s built to **progress**.  \n",
    "Everything here is structured to enable **daily iteration**, focused experimentation, and clean documentation.\n",
    "\n",
    "If you're exploring RAG from first principles, feel free to use this as a scaffold for your own builds. And of course — check out the original repository for broader implementations and ideas."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
