{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ebecedf",
   "metadata": {},
   "source": [
    "## üß© Relevant Segment Extraction (RSE) for Better Context | RAG100X\n",
    "\n",
    "This notebook implements **Relevant Segment Extraction (RSE)** ‚Äî a retrieval-time optimization that reconstructs the *most contextually useful* segments from your documents by selecting **contiguous runs of relevant chunks**, not just isolated ones.\n",
    "\n",
    "Instead of naively picking the Top-k highest scoring chunks, RSE uses a reranking model (like Cohere Rerank) to assign relevance scores to all chunks, and then intelligently finds the **best spans** of text ‚Äî even if some relevant chunks weren‚Äôt individually top-ranked.\n",
    "\n",
    "The result? Better grounding, smoother flow, and improved performance for LLM answers.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What You‚Äôll Learn\n",
    "\n",
    "- Why Top-k retrieval can lead to broken context and hallucinations  \n",
    "- How RSE scores and selects the *best chunk spans*, not individual pieces  \n",
    "- How segment optimization helps recover missing context that matters  \n",
    "- When RSE outperforms vanilla retrieval in real-world QA tasks  \n",
    "\n",
    "---\n",
    "\n",
    "### üîç Real-world Analogy\n",
    "\n",
    "Imagine you‚Äôre watching a movie, and someone asks:\n",
    "\n",
    "> *\"What caused the main character‚Äôs breakdown?\"*\n",
    "\n",
    "If you only watch 3 random dramatic scenes (Top-k chunks), you‚Äôll miss the full story.\n",
    "\n",
    "But if you instead watch a **full 5-minute clip** leading up to the breakdown ‚Äî even if it includes a few ‚Äúboring‚Äù moments ‚Äî you‚Äôll get the full emotional arc.\n",
    "\n",
    "‚úÖ **RSE gives you that full arc ‚Äî not just scattered scenes.**\n",
    "\n",
    "---\n",
    "\n",
    "### üî¨ How RSE Works Under the Hood\n",
    "\n",
    "Let‚Äôs say your document is split into 500 chunks. For a given query, we want to extract the *best possible segments* ‚Äî ideally, groups of neighboring chunks that flow well together and are jointly relevant.\n",
    "\n",
    "| Step               | What Happens                                                                 |\n",
    "|--------------------|------------------------------------------------------------------------------|\n",
    "| 1. Chunking        | Document is split into non-overlapping chunks using LangChain‚Äôs splitter     |\n",
    "| 2. Reranking       | Each chunk is scored using Cohere Rerank for its relevance to the query     |\n",
    "| 3. Value Mapping   | Scores are converted into ‚Äúchunk values‚Äù ‚Äî good chunks = +ve, bad = ‚Äìve     |\n",
    "| 4. Segmentation    | A search algorithm finds **contiguous spans** with high total chunk value   |\n",
    "| 5. Filtering       | Segments are pruned based on token budget, redundancy, and quality          |\n",
    "| 6. Output          | The selected segments are passed to the LLM as context                      |\n",
    "\n",
    "üß† Even if a few chunks inside a span are low scoring, they might **complete a relevant section** ‚Äî so we include them if the segment as a whole is valuable.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Why This Works So Well\n",
    "\n",
    "- üß± **Context continuity**: Relevant ideas rarely exist in isolation ‚Äî they live in flow  \n",
    "- ü§ñ **LLMs love coherence**: Chunks from the same topic block improve grounding and reduce hallucination  \n",
    "- üéØ **Span-level optimization**: Instead of picking top chunks, RSE picks *top stories*  \n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è Why This Matters in Production\n",
    "\n",
    "Imagine this chunk retrieved on its own:\n",
    "\n",
    "> *‚ÄúThe final experiment used a dropout rate of 0.2 and achieved 87% accuracy.‚Äù*\n",
    "\n",
    "It‚Äôs informative ‚Äî but out of context. What was the task? Why that setup? What came before?\n",
    "\n",
    "With RSE, we retrieve a **span** like:\n",
    "\n",
    "1. *‚ÄúDataset and Preprocessing‚Ä¶‚Äù*  \n",
    "2. *‚ÄúModel Architecture: 3-layer BiLSTM‚Ä¶‚Äù*  \n",
    "3. *‚ÄúFinal experiment used a dropout rate‚Ä¶‚Äù*\n",
    "\n",
    "That full thread provides **complete reasoning**, improving both faithfulness and fluency.\n",
    "\n",
    "**RSE ensures retrieval returns meaningful *segments*, not fragmented sentences.**\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Where This Fits in RAG100X\n",
    "\n",
    "So far in RAG100X, we‚Äôve explored:\n",
    "\n",
    "1. PDF-based QA  \n",
    "2. CSV-based semantic search  \n",
    "3. DeepLearning.ai RAG QA  \n",
    "4. Chunk size & latency tuning  \n",
    "5. Proposition-aware chunking  \n",
    "6. Query rewriting + decomposition  \n",
    "7. HyDE: Embed imagined answers  \n",
    "8. HyPE: Embed imagined questions  \n",
    "9. CCH: Add intelligent titles to chunks  \n",
    "\n",
    "Now in **Day 10**, we zoom in on post-retrieval ‚Äî and **optimize which chunks make it into the final context**.\n",
    "\n",
    "> üí° **RSE doesn't just pick relevant chunks ‚Äî it picks relevant *stories*.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bec1bd",
   "metadata": {},
   "source": [
    "## üì¶ Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f23ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install matplotlib numpy python-dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from scipy.stats import beta\n",
    "import matplotlib.pyplot as plt\n",
    "import cohere\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "os.environ[\"CO_API_KEY\"] = os.getenv('CO_API_KEY') # Cohere API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c300a9f",
   "metadata": {},
   "source": [
    "## üß© Define Helper Functions for Chunk Scoring with Cohere Reranker\n",
    "\n",
    "Before we extract relevant segments, we need a way to **measure how relevant each chunk is** to a given user query.\n",
    "\n",
    "In this section:\n",
    "\n",
    "- We'll define helper functions to **split raw text into chunks**, **score them using Cohere's rerank API**, and **visualize the results**.\n",
    "- Since we're only working with a single document, we skip vector search and send all chunks directly to the reranker.\n",
    "\n",
    "### üìå Functions Defined\n",
    "\n",
    "1. `split_into_chunks`: Uses LangChain‚Äôs `RecursiveCharacterTextSplitter` to break large documents into overlapping or non-overlapping chunks.\n",
    "2. `transform`: Applies a Beta CDF to spread Cohere's relevance scores (which are often bunched near 0 or 1) into a smoother 0‚Äì1 range.\n",
    "3. `rerank_chunks`: Calls Cohere‚Äôs `rerank` API to assign relevance scores to each chunk based on a query.\n",
    "4. `plot_relevance_scores`: Visualizes the relevance scores across all chunks.\n",
    "\n",
    "These scores will later help us **extract only the most relevant segments**, minimizing hallucinations and improving grounding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572bd2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Function to split a document into smaller chunks using LangChain's RecursiveCharacterTextSplitter\n",
    "def split_into_chunks(text: str, chunk_size: int = 800) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits a large document into overlapping or non-overlapping chunks.\n",
    "\n",
    "    Args:\n",
    "        text (str): Full text of the document.\n",
    "        chunk_size (int): Size of each chunk in characters.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of string chunks.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=0, length_function=len\n",
    "    )\n",
    "    texts = text_splitter.create_documents([text])\n",
    "    chunks = [text.page_content for text in texts]\n",
    "    return chunks\n",
    "\n",
    "# Beta transformation to smooth out Cohere's sharp relevance scores (usually close to 0 or 1)\n",
    "def transform(x: float) -> float:\n",
    "    \"\"\"\n",
    "    Transforms the sharp relevance scores from Cohere Rerank into smoother values.\n",
    "\n",
    "    Args:\n",
    "        x (float): Raw relevance score from the reranker.\n",
    "\n",
    "    Returns:\n",
    "        float: Smoothed score using the beta cumulative distribution function.\n",
    "    \"\"\"\n",
    "    a, b = 0.4, 0.4  # Tunable parameters controlling smoothness\n",
    "    return beta.cdf(x, a, b)\n",
    "\n",
    "# Use Cohere Rerank API to evaluate relevance of each chunk to the input query\n",
    "def rerank_chunks(query: str, chunks: List[str]):\n",
    "    \"\"\"\n",
    "    Reranks all chunks using Cohere's rerank model and computes relevance-weighted scores.\n",
    "\n",
    "    Args:\n",
    "        query (str): Natural language query.\n",
    "        chunks (List[str]): List of document chunks to score.\n",
    "\n",
    "    Returns:\n",
    "        similarity_scores (List[float]): Normalized similarity scores.\n",
    "        chunk_values (List[float]): Final chunk importance scores (with decay applied).\n",
    "    \"\"\"\n",
    "    model = \"rerank-english-v3.0\"\n",
    "    client = cohere.Client(api_key=os.environ[\"CO_API_KEY\"])\n",
    "    decay_rate = 30  # Controls how quickly lower-ranked chunks decay in importance\n",
    "\n",
    "    reranked_results = client.rerank(model=model, query=query, documents=chunks)\n",
    "    results = reranked_results.results\n",
    "\n",
    "    reranked_indices = [r.index for r in results]\n",
    "    raw_scores = [r.relevance_score for r in results]\n",
    "\n",
    "    similarity_scores = [0] * len(chunks)\n",
    "    chunk_values = [0] * len(chunks)\n",
    "\n",
    "    for i, index in enumerate(reranked_indices):\n",
    "        abs_score = transform(raw_scores[i])\n",
    "        similarity_scores[index] = abs_score\n",
    "        # Apply exponential decay based on the rank\n",
    "        chunk_values[index] = np.exp(-i / decay_rate) * abs_score\n",
    "\n",
    "    return similarity_scores, chunk_values\n",
    "\n",
    "# Visualize how relevant each chunk is to the input query\n",
    "def plot_relevance_scores(chunk_values: List[float], start_index: int = None, end_index: int = None) -> None:\n",
    "    \"\"\"\n",
    "    Plots a scatter graph showing how relevant each chunk is to the query.\n",
    "\n",
    "    Args:\n",
    "        chunk_values (List[float]): Importance scores per chunk.\n",
    "        start_index (int, optional): Starting chunk index for the plot.\n",
    "        end_index (int, optional): Ending chunk index for the plot.\n",
    "    \"\"\"\n",
    "    if start_index is None:\n",
    "        start_index = 0\n",
    "    if end_index is None:\n",
    "        end_index = len(chunk_values)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.title(\"üîç Query-to-Chunk Relevance Across Document\")\n",
    "    plt.xlabel(\"Chunk Index\")\n",
    "    plt.ylabel(\"Relevance Score\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.scatter(range(start_index, end_index), chunk_values[start_index:end_index])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ab0e6",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e177bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the input document\n",
    "FILE_PATH = \"data/nike_2023_annual_report.txt\"\n",
    "\n",
    "with open(FILE_PATH, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "chunks = split_into_chunks(text, chunk_size=800)\n",
    "\n",
    "print (f\"Split the document into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21e5fda",
   "metadata": {},
   "source": [
    "## üìä Visualize Chunk Relevance Across a Single Document\n",
    "\n",
    "Now that we‚Äôve defined the reranker and scoring helpers, let‚Äôs test them on a real query.\n",
    "\n",
    "In this step:\n",
    "- We pass a **realistic financial query** to the reranker.\n",
    "- We **visualize how relevant each chunk** in the document is.\n",
    "- This helps us identify which parts of the document should be used to construct a grounded response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78b15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Nike consolidated financial statements\"\n",
    "similarity_scores, chunk_values = rerank_chunks(query, chunks)\n",
    "plot_relevance_scores(chunk_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4979a5",
   "metadata": {},
   "source": [
    "## üìä Interpreting the Chunk Relevance Plot\n",
    "\n",
    "The plot above visualizes how relevant each document chunk is to the input query.\n",
    "\n",
    "- The **x-axis** shows the chunk index ‚Äî starting from 0 for the first chunk in the document.\n",
    "- The **y-axis** shows the **final relevance score**, which is not just the raw similarity score from the reranker.\n",
    "\n",
    "### üß† How is the Relevance Score Computed?\n",
    "\n",
    "Each chunk is passed through a reranker (like Cohere), which gives:\n",
    "- A **raw relevance score** ‚Äî how semantically similar the chunk is to the query.\n",
    "- A **rank** ‚Äî where it stands among all chunks in terms of similarity (1st, 2nd, etc.).\n",
    "\n",
    "Instead of relying only on one of these signals, we combine them:\n",
    "- The **rank** is converted using an **exponential decay function** ‚Äî so the top-ranked chunks contribute more to the final score.\n",
    "- This decay value is **multiplied with the raw relevance score**.\n",
    "- Optionally, a **beta transformation** is applied to distribute the scores more smoothly.\n",
    "\n",
    "This makes the score both **position-aware** (top results matter more) and **content-aware** (chunks still need semantic similarity).\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Zooming into a Relevant Cluster\n",
    "\n",
    "We noticed that the similarity scores in the range of **chunk indices 320‚Äì333** were noticeably higher than others, indicating a dense cluster of relevance.\n",
    "\n",
    "To examine this section more closely:\n",
    "\n",
    "```python\n",
    "plot_relevance_scores(chunk_values, 320, 340)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51d401",
   "metadata": {},
   "source": [
    "## üîç Understanding the Retrieved Document Segment\n",
    "\n",
    "After identifying a cluster of high-relevance chunks between **chunk 320 and 340**, we inspect the actual content and find something insightful.\n",
    "\n",
    "From our observation:\n",
    "- **Chunk 323** starts the section titled **\"Consolidated Statement of Income\"**.\n",
    "- The following chunks up to **chunk 333** contain **detailed financial statements** ‚Äî exactly what our query is looking for.\n",
    "\n",
    "This means that **all the chunks from 323 to 333 are semantically relevant** to the query:  \n",
    "**\"Nike consolidated financial statements\"**.\n",
    "\n",
    "---\n",
    "\n",
    "### ü§î Then Why Were Only Half Marked as Relevant?\n",
    "\n",
    "Here‚Äôs what this means:\n",
    "\n",
    "- The reranker model evaluates each chunk **independently**, and returns a binary decision:  \n",
    "  ‚úÖ **Relevant** or ‚ùå **Not Relevant**.\n",
    "- In the case of chunks 323‚Äì333:\n",
    "  - Only about **half** were labeled as **relevant** by the reranker.\n",
    "  - The **others were marked as irrelevant**, even though they‚Äôre clearly part of the same logical section.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Why Does This Happen?\n",
    "\n",
    "The reranker doesn‚Äôt have access to **context beyond individual chunks**. So:\n",
    "\n",
    "- Some chunks may contain **section headers or highly keyword-aligned sentences** ‚Äî these get marked as relevant.\n",
    "- Others may contain **numeric tables, continuation of previous content, or subtler phrasing** ‚Äî these don‚Äôt match the query as explicitly, so they‚Äôre marked as irrelevant.\n",
    "\n",
    "> In short, **chunks that are contextually important but less textually aligned** might be ignored by the reranker.\n",
    "\n",
    "This leads to a situation where **important but low-signal chunks are sandwiched between high-signal ones**, and get missed in naive filtering.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Why This Matters\n",
    "\n",
    "If we blindly include only those chunks marked as relevant:\n",
    "- We may **break the logical continuity** of the document section.\n",
    "- The LLM might **miss key information**, especially in structured or tabular formats.\n",
    "\n",
    "By instead considering **clusters of adjacent relevant chunks**, and including the chunks in between, we can:\n",
    "- Provide the LLM with a **more coherent and complete context**.\n",
    "- Avoid missing content that‚Äôs **crucial but subtle**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Takeaway\n",
    "\n",
    "- Binary relevance labels are **not enough** ‚Äî proximity and continuity also matter.\n",
    "- For structured documents like financial reports, always consider **nearby chunks as potential context**, even if they weren‚Äôt ranked as top matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feea1e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_document_segment(chunks: List[str], start_index: int, end_index: int):\n",
    "    \"\"\"\n",
    "    Print the text content of a segment of the document\n",
    "\n",
    "    Args:\n",
    "        chunks (list): List of text chunks\n",
    "        start_index (int): Start index of the segment\n",
    "        end_index (int): End index of the segment (not inclusive)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Prints:\n",
    "        The text content of the specified segment of the document\n",
    "    \"\"\"\n",
    "    for i in range(start_index, end_index):\n",
    "        print(f\"\\nChunk {i}\")\n",
    "        print(chunks[i])\n",
    "\n",
    "print_document_segment(chunks, 320, 340)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147b5f88",
   "metadata": {},
   "source": [
    "## üß© Why Use Clusters of Relevant Chunks?\n",
    "\n",
    "When answering complex or structured queries, **individual chunks** often fail to provide enough context. But when we group **contiguous relevant chunks together as a cluster**, we preserve the **semantic continuity** of the original document ‚Äî something large language models (LLMs) benefit from significantly.\n",
    "\n",
    "Instead of feeding isolated chunks to the LLM, we now aim to feed **entire segments of highly relevant, adjacent chunks**.\n",
    "\n",
    "\n",
    "## üß† How Do We Find These Clusters?\n",
    "\n",
    "This is where the challenge lies:  \n",
    "We need to **automatically detect groups of adjacent chunks** that together form a high-quality, coherent segment.\n",
    "\n",
    "To do this efficiently, we reframe the problem as a variation of the **maximum subarray problem**, which is a well-known algorithmic problem with efficient solutions.\n",
    "\n",
    "But how?\n",
    "\n",
    "\n",
    "\n",
    "## üî¢ Defining Chunk Values for Optimization\n",
    "\n",
    "We already have **relevance scores** (e.g., from a reranker) for each chunk ‚Äî typically between `0` and `1`. To use them in an optimization algorithm, we tweak them slightly:\n",
    "\n",
    "### ‚ûï Relevant chunks ‚Üí Positive score  \n",
    "### ‚ûñ Irrelevant chunks ‚Üí Negative score  \n",
    "\n",
    "To make this transformation:\n",
    "- We subtract a constant value (say, **0.2**) from each relevance score.\n",
    "- This shifts **low-relevance chunks below zero**, making them penalize the segment value.\n",
    "- We call this constant the `irrelevant_chunk_penalty`.\n",
    "\n",
    "This setup ensures that the more relevant chunks a segment has ‚Äî and the fewer irrelevant ones ‚Äî the **higher the total score**.\n",
    "\n",
    "\n",
    "\n",
    "## üîç What Does the Algorithm Do?\n",
    "\n",
    "The function `get_best_segments(...)` solves this modified version of the **maximum subarray problem**, with some added constraints:\n",
    "\n",
    "1. **Inputs:**\n",
    "   - A list of **relevance values** (after applying the penalty).\n",
    "   - A **max length** for a single segment.\n",
    "   - A **total length budget** for all returned segments.\n",
    "   - A **minimum score** required to consider a segment \"good\".\n",
    "\n",
    "2. **Goal:**  \n",
    "   Find non-overlapping segments of chunks that:\n",
    "   - Stay within the max length.\n",
    "   - Stay within the overall chunk budget.\n",
    "   - Have a total score above a threshold.\n",
    "   - Don‚Äôt overlap with previously selected segments.\n",
    "\n",
    "3. **Output:**  \n",
    "   A list of `(start_index, end_index)` pairs representing the best segments to include, plus their respective scores.\n",
    "\n",
    "\n",
    "## ‚úÖ Why This Works\n",
    "\n",
    "This approach ensures:\n",
    "- **High-density clusters of relevance** are selected, not just isolated peaks.\n",
    "- **Adjacent useful chunks are grouped**, preserving context.\n",
    "- **Irrelevant noise is penalized**, making the output cleaner.\n",
    "\n",
    "This is especially powerful for documents like financial reports or technical documentation, where meaningful information spans multiple nearby chunks, and context is everything.\n",
    "\n",
    "\n",
    "## üí° Summary\n",
    "\n",
    "By reframing segment selection as a **scoring and optimization problem**, we can smartly select the most useful and coherent parts of a document. This significantly boosts the **retrieval quality** and ultimately leads to more accurate, grounded, and complete answers from the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c435853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_segments(relevance_values: list, max_length: int, overall_max_length: int, minimum_value: float):\n",
    "    \"\"\"\n",
    "    This function takes the chunk relevance values and then runs an optimization algorithm to find the best segments. In more technical terms, it solves a constrained version of the maximum sum subarray problem.\n",
    "\n",
    "    Note: this is a simplified implementation intended for demonstration purposes. A more sophisticated implementation would be needed for production use and is available in the dsRAG library.\n",
    "\n",
    "    Args:\n",
    "        relevance_values (list): a list of relevance values for each chunk of a document\n",
    "        max_length (int): the maximum length of a single segment (measured in number of chunks)\n",
    "        overall_max_length (int): the maximum length of all segments (measured in number of chunks)\n",
    "        minimum_value (float): the minimum value that a segment must have to be considered\n",
    "\n",
    "    Returns:\n",
    "        best_segments (list): a list of tuples (start, end) that represent the indices of the best segments (the end index is non-inclusive) in the document\n",
    "        scores (list): a list of the scores for each of the best segments\n",
    "    \"\"\"\n",
    "    best_segments = []\n",
    "    scores = []\n",
    "    total_length = 0\n",
    "    while total_length < overall_max_length:\n",
    "        # find the best remaining segment\n",
    "        best_segment = None\n",
    "        best_value = -1000\n",
    "        for start in range(len(relevance_values)):\n",
    "            # skip over negative value starting points\n",
    "            if relevance_values[start] < 0:\n",
    "                continue\n",
    "            for end in range(start+1, min(start+max_length+1, len(relevance_values)+1)):\n",
    "                # skip over negative value ending points\n",
    "                if relevance_values[end-1] < 0:\n",
    "                    continue\n",
    "                # check if this segment overlaps with any of the best segments and skip if it does\n",
    "                if any(start < seg_end and end > seg_start for seg_start, seg_end in best_segments):\n",
    "                    continue\n",
    "                # check if this segment would push us over the overall max length and skip if it would\n",
    "                if total_length + end - start > overall_max_length:\n",
    "                    continue\n",
    "                \n",
    "                # define segment value as the sum of the relevance values of its chunks\n",
    "                segment_value = sum(relevance_values[start:end])\n",
    "                if segment_value > best_value:\n",
    "                    best_value = segment_value\n",
    "                    best_segment = (start, end)\n",
    "        \n",
    "        # if we didn't find a valid segment then we're done\n",
    "        if best_segment is None or best_value < minimum_value:\n",
    "            break\n",
    "\n",
    "        # otherwise, add the segment to the list of best segments\n",
    "        best_segments.append(best_segment)\n",
    "        scores.append(best_value)\n",
    "        total_length += best_segment[1] - best_segment[0]\n",
    "    \n",
    "    return best_segments, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7b2f2",
   "metadata": {},
   "source": [
    "## üß© Segment Optimization: Selecting the Most Valuable Context\n",
    "\n",
    "Now that we've defined how to score contiguous chunks using our relevance-based method, the next step is to **actually run the optimization** and select the most valuable text segments to pass to the LLM.\n",
    "\n",
    "\n",
    "\n",
    "### ‚öôÔ∏è Setting the Optimization Parameters\n",
    "\n",
    "We first define a few important constraints and thresholds that guide the optimization process:\n",
    "\n",
    "- **Irrelevant Chunk Penalty (`0.2`)**:  \n",
    "  This value is subtracted from each chunk's relevance score to convert it into a \"value.\" Chunks with low original relevance will now have negative values, while highly relevant ones remain positive. This makes it easier to find high-density regions of relevance using a simple sum.\n",
    "\n",
    "- **Maximum Segment Length (`20` chunks)**:  \n",
    "  This restricts how long a single segment can be. It ensures we don‚Äôt create excessively large spans, which could reduce precision.\n",
    "\n",
    "- **Overall Maximum Length (`30` chunks)**:  \n",
    "  This is a hard limit on the total number of chunks that can be selected across all segments. It helps control context size and prevents the model input from overflowing.\n",
    "\n",
    "- **Minimum Value Threshold (`0.7`)**:  \n",
    "  This ensures that only meaningful, high-value segments are retained. Segments with total value below this threshold are discarded.\n",
    "\n",
    "\n",
    "\n",
    "### ‚ûñ Converting Relevance Scores into Optimization-Friendly Values\n",
    "\n",
    "To enable the optimizer to find high-value clusters, we convert each chunk‚Äôs relevance score into a new value by subtracting a constant penalty. This transformation ensures:\n",
    "- **Positive values** indicate useful, highly relevant chunks.\n",
    "- **Negative values** indicate noise or irrelevant content.\n",
    "\n",
    "This lets us use a greedy optimization approach to find sequences of chunks with maximum total value ‚Äî i.e., those that are not only relevant but tightly packed together in the document.\n",
    "\n",
    "\n",
    "\n",
    "### üöÄ Running the Optimizer\n",
    "\n",
    "We now run the segment selection algorithm. It scans through the adjusted chunk values and selects a set of **non-overlapping segments** that:\n",
    "- Respect the `max_length` and `overall_max_length` constraints,\n",
    "- Do not overlap with each other,\n",
    "- And each individually score above the `minimum_value` threshold.\n",
    "\n",
    "This results in a list of the most valuable segments from the document ‚Äî clusters of chunks that are highly relevant **and** contextually cohesive.\n",
    "\n",
    "\n",
    "\n",
    "### üßæ Interpreting the Output\n",
    "\n",
    "Once the optimizer finishes, it returns:\n",
    "- The **indices** of the best segments (start and end positions of each cluster),\n",
    "- The **value scores** of each segment (total relevance after applying the penalty).\n",
    "\n",
    "This gives us a clean and interpretable way to **identify exactly which parts of the document are worth passing to the LLM** ‚Äî not based on arbitrary rules, but on quantified relevance and context density.\n",
    "\n",
    "\n",
    "\n",
    "### ‚úÖ Why This Matters\n",
    "\n",
    "Instead of feeding isolated chunks into the LLM ‚Äî which may be individually relevant but lack surrounding context ‚Äî we now extract **tight, coherent clusters**. These segments:\n",
    "- Preserve narrative flow,\n",
    "- Carry stronger semantic signals,\n",
    "- And lead to significantly better downstream answer quality in RAG pipelines.\n",
    "\n",
    "This method offers a **principled, efficient way to ground generation in the best parts of the document**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2421f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some parameters and constraints for the optimization\n",
    "irrelevant_chunk_penalty = 0.2 # empirically, something around 0.2 works well; lower values bias towards longer segments\n",
    "max_length = 20\n",
    "overall_max_length = 30\n",
    "minimum_value = 0.7\n",
    "\n",
    "# subtract constant threshold value from chunk relevance values\n",
    "relevance_values = [v - irrelevant_chunk_penalty for v in chunk_values] \n",
    "\n",
    "# run the optimization\n",
    "best_segments, scores = get_best_segments(relevance_values, max_length, overall_max_length, minimum_value)\n",
    "\n",
    "# print results\n",
    "print (\"Best segment indices\")\n",
    "print (best_segments) # indices of the best segments, with the end index non-inclusive\n",
    "print ()\n",
    "print (\"Best segment scores\")\n",
    "print (scores)\n",
    "print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c5f3f",
   "metadata": {},
   "source": [
    "## üîé What Happens When Only a Single Chunk is Relevant?\n",
    "\n",
    "RSE (Relevant Segment Extraction) isn‚Äôt just for finding large, dense clusters of relevant content ‚Äî it‚Äôs also smart enough to handle the opposite case.\n",
    "\n",
    "When the answer to a query is contained in just **one or two isolated chunks**, RSE doesn‚Äôt try to forcefully build large segments. Instead, it gracefully falls back to behavior that resembles **top-k retrieval**, returning those specific high-scoring chunks on their own. \n",
    "\n",
    "This adaptive behavior makes RSE robust:  \n",
    "- It performs well whether the information is **spread across multiple nearby chunks**  \n",
    "- Or **concentrated in just a single chunk**\n",
    "\n",
    "If you visualize the chunk relevance values for such a query, you‚Äôll typically see isolated spikes instead of smooth clusters ‚Äî and RSE will correctly identify and return those spikes.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Performance Impact (Based on Public Benchmarks)\n",
    "\n",
    "Although we haven‚Äôt run the evaluations ourselves, existing benchmarks suggest that RSE leads to **significant improvements in answer quality** over standard top-k retrieval ‚Äî even when cost and context size are kept roughly constant.\n",
    "\n",
    "Some key takeaways from the evaluations:\n",
    "- RSE consistently outperforms top-k retrieval across **diverse datasets** ‚Äî including long PDFs, markdown handbooks, court opinions, and 10-K filings.\n",
    "- Average quality scores improved by over **40%** in certain settings.\n",
    "- When paired with techniques like **Contextual Chunk Headers (CCH)**, RSE has shown strong gains on finance-focused QA tasks as well.\n",
    "\n",
    "These results reinforce the core value of RSE:  \n",
    "By focusing not just on individual chunk scores but also on **how relevant chunks cluster together**, RSE provides the LLM with richer, more coherent context ‚Äî leading to more accurate answers.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ Whether you're dealing with scattered facts or dense documents, RSE adapts intelligently to return the best possible input for your RAG system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9de05c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìò Summary & Credits\n",
    "\n",
    "This notebook is based on the excellent open-source repository [RAG_Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques).  \n",
    "I referred to that work to understand how the pipeline is structured and then reimplemented the same concept in a **fully self-contained** way, but using recent models ‚Äî as part of my personal learning journey.\n",
    "\n",
    "The purpose of this notebook is purely **educational**:  \n",
    "- To deepen my understanding of Retrieval-Augmented Generation systems  \n",
    "- To keep a clean, trackable log of what I‚Äôve built and learned  \n",
    "- And to serve as a future reference for myself or others starting from scratch\n",
    "\n",
    "To support that, I‚Äôve added clear, concise markdowns throughout the notebook ‚Äî explaining *why* each package was installed, *why* each line of code exists, and *how* each component fits into the overall RAG pipeline. It‚Äôs designed to help anyone (including my future self) grasp the **how** and the **why**, not just the **what**.\n",
    "\n",
    "## üîç Why Use RSE in RAG?\n",
    "\n",
    "Standard top-k retrieval often returns scattered chunks, missing the fact that relevant information is **clustered** in documents.\n",
    "\n",
    "**Relevant Segment Extraction (RSE)** solves this by:\n",
    "- üìä Scoring chunks in context using a **sliding window**\n",
    "- üß± Extracting **coherent high-relevance segments** instead of isolated chunks\n",
    "- üéØ Improving **context quality** for LLMs with minimal extra cost\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What‚Äôs New in This Version?\n",
    "\n",
    "This RSE implementation includes:\n",
    "\n",
    "- üìä **Smoothed relevance scoring** across neighbors  \n",
    "- üß† **Local maxima-based segment detection**  \n",
    "- ‚öôÔ∏è **Greedy selection of best non-overlapping spans**  \n",
    "- üì¶ **Lightweight, reusable logic** for RAG pipelines  \n",
    "\n",
    "---\n",
    "\n",
    "## üìà Inferences & Key Takeaways\n",
    "\n",
    "- üöÄ RSE improves grounding and relevance over top-k retrieval  \n",
    "- üß† Helps especially on **long, dense, or structured documents**  \n",
    "- üîÑ Falls back to top-k behavior when only isolated chunks are relevant  \n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ What Could Be Added Next?\n",
    "\n",
    "- ü§ñ Plug into rerankers like Cohere or GPT-4o  \n",
    "- üìä Evaluate on datasets like KITE or FinanceBench  \n",
    "- üîß Wrap as a LangChain retriever for modular use  \n",
    "\n",
    "---\n",
    "## üí° Final Word\n",
    "\n",
    "This notebook is part of my larger personal project: **RAG100x** ‚Äî a challenge to build and log my journney in RAG from 0 100 in the coming months.\n",
    "\n",
    "It‚Äôs not built to impress ‚Äî it‚Äôs built to **progress**.  \n",
    "Everything here is structured to enable **daily iteration**, focused experimentation, and clean documentation.\n",
    "\n",
    "If you're exploring RAG from first principles, feel free to use this as a scaffold for your own builds. And of course ‚Äî check out the original repository for broader implementations and ideas."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
