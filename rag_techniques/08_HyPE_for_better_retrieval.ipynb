{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "becace90",
   "metadata": {},
   "source": [
    "## üß† Hypothetical Prompt Embeddings (HyPE) for Better Retrieval | RAG100X\n",
    "\n",
    "This notebook implements **Hypothetical Prompt Embeddings (HyPE)** ‚Äî a powerful technique for improving document retrieval in RAG systems by precomputing synthetic questions for every chunk of your documents.\n",
    "\n",
    "Instead of embedding the raw document chunks (which might be long, descriptive, or poorly aligned with a user's query), HyPE uses an LLM to generate multiple relevant questions for each chunk ahead of time ‚Äî and stores those questions in a vector database. At retrieval time, the user‚Äôs query is matched directly against these synthetic questions, leading to much more semantically aligned retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What You‚Äôll Learn\n",
    "\n",
    "- Why raw document embeddings often fail to match real user queries  \n",
    "- How HyPE turns every document chunk into a set of ‚Äúlikely questions‚Äù using GPT-4  \n",
    "- How we embed these questions and use them in FAISS for retrieval  \n",
    "- When HyPE gives better results ‚Äî and how it compares to standard document-based RAG  \n",
    "\n",
    "---\n",
    "\n",
    "### üîç Real-world Analogy\n",
    "\n",
    "Imagine you‚Äôre organizing a library.  \n",
    "Each book has:\n",
    "- A summary (raw document chunk)  \n",
    "- And a set of FAQ-style questions written on the back:  \n",
    "  - ‚ÄúWhat is the cause of rising sea levels?‚Äù  \n",
    "  - ‚ÄúHow does agriculture impact climate change?‚Äù\n",
    "\n",
    "Now a user walks in and asks:  \n",
    "> *\"What are the main drivers of global warming?\"*\n",
    "\n",
    "With a standard system, you‚Äôd match their query to the book summary. That might or might not work ‚Äî the wording could be too different.\n",
    "\n",
    "But with HyPE, you match the user‚Äôs question directly to one of those prewritten questions ‚Äî and boom, you find the right book instantly. üîç\n",
    "\n",
    "‚úÖ **HyPE bridges the gap between how users ask and how content is written.**\n",
    "\n",
    "---\n",
    "\n",
    "### üî¨ How HyPE Works Under the Hood\n",
    "\n",
    "Let‚Äôs say we have this chunk of a document:\n",
    "\n",
    "> \"Greenhouse gases like CO‚ÇÇ, methane, and nitrous oxide trap heat in the Earth‚Äôs atmosphere. These gases are largely released through fossil fuel burning, agriculture, and industrial processes.\"\n",
    "\n",
    "With **traditional RAG**:\n",
    "\n",
    "| Step    | What Happens                                                  |\n",
    "|---------|---------------------------------------------------------------|\n",
    "| Embed   | The full paragraph is embedded as-is                          |\n",
    "| Retrieve| User query is compared to this raw embedding                  |\n",
    "| Issue   | Mismatch if query wording doesn‚Äôt align                       |\n",
    "\n",
    "With **HyPE**, we do something smarter:\n",
    "\n",
    "| Step              | What Happens                                                                 |\n",
    "|-------------------|------------------------------------------------------------------------------|\n",
    "| 1. Chunk          | We take the original document chunk                                          |\n",
    "| 2. Prompting      | GPT-4 is asked: *‚ÄúWrite 3‚Äì5 questions that this chunk could answer‚Äù*         |\n",
    "| 3. Example Output | ‚Üí ‚ÄúWhat are the major greenhouse gases?‚Äù  \n",
    "                    ‚Üí ‚ÄúHow does agriculture affect emissions?‚Äù  \n",
    "                    ‚Üí ‚ÄúWhy is CO‚ÇÇ a concern?‚Äù                         |\n",
    "| 4. Embedding      | Each of these questions is embedded using OpenAI                             |\n",
    "| 5. Storage        | All these vectors are stored in FAISS, pointing to the same original chunk   |\n",
    "| 6. Retrieval      | At query time, user‚Äôs question is compared against these synthetic ones      |\n",
    "\n",
    "‚úÖ Now, if someone asks:  \n",
    "> *\"What causes climate change?\"*\n",
    "\n",
    "‚Ä¶it will match closely with the synthetic question *\"What are the major greenhouse gases?\"* ‚Äî leading us to the right context.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Why This Works So Well\n",
    "\n",
    "- üìå **Style alignment**: User queries are questions. Now the stored vectors are questions too.  \n",
    "- üîÅ **Multi-angle coverage**: A single chunk can answer many different questions ‚Äî and HyPE captures them all.  \n",
    "- ‚ö° **Fast at runtime**: All the hard work (prompting + embedding) is done offline. Retrieval is as fast as FAISS.\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è Why This Matters in Production\n",
    "\n",
    "Most real-world users don‚Äôt speak in textbook paragraphs. They ask questions like:\n",
    "- ‚ÄúHow does plastic harm the ocean?‚Äù\n",
    "- ‚ÄúWhat‚Äôs the cause of inflation?‚Äù\n",
    "\n",
    "But if your document says:\n",
    "> *‚ÄúMicroplastics accumulate in marine ecosystems over time‚Ä¶‚Äù*\n",
    "\n",
    "The vector match might fail ‚Äî because the query and text just don‚Äôt sound alike.\n",
    "\n",
    "**HyPE fixes this with precomputed question embeddings that match the user‚Äôs language, not the author‚Äôs.**\n",
    "\n",
    "üîë It‚Äôs like your system ‚Äúthinks ahead‚Äù and asks:  \n",
    "> *\"If someone needed this chunk, what kind of questions would they ask?\"*\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Where This Fits in RAG100X\n",
    "\n",
    "So far, RAG100x has explored:\n",
    "\n",
    "1. PDF-based QA  \n",
    "2. CSV-based retrieval  \n",
    "3. Blog-based hallucination checks  \n",
    "4. Chunk-size tuning  \n",
    "5. Proposition-level chunking  \n",
    "6. Query rewriting & decomposition  \n",
    "7. HyDE: Imagine the answer before searching  \n",
    "\n",
    "Now in **Day 8**, we go further ‚Äî not by rewriting the user‚Äôs question, but by **rewriting the documents as questions in advance**.\n",
    "\n",
    "> üí° **HyPE is like giving your documents a voice ‚Äî so they can ‚Äúraise their hand‚Äù when a user asks something they know the answer to.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de3f9d0",
   "metadata": {},
   "source": [
    "## üì¶ Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c95c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required packages\n",
    "!pip install faiss-cpu futures langchain-community python-dotenv tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable (comment out if not using OpenAI)\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = input(\"Please enter your OpenAI API key: \")\n",
    "else:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f0273",
   "metadata": {},
   "source": [
    "### üìå Define Constants and Download Dataset\n",
    "\n",
    "We start by setting up some important constants and downloading the data:\n",
    "\n",
    "- **PATH**: This points to the PDF file that we will use as our document source.\n",
    "- **LANGUAGE_MODEL_NAME**: Specifies the LLM we will use (e.g., `gpt-4o-mini`).\n",
    "- **EMBEDDING_MODEL_NAME**: Sets the embedding model (e.g., `text-embedding-3-small`) to convert chunks into vector representations.\n",
    "- **CHUNK_SIZE** and **CHUNK_OVERLAP**: These control how the document is split:\n",
    "  - `CHUNK_SIZE` defines the target length of each chunk.\n",
    "  - `CHUNK_OVERLAP` adds redundancy between chunks to preserve context at chunk boundaries.\n",
    "\n",
    "We also download a sample PDF document on climate change, which will be embedded and used throughout the RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2819bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the input document used in the RAG pipeline\n",
    "PATH = \"data/Understanding_Climate_Change.pdf\"\n",
    "\n",
    "# Language model for generating responses (LLM)\n",
    "LANGUAGE_MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "# Embedding model for converting text into vector format\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-3-small\"\n",
    "\n",
    "# Chunking configuration for RecursiveCharacterTextSplitter\n",
    "CHUNK_SIZE = 1000        # Target size of each document chunk\n",
    "CHUNK_OVERLAP = 200      # Number of overlapping characters between consecutive chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f6719",
   "metadata": {},
   "source": [
    "### üß† Generate Hypothetical Prompt Embeddings (HyPE)\n",
    "\n",
    "In this step, we simulate how users might query a specific chunk of text by generating **hypothetical questions** using an LLM. These questions are treated as semantic proxies for the chunk and embedded for later retrieval.\n",
    "\n",
    "#### üîç Why this matters:\n",
    "Instead of embedding raw chunks directly, we extract likely *questions* someone might ask about that chunk. This results in:\n",
    "- Better semantic alignment between queries and documents.\n",
    "- Improved retrieval performance for natural user queries.\n",
    "\n",
    "#### üõ†Ô∏è Under the hood:\n",
    "1. A prompt is used to instruct the LLM to generate multiple natural questions about the given chunk.\n",
    "2. These questions are parsed and cleaned (e.g., removing extra newlines).\n",
    "3. Each question is embedded using OpenAI's embedding model.\n",
    "4. We return both the original chunk and its corresponding question embeddings.\n",
    "\n",
    "#### ‚öôÔ∏è Why return the `chunk_text` along with the embeddings?\n",
    "\n",
    "When using **multithreading** or **parallel processing** to speed up embedding generation, each thread typically works on a different chunk independently.\n",
    "\n",
    "However, Python's multithreaded map functions (like `ThreadPoolExecutor.map`) expect the function to return a result that still links back to the original input ‚Äî in this case, the `chunk_text`.\n",
    "\n",
    "Returning both the original chunk and its associated embeddings allows us to:\n",
    "- Keep track of which embeddings belong to which text chunk.\n",
    "- Avoid issues with unordered or mismatched outputs in parallel processing.\n",
    "- Efficiently construct the final vectorstore after embedding is complete.\n",
    "\n",
    "So even though we only need the embeddings for retrieval, we return `chunk_text` too for traceability and mapping during parallel execution.\n",
    "\n",
    "\n",
    "This technique, called **Hypothetical Prompt Embeddings (HyPE)**, helps bridge the gap between document phrasing and user queries by mimicking how people naturally ask about content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f9032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypothetical_prompt_embeddings(chunk_text: str):\n",
    "    \"\"\"\n",
    "    Uses the LLM to generate multiple hypothetical questions for a single chunk.\n",
    "    These questions act as proxies for the chunk during retrieval.\n",
    "\n",
    "    Parameters:\n",
    "    chunk_text (str): Text contents of the chunk.\n",
    "\n",
    "    Returns:\n",
    "    chunk_text (str): Returned as-is to simplify multi-threaded processing.\n",
    "    hypothetical_embeddings (List[float]): Embedding vectors for the generated questions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load LLM and embedding model with specified names\n",
    "    llm = ChatOpenAI(temperature=0, model_name=LANGUAGE_MODEL_NAME)\n",
    "    embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "    # PromptTemplate defines the instructions given to the LLM\n",
    "    question_gen_prompt = PromptTemplate.from_template(\n",
    "        \"Analyze the input text and generate essential questions that, when answered, \\\n",
    "        capture the main points of the text. Each question should be one line, \\\n",
    "        without numbering or prefixes.\\n\\n \\\n",
    "        Text:\\n{chunk_text}\\n\\nQuestions:\\n\"\n",
    "    )\n",
    "\n",
    "    # Chain the prompt to LLM and parse output as string\n",
    "    question_chain = question_gen_prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Invoke the chain to generate a list of questions\n",
    "    raw_questions = question_chain.invoke({\"chunk_text\": chunk_text})\n",
    "\n",
    "    # Remove extra newlines (gpt-4o often uses \\n\\n between questions)\n",
    "    questions = raw_questions.replace(\"\\n\\n\", \"\\n\").split(\"\\n\")\n",
    "\n",
    "    # Embed the list of questions into vector format\n",
    "    return chunk_text, embedding_model.embed_documents(questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecd921d",
   "metadata": {},
   "source": [
    "### üß† What Does This Block Do?\n",
    "\n",
    "This code builds a **FAISS vector store** that helps find relevant chunks later when a user asks a question.  \n",
    "But instead of just embedding the raw text chunks, we use **Hypothetical Prompt Embeddings (HyPE)** ‚Äî meaning we:\n",
    "\n",
    "- Generate **questions** that the chunk could answer.\n",
    "- Embed those questions instead of the chunk itself.\n",
    "- Store those embeddings in FAISS for fast, accurate retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° Why Use Hypothetical Prompt Embeddings?\n",
    "\n",
    "Because when someone asks a question, it's easier to match it against other **questions** than against raw text.\n",
    "\n",
    "> üîç For example:  \n",
    "> Suppose we have a chunk:  \n",
    "> _\"LangChain is a framework for developing LLM-powered apps using chains of components like retrievers, memory, and tools.\"_  \n",
    ">\n",
    "> We ask the LLM:  \n",
    "> üëâ ‚ÄúWhat are the key questions this chunk could answer?‚Äù  \n",
    "> It might return:\n",
    "> - What is LangChain?\n",
    "> - What is LangChain used for?\n",
    "> - What are the components in a LangChain pipeline?\n",
    "\n",
    "These questions are embedded and stored.\n",
    "\n",
    "Later, when a user asks:  \n",
    "üí¨ ‚ÄúHow does LangChain work?‚Äù  \n",
    "...it will match well with these hypothetical questions and retrieve the right chunk.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è How It Works Under the Hood\n",
    "\n",
    "1. **Multithreading with `ThreadPoolExecutor`**  \n",
    "   We use multithreading to generate embeddings **in parallel** ‚Äî so if there are 100 chunks, we don‚Äôt wait for one to finish before starting the next.\n",
    "\n",
    "2. **Generate Hypothetical Questions**  \n",
    "   For each chunk, we call an LLM (like GPT-4o) to create 3‚Äì5 questions the chunk might answer.\n",
    "\n",
    "3. **Embed the Questions**  \n",
    "   Each question is embedded using OpenAI's embedding model, turning it into a vector of numbers.\n",
    "\n",
    "4. **Store in FAISS**  \n",
    "   FAISS is initialized to store these vectors efficiently:\n",
    "   - It uses **L2 distance** to find which vectors are \"closest\" (most relevant).\n",
    "   - It also keeps the original text so we can show it later.\n",
    "\n",
    "5. **Add Multiple Embeddings per Chunk**  \n",
    "   Since one chunk can answer many questions, we embed and store each question separately ‚Äî even if they all point to the same chunk.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Result\n",
    "\n",
    "You now have a FAISS store where each text chunk is **indexed through its most likely questions**, improving both **retrieval relevance** and **semantic alignment**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vector_store(chunks: List[str]):\n",
    "    # Wait with initialization to see vector lengths\n",
    "    vector_store = None  \n",
    "    # Run prompt embedding generation in parallel using multithreading\n",
    "    # This speeds up processing across multiple chunks\n",
    "    with ThreadPoolExecutor() as pool:  \n",
    "        futures = [pool.submit(generate_hypothetical_prompt_embeddings, c) for c in chunks]\n",
    "\n",
    "        # Process completed results in order of finish (not submission)\n",
    "        for f in tqdm(as_completed(futures), total=len(chunks)):  \n",
    "            chunk, vectors = f.result()  # Get chunk and list of its prompt embeddings\n",
    "\n",
    "            # Initialize FAISS on first result to get vector dimension\n",
    "            if vector_store == None:  \n",
    "                vector_store = FAISS(\n",
    "                    embedding_function=OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME),\n",
    "                    index=faiss.IndexFlatL2(len(vectors[0])),  # L2 distance-based similarity\n",
    "                    docstore=InMemoryDocstore(),\n",
    "                    index_to_docstore_id={}\n",
    "                )\n",
    "\n",
    "            # Each chunk is stored multiple times ‚Äî once per generated question embedding\n",
    "            chunks_with_embedding_vectors = [(chunk.page_content, vec) for vec in vectors]\n",
    "\n",
    "            # Add all embeddings (and corresponding content) to FAISS\n",
    "            vector_store.add_embeddings(chunks_with_embedding_vectors)\n",
    "    return vector_store  # Return the populated vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f4cfd",
   "metadata": {},
   "source": [
    "### Encode PDF into FAISS Vector Store\n",
    "\n",
    "- Loads PDF and extracts raw text.\n",
    "- Splits text into overlapping chunks (preserve context).\n",
    "- Cleans the text (removes noisy characters like \\t).\n",
    "- Embeds each chunk using HyPE (hypothetical questions).\n",
    "- Stores embeddings in FAISS for fast and accurate retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF into a FAISS vector store using HyPE-based embeddings.\n",
    "    \"\"\"\n",
    "    # Load the PDF and extract all text\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split text into overlapping chunks to preserve context\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Clean the text to remove \\t and other noisy characters\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    # Generate hypothetical embeddings and store in FAISS\n",
    "    vectorstore = prepare_vector_store(cleaned_texts)\n",
    "\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62410a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context_per_question(question, retriever):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    return [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43cb5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_context(context_list):\n",
    "    for i, chunk in enumerate(context_list):\n",
    "        print(f\"\\n--- Chunk {i+1} ---\\n{chunk[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e66a7",
   "metadata": {},
   "source": [
    "### Create & Test HyPE Retriever\n",
    "\n",
    "- üìö Process the PDF and encode it into a FAISS vector store.\n",
    "- üîç Initialize a retriever to fetch top-k relevant chunks.\n",
    "- üß™ Run a sample query to test retrieval quality.\n",
    "- ‚úÖ Deduplicates and displays matched context for inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93860167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the PDF into a FAISS vector store using HyPE embeddings\n",
    "# Chunk size can be large with HyPE ‚Äî more info improves question generation\n",
    "chunks_vector_store = encode_pdf(PATH, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "# Create a retriever to fetch top-k relevant chunks based on query similarity\n",
    "chunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Test the retriever with a sample query\n",
    "test_query = \"What is the main cause of climate change?\"\n",
    "\n",
    "# Retrieve and deduplicate context chunks\n",
    "context = retrieve_context_per_question(test_query, chunks_query_retriever)\n",
    "context = list(set(context))\n",
    "\n",
    "# Display the context returned by the retriever\n",
    "show_context(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f32adf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìò Summary & Credits\n",
    "\n",
    "This notebook is based on the excellent open-source repository [RAG_Techniques by NirDiamant](https://github.com/NirDiamant/RAG_Techniques).  \n",
    "I referred to that work to understand how the pipeline is structured and then reimplemented the same concept in a **fully self-contained** way, but using recent models ‚Äî as part of my personal learning journey.\n",
    "\n",
    "The purpose of this notebook is purely **educational**:  \n",
    "- To deepen my understanding of Retrieval-Augmented Generation systems  \n",
    "- To keep a clean, trackable log of what I‚Äôve built and learned  \n",
    "- And to serve as a future reference for myself or others starting from scratch\n",
    "\n",
    "To support that, I‚Äôve added clear, concise markdowns throughout the notebook ‚Äî explaining *why* each package was installed, *why* each line of code exists, and *how* each component fits into the overall RAG pipeline. It‚Äôs designed to help anyone (including my future self) grasp the **how** and the **why**, not just the **what**.\n",
    "\n",
    "\n",
    "## üîç Why Use HyPE in RAG?\n",
    "\n",
    "Traditional RAG systems often struggle when there's a mismatch between the **style of the user query** and the way documents are written. This is especially problematic when user queries are short, vague, or phrased as questions.\n",
    "\n",
    "**HyPE (Hypothetical Prompt Embeddings)** solves this by:\n",
    "- ‚ùì **Generating multiple hypothetical questions per chunk during indexing**\n",
    "- üìå **Embedding those questions** so queries can match with them instead of raw chunk text\n",
    "\n",
    "This turns retrieval into a **question-to-question matching problem**, which better aligns user queries with relevant information.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What‚Äôs New in This Version?\n",
    "\n",
    "This implementation of HyPE offers:\n",
    "\n",
    "- ü§ñ **GPT-4o-based prompt generation** ‚Äî Generates multiple smart questions per chunk  \n",
    "- ‚ö° **Offline embedding at indexing time** ‚Äî No runtime query expansion needed  \n",
    "- üìÑ **Multi-vector indexing** ‚Äî Each chunk gets several semantic representations  \n",
    "- üß± **Colab-ready & self-contained** ‚Äî All logic included, no copy-paste from modules  \n",
    "\n",
    "It‚Äôs designed for flexibility and can be used with any PDF input.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Inferences & Key Takeaways\n",
    "\n",
    "Running HyPE on real-world queries like *\"What is the main cause of climate change?\"* reveals:\n",
    "\n",
    "- ‚ùî The hypothetical questions align closely with what users typically ask  \n",
    "- üì• FAISS retrieves more accurate and diverse chunks by matching on these questions  \n",
    "- üí° This approach improves grounding **without increasing latency or complexity**  \n",
    "\n",
    "It‚Äôs an efficient way to enhance retrieval **without extra cost at query time**.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ What Could Be Added Next?\n",
    "\n",
    "For a production-grade HyPE-enhanced RAG pipeline:\n",
    "\n",
    "- üîÅ **Combine with traditional retrieval** ‚Äî Use both chunk and prompt-based indexes  \n",
    "- üß™ **Run evaluation benchmarks** ‚Äî Measure hallucination and relevance gains  \n",
    "- ‚öôÔ∏è **Swap OpenAI with local models** ‚Äî Try embedding with BGE or Cohere for cost savings  \n",
    "- üßµ **Pair with rerankers** ‚Äî Filter top chunks using LLM scoring after retrieval  \n",
    "- üîå **Support other vectorstores** ‚Äî Easily integrate with pgvector, Chroma, or Weaviate  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## üí° Final Word\n",
    "\n",
    "This notebook is part of my larger personal project: **RAG100x** ‚Äî a challenge to build and log my journney in RAG from 0 100 in the coming months.\n",
    "\n",
    "It‚Äôs not built to impress ‚Äî it‚Äôs built to **progress**.  \n",
    "Everything here is structured to enable **daily iteration**, focused experimentation, and clean documentation.\n",
    "\n",
    "If you're exploring RAG from first principles, feel free to use this as a scaffold for your own builds. And of course ‚Äî check out the original repository for broader implementations and ideas."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
